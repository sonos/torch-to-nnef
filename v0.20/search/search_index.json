{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-the-documentation","title":"Welcome to the documentation","text":""},{"location":"#goals-scope","title":"Goals &amp; Scope","text":"<p><code>torch_to_nnef</code> Python package is used to export any model formulated with vanilla PyTorch, whatever the internal tensor types (including quantized models), into NNEF format.</p> <p>, the neural network inference engine developed openly by , is the primary supported target, and best compatibility with it is ensured. To use it, the <code>TractNNEF</code> inference_target must be specified. This allows extended NNEF operators and specificities to be expressed:</p> <ul> <li>Transformer blocks</li> <li>recurrent layers (LSTM, GRU, \u2026)</li> <li>dynamic streamable input dimensions</li> <li>data type casting (since NNEF spec is too vague in this regard)</li> </ul> <p>Minimal dependencies are kept in this package (to allow easy integration in other projects).</p>"},{"location":"#support","title":"Support","text":"<p>PyTorch &gt;= 1.10.0 with the last 2 major releases of tract (&gt;= 0.20.22 to date) over Linux and MacOS systems is officially supported, and the package is maintained/tested for Python versions that are not end of life, nor pre-release. Only pre-compiled PyTorch wheels and dependencies available on pypi are used in CI, so this support evolves over time. Latest package versions ensure better opset coverage and unlock all features.</p>"},{"location":"#install","title":"Install","text":"<p>Today, the project is packaged in PyPi. Installation can be performed depending on the package manager:</p> pipuvpoetry <pre><code>pip install torch_to_nnef\n</code></pre> <pre><code>uv add torch_to_nnef\n</code></pre> <pre><code>poetry add torch_to_nnef\n</code></pre> <p>Note</p> <p>The project scope is broad and contributions are welcome, if any bug is encountered, the Bug report instructions should be followed.</p>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#changelog","title":"Changelog","text":""},{"location":"CHANGELOG/#unreleased","title":"Unreleased","text":""},{"location":"CHANGELOG/#0200-2025-09-13","title":"[0.20.0] - 2025-09-13","text":""},{"location":"CHANGELOG/#added","title":"Added","text":"<ul> <li>Open-Sourced the project under MIT|Apache2 license</li> <li>Official support for tract <code>v0.22.0</code></li> <li>test coverage of LLM export with various <code>transformers</code> lib version (trying to support last 10ish minor versions with CI/CD)</li> <li>Add context manager to force loading with offloaded tensor</li> <li>Added opt-in support for reification of <code>spda</code> operator when targeting tract export (thanks to @emricksinisonos contribution) this should help further optimization in tract of attention blocks</li> <li>Added support for <code>upsample</code> operator via <code>deconv</code> or <code>debox</code> depending on tract version</li> <li>Added Licenses file</li> <li><code>ModTensorUpdater</code> is now useful with legacy torch version (bellow 2.0)</li> <li>Add <code>aten::new</code></li> <li>New logo (thanks to @lizavetaobadava-commits)</li> </ul>"},{"location":"CHANGELOG/#formatting-style","title":"Formatting &amp; style","text":"<ul> <li>All exception now inherit from <code>T2NError</code> (allow easier catch)</li> <li>Stricter line length (even in doc)</li> <li>Stricter doc formatting with <code>ruff</code></li> <li>Improved <code>prospector</code> strictness</li> <li><code>isort</code> retired in favor of <code>ruff</code></li> </ul>"},{"location":"CHANGELOG/#documentation","title":"Documentation","text":"<ul> <li>Documentation versioning with <code>mike</code> (allowing to get older version doc)</li> <li>Documentation: fixed typos, rewording (thanks to @thomasnigoghossiansonos for the review)</li> <li>WASM LLM poetry generator example expose the prompt for clarity</li> <li>Nicer WASM example with more loading state infos</li> <li>Fix WASM VAD example handling more audio context (more robust)</li> <li>Added WASM Yolo example with pose-estimation</li> </ul>"},{"location":"CHANGELOG/#fixed","title":"Fixed","text":"<ul> <li>transformers regression since 4.56 around cache handling</li> <li>better support for OffloadedTensor with assignations and some in-place operations</li> <li>pylint tweaks</li> </ul>"},{"location":"CHANGELOG/#change","title":"Change","text":"<ul> <li>Following open-sourcing of the project, packaging is now targeting PyPI.</li> </ul>"},{"location":"CHANGELOG/#0191-2025-08-06","title":"[0.19.1] - 2025-08-06","text":""},{"location":"CHANGELOG/#added_1","title":"Added","text":"<ul> <li>CI/CD for torch version bellow 2.7: 2.2, 1.13 and 1.10</li> <li>specific checks around dtype for qtensors tests generated assets</li> </ul>"},{"location":"CHANGELOG/#fixes","title":"Fixes","text":"<ul> <li>make this package work again for torch version between 1.10 and 2.3.</li> </ul>"},{"location":"CHANGELOG/#0190-2025-07-25","title":"[0.19.0] - 2025-07-25","text":""},{"location":"CHANGELOG/#added_2","title":"Added","text":"<ul> <li>mkdoc documentation revamp</li> <li>no more approximation of <code>logsofmax</code> for TractNNEF</li> <li>added support for operators: <code>fmod</code>, <code>expm1</code>, <code>atan2</code>, <code>addmm</code>, <code>maximum</code>, <code>minimum</code>, <code>logical_or</code>, <code>logical_and</code>, <code>logical_not</code>, <code>fill_</code>, <code>var</code>, <code>avg_adaptive_pool_nd</code>, <code>max_adaptive_pool_nd</code>, <code>amin</code>, <code>amax</code>, <code>nn.LocalResponseNorm</code></li> </ul>"},{"location":"CHANGELOG/#0186-2025-07-03","title":"[0.18.6] - 2025-07-03","text":""},{"location":"CHANGELOG/#added_3","title":"Added","text":"<ul> <li>base param updater</li> </ul>"},{"location":"CHANGELOG/#0185-2025-06-13","title":"[0.18.5] - 2025-06-13","text":""},{"location":"CHANGELOG/#added_4","title":"Added","text":"<ul> <li><code>update_values</code> in OffloadedTensor</li> </ul>"},{"location":"CHANGELOG/#fix","title":"Fix","text":"<ul> <li><code>Parameter</code> addition in OffloadedTensor</li> <li><code>to_json_file</code> use in config dump in LLM</li> </ul>"},{"location":"CHANGELOG/#0184-2025-06-11","title":"[0.18.4] - 2025-06-11","text":""},{"location":"CHANGELOG/#change_1","title":"Change","text":"<ul> <li>bunch of cache, ordered conditioning, torch.compile to make export faster</li> </ul>"},{"location":"CHANGELOG/#0183-2025-06-05","title":"[0.18.3] - 2025-06-05","text":""},{"location":"CHANGELOG/#fix_1","title":"Fix","text":"<ul> <li><code>dtype</code> change (via .to) of OffloadedTensor handled</li> <li><code>numel</code> call avoid OffloadedTensor reload</li> <li><code>dtype</code> getter aligned</li> <li>dissociated getter aligned</li> </ul>"},{"location":"CHANGELOG/#added_5","title":"Added","text":"<ul> <li>support for python 3.13 landed (some issue still with Quant+Offloaded tensor mem alloc)</li> <li><code>aten::bitwise_or</code> operator</li> </ul>"},{"location":"CHANGELOG/#0182-2025-06-04","title":"[0.18.2] - 2025-06-04","text":""},{"location":"CHANGELOG/#fix_2","title":"Fix","text":"<ul> <li><code>safetensors</code> import, only called when needed</li> </ul>"},{"location":"CHANGELOG/#0181-2025-06-03","title":"[0.18.1] - 2025-06-03","text":""},{"location":"CHANGELOG/#added_6","title":"Added","text":"<ul> <li>Official tract support version: <code>0.21.13</code></li> </ul>"},{"location":"CHANGELOG/#0180-2025-06-03","title":"[0.18.0] - 2025-06-03","text":""},{"location":"CHANGELOG/#added_7","title":"Added","text":"<ul> <li>addition of an <code>OffloadTensor</code> that allow to write on disk the tensor and reload it each time from there (trading memory space for disk usage/reloading speed -&gt; this is not intended to be used beyond compression and export of neural net stage).</li> <li>Plug of a load step by step into <code>OffloadTensor</code> method for <code>tract_llm</code> (as an opt-in via <code>--device-map</code>=<code>t2n_offload_disk</code> option). This option is also compatible with accelerate if installed to spread model partitions load across available hardware devices in an instance.</li> </ul>"},{"location":"CHANGELOG/#change_2","title":"Change","text":"<ul> <li>refactor of all custom PyTorch tensors used on torch to NNEF into a unified module</li> <li>[OPTIM] removal of part of redundant inference tracing computation for shape and type</li> </ul>"},{"location":"CHANGELOG/#fix_3","title":"Fix","text":"<ul> <li>avoid duplicate weights in Numpy data within <code>nnef.Graph until</code> serialization (write) step</li> </ul>"},{"location":"CHANGELOG/#0174-2025-05-15","title":"[0.17.4] - 2025-05-15","text":""},{"location":"CHANGELOG/#fix_4","title":"Fix","text":"<ul> <li>Add eq in TensorVariable to build proper dict keys and in queries from it (without traced data accounted)</li> <li>all tract_core_gather add attrs datum_type</li> <li>Q4 compression_method tag compat with internal llm lib</li> <li>Skip check_io between wrapper_model vs hf_model if wrapped_model</li> </ul>"},{"location":"CHANGELOG/#0173-2025-05-09","title":"[0.17.3] - 2025-05-09","text":""},{"location":"CHANGELOG/#added_8","title":"Added","text":"<ul> <li>aten::<code>full_like</code>, <code>_softmax</code>, <code>mm</code>, <code>logical_not</code>, <code>scalar_tensor</code>, <code>to_copy</code></li> <li>forward signature of wrapped llm models is updated live based on model KV cache quantity to help <code>torch.export</code> understand all parameters (args, *kwargs does not work)</li> </ul>"},{"location":"CHANGELOG/#change_3","title":"Change","text":"<ul> <li><code>HFConfigHelper</code> now only need HF conf (no more slug name)</li> </ul>"},{"location":"CHANGELOG/#0172-2025-04-10","title":"[0.17.2] - 2025-04-10","text":""},{"location":"CHANGELOG/#added_9","title":"Added","text":"<ul> <li>bump tract <code>0.21.12</code></li> <li>avoid compress weight if shared with 1 module that is not requested to compress (by example: request <code>nn.Linear</code> only while shared with <code>nn.Embedding</code>)</li> </ul>"},{"location":"CHANGELOG/#fix_5","title":"Fix","text":"<ul> <li>some <code>ignore-already-exist-dir</code> missing case in <code>llm_tract</code></li> </ul>"},{"location":"CHANGELOG/#0171-2025-04-02","title":"[0.17.1] - 2025-04-02","text":""},{"location":"CHANGELOG/#fix_6","title":"Fix","text":"<ul> <li>Avoid duplicating weights in case they are shared with assignation post <code>nn.Module</code> load</li> </ul>"},{"location":"CHANGELOG/#0170-2025-03-31","title":"[0.17.0] - 2025-03-31","text":""},{"location":"CHANGELOG/#change_4","title":"Change","text":"<ul> <li>All parameters variable in graph are be named the same their label if <code>NamedTensor</code></li> </ul>"},{"location":"CHANGELOG/#fix_7","title":"Fix","text":"<ul> <li>RNN expansion with multiple call within same graph now refer to same set of weight instead of duplicating them</li> </ul>"},{"location":"CHANGELOG/#01611-2025-03-27","title":"[0.16.11] - 2025-03-27","text":""},{"location":"CHANGELOG/#fix_8","title":"Fix","text":"<ul> <li><code>set_priority</code> in <code>with sdpa_kernel</code> only appear in torch 2.6</li> </ul>"},{"location":"CHANGELOG/#01610-2025-03-24","title":"[0.16.10] - 2025-03-24","text":""},{"location":"CHANGELOG/#fix_9","title":"Fix","text":"<ul> <li><code>aten::flatten</code> with partial dimensions</li> <li><code>aten::remainder</code> force f32 (if other implicit dtype support like ints)</li> <li><code>aten::pad...</code> now support dynamic dimensions</li> <li><code>aten::zeros</code>, ... now default to f32 in cases where unspecified in jit graph</li> <li>Merge of subgraph in ir_graph is now done with preserving <code>subgraph</code> output names (needed since some output may be repeated while main graph unaware of it)</li> </ul>"},{"location":"CHANGELOG/#added_10","title":"Added","text":"<ul> <li>Conv are now supported for Q40 exports (tract <code>v0.21.12</code>)</li> <li>compress registry <code>min_max_q4_0_all</code> export all supported tensors in Q40 (including Conv1d, Conv2d)</li> </ul>"},{"location":"CHANGELOG/#0169-2025-03-20","title":"[0.16.9] - 2025-03-20","text":""},{"location":"CHANGELOG/#fix_10","title":"Fix","text":"<ul> <li>regression on <code>uint32</code>, <code>uint64</code> support (pre torch 2.4)</li> </ul>"},{"location":"CHANGELOG/#0168-2025-03-20","title":"[0.16.8] - 2025-03-20","text":""},{"location":"CHANGELOG/#fix_11","title":"Fix","text":"<ul> <li>regression on <code>uint16</code> support (pre torch 2.4)</li> </ul>"},{"location":"CHANGELOG/#0167-2025-03-20","title":"[0.16.7] - 2025-03-20","text":""},{"location":"CHANGELOG/#fix_12","title":"Fix","text":"<ul> <li>complex slice index gather nd fix</li> </ul>"},{"location":"CHANGELOG/#0166-2025-03-20","title":"[0.16.6] - 2025-03-20","text":""},{"location":"CHANGELOG/#added_11","title":"Added","text":"<ul> <li>official tract support is now <code>0.21.11</code> (new default target)</li> <li>support <code>to</code> device like <code>cuda</code>,<code>mps</code> for our internal QTensor  ...</li> <li>support for new operators: <code>aten::empty_like</code>, <code>aten::prod</code>, <code>aten::index_select</code>, <code>aten::scatter</code>, <code>aten::numel</code></li> </ul>"},{"location":"CHANGELOG/#change_5","title":"Change","text":"<ul> <li>additional tracing cues for whole number values that may be used in tensors shaping/construction.</li> <li>disabled support for Python &gt;=3.13 as of now as it leads to unexpected hash/set issues to be investigated</li> </ul>"},{"location":"CHANGELOG/#fix_13","title":"Fix","text":"<ul> <li><code>aten::baddbmm</code> extra args handled during tracing</li> <li>better alignment of arity for rnn inputs</li> <li>equality operators (<code>ne</code>, <code>ge</code>, <code>le</code>, <code>gt</code>, <code>eq</code>) now implicit cast to common dtype if heterogeneous</li> <li><code>to</code> operators with from float to unsigned with negative values was found to have an arch dependant behavior (code now align to the arch used at export with warning for non arm)</li> <li>tolerate export pad operators with dynamic values</li> </ul>"},{"location":"CHANGELOG/#0165-2025-03-11","title":"[0.16.5] - 2025-03-11","text":""},{"location":"CHANGELOG/#change_6","title":"Change","text":"<ul> <li>test by default on 2.6</li> </ul>"},{"location":"CHANGELOG/#fix_14","title":"Fix","text":"<ul> <li>SPDA regression if pytorch &gt; 2.3 and usage of specific scale</li> </ul>"},{"location":"CHANGELOG/#0164-2025-03-11","title":"[0.16.4] - 2025-03-11","text":""},{"location":"CHANGELOG/#added_12","title":"Added","text":"<ul> <li>support new <code>Q40</code> tract format starting with target tract&gt;=0.21.11</li> </ul>"},{"location":"CHANGELOG/#fix_15","title":"Fix","text":"<ul> <li>remove useless hard dependencies (regression since 0.15.10 about) and relaxing numpy version</li> </ul>"},{"location":"CHANGELOG/#0163-2025-03-07","title":"[0.16.3] - 2025-03-07","text":""},{"location":"CHANGELOG/#fix_16","title":"Fix","text":"<ul> <li>edge-case in <code>tract_llm</code> export forward_kwargs</li> </ul>"},{"location":"CHANGELOG/#0162-2025-03-07","title":"[0.16.2] - 2025-03-07","text":""},{"location":"CHANGELOG/#added_13","title":"Added","text":"<ul> <li>better debug dump with shell script to reproduce failing case</li> </ul>"},{"location":"CHANGELOG/#fix_17","title":"Fix","text":"<ul> <li>export RNN with 2nd or 3rd outputs used only</li> <li>export support <code>tract_llm</code> architecture without <code>num-logits-to-keep</code></li> <li>explicit peft dependency referenced in pyproject</li> </ul>"},{"location":"CHANGELOG/#0161-2025-03-06","title":"[0.16.1] - 2025-03-06","text":""},{"location":"CHANGELOG/#added_14","title":"Added","text":"<ul> <li>export with <code>tract_llm</code> merge PEFT option is set</li> <li>CI now fail-fast</li> <li>VERSION is set at project root to help compare with str</li> <li>better test_suite naming for dump and debug</li> </ul>"},{"location":"CHANGELOG/#change_7","title":"Change","text":"<ul> <li>export with <code>tract_llm</code> will use <code>num-logits-to-keep</code> avoiding useless compute at inference</li> </ul>"},{"location":"CHANGELOG/#0160-2025-03-03","title":"[0.16.0] - 2025-03-03","text":""},{"location":"CHANGELOG/#change_8","title":"Change","text":"<ul> <li>Breaking change <code>-f16</code>,<code>--as-float16</code> removed and replaced by <code>--force-module-dtype</code>, <code>--force-inputs-dtype</code> that re-express this</li> </ul>"},{"location":"CHANGELOG/#01518-2025-02-28","title":"[0.15.18] - 2025-02-28","text":""},{"location":"CHANGELOG/#fix_18","title":"Fix","text":"<ul> <li>PEFT loading from tract llm cli regression</li> <li>using embedding gather with 1d tensor indices input</li> </ul>"},{"location":"CHANGELOG/#01517-2025-02-24","title":"[0.15.17] - 2025-02-24","text":""},{"location":"CHANGELOG/#fix_19","title":"Fix","text":"<ul> <li>correct branching in tract selection cmd llm export</li> </ul>"},{"location":"CHANGELOG/#01516-2025-02-24","title":"[0.15.16] - 2025-02-24","text":""},{"location":"CHANGELOG/#fix_20","title":"Fix","text":"<ul> <li>Avoid auto log settings except in cli's</li> </ul>"},{"location":"CHANGELOG/#added_15","title":"Added","text":"<ul> <li>f32 norm options in llm cli</li> </ul>"},{"location":"CHANGELOG/#01515-2025-02-19","title":"[0.15.15] - 2025-02-19","text":""},{"location":"CHANGELOG/#fix_21","title":"Fix","text":"<ul> <li>Format safety in tract_properties (avoid caret return escape and other closing quote)</li> </ul>"},{"location":"CHANGELOG/#01514-2025-02-19","title":"[0.15.14] - 2025-02-19","text":""},{"location":"CHANGELOG/#fix_22","title":"Fix","text":"<ul> <li>another compress import issue</li> </ul>"},{"location":"CHANGELOG/#01513-2025-02-19","title":"[0.15.13] - 2025-02-19","text":""},{"location":"CHANGELOG/#fix_23","title":"Fix","text":"<ul> <li>wrong default for compress registry llm_tract cli</li> </ul>"},{"location":"CHANGELOG/#01512-2025-02-19","title":"[0.15.12] - 2025-02-19","text":""},{"location":"CHANGELOG/#change_9","title":"Change","text":"<ul> <li>move <code>torch_to_nnef.llm_tract.compress</code> to <code>torch_to_nnef.compress</code> as it is generic</li> </ul>"},{"location":"CHANGELOG/#fix_24","title":"Fix","text":"<ul> <li>test suite pass again on Darwin OS</li> <li>some remaining trace of <code>flake8</code>,<code>black</code> to <code>ruff</code></li> </ul>"},{"location":"CHANGELOG/#01511-2025-02-17","title":"[0.15.11] - 2025-02-17","text":""},{"location":"CHANGELOG/#added_16","title":"Added","text":"<ul> <li>support p norm with p != 1 or 2 (including inf and -inf norms)</li> <li>upcast to f32 norm operations if f16 inputs such as <code>BatchNorm</code>, <code>norm_p2</code>, <code>group_norm</code>, <code>weight_norm</code></li> <li>more tract default properties among which export command, python version, (and opt-out) username, hostname, OS info (uname -a)</li> </ul>"},{"location":"CHANGELOG/#01510-2025-02-14","title":"[0.15.10] - 2025-02-14","text":""},{"location":"CHANGELOG/#change_10","title":"Change","text":"<ul> <li>packaging/building project with <code>uv</code> (<code>poetry</code> deprecated since latest uv version are better)</li> </ul>"},{"location":"CHANGELOG/#0159-2025-02-10","title":"[0.15.9] - 2025-02-10","text":""},{"location":"CHANGELOG/#added_17","title":"Added","text":"<ul> <li>ready to support tract 0.21.9 (once regression tract side solved)</li> </ul>"},{"location":"CHANGELOG/#0158-2025-02-07","title":"[0.15.8] - 2025-02-07","text":""},{"location":"CHANGELOG/#added_18","title":"Added","text":"<ul> <li>TractNNEF now dump: <code>tract_properties</code> in graph.nnef with metadata infos and possible additional custom infos can be passed with <code>specific_properties</code></li> <li>TractNNEF: control over check io precision with <code>check_io_tolerance</code> parameters (exposed in llm cli)</li> <li>TractNNEF: has now <code>force_attention_inner_in_f32</code> that force f32 compute for SDPA in tract</li> <li>TractNNEF: has now <code>force_linear_accumulation_in_f32</code> that should be active after tract release <code>0.21.10</code> and allow accumulation in f32 for linears (opt-in)</li> <li>cli llm: export of specific model like qwen force f32 parameters defined upper by default (for others architectures those are exposed in cli directly)</li> </ul>"},{"location":"CHANGELOG/#0157-2025-01-29","title":"[0.15.7] - 2025-01-29","text":""},{"location":"CHANGELOG/#fix_25","title":"Fix","text":"<ul> <li>LLM cli export: <code>PEFT</code> better support</li> <li>LLM cli export: multiple <code>.safetensors</code> support</li> <li><code>LLMExporter</code> decoupled and better supported</li> </ul>"},{"location":"CHANGELOG/#0156-2025-01-10","title":"[0.15.6] - 2025-01-10","text":""},{"location":"CHANGELOG/#fix_26","title":"Fix","text":"<ul> <li><code>unsqueeze</code> on dim -1</li> <li><code>sum</code> without arguments</li> </ul>"},{"location":"CHANGELOG/#added_19","title":"Added","text":"<ul> <li><code>uint16</code> support (since PyTorch 2.4)</li> <li><code>gather</code>, <code>sort</code>, <code>argsort</code>, <code>topk</code> PyTorch operators support</li> </ul>"},{"location":"CHANGELOG/#0155-2024-12-13","title":"[0.15.5] - 2024-12-13","text":""},{"location":"CHANGELOG/#change_11","title":"Change","text":"<ul> <li><code>erf</code>, <code>hardswish</code> use tract NNEF core component if inference targeted.</li> </ul>"},{"location":"CHANGELOG/#0154-2024-11-04","title":"[0.15.4] - 2024-11-04","text":""},{"location":"CHANGELOG/#fix_27","title":"Fix","text":"<ul> <li>test suite working again for KhronosNNEF (full test suite green)</li> <li>hide some warning</li> </ul>"},{"location":"CHANGELOG/#change_12","title":"Change","text":"<ul> <li><code>export_tensors_to_nnef</code>, <code>export_tensors_from_disk_to_nnef</code> as root module access</li> <li>allow compression method to use gradients if needed</li> <li>expose ability to manage device in QTensor mechanism with <code>.to_device</code> in <code>QScheme</code> &amp; <code>U8Compressor</code></li> <li>better collision handling of tensor with different dtype in <code>QTensorTractScaleOnly</code></li> </ul>"},{"location":"CHANGELOG/#added_20","title":"Added","text":"<ul> <li>dump debug bundle with <code>KhronosNNEF</code> inference_target</li> <li>new option in cli <code>--no-verify</code> skip all correctness checks of exported LLM model</li> <li>new option in cli <code>--sample-generation-total-size</code> Number of tokens to generate in total for reference 'modes' samples npz dumped</li> <li>new option in compress quantization <code>min_max_q4_0_with_embeddings</code></li> </ul>"},{"location":"CHANGELOG/#0153-2024-10-16","title":"[0.15.3] - 2024-10-16","text":""},{"location":"CHANGELOG/#fix_28","title":"Fix","text":"<ul> <li>implicit casting of dtype in mixed tensor math ops (better strategy)</li> </ul>"},{"location":"CHANGELOG/#change_13","title":"Change","text":"<ul> <li>API of <code>llm_tract</code> compress registry functions</li> </ul>"},{"location":"CHANGELOG/#0152-2024-10-14","title":"[0.15.2] - 2024-10-14","text":""},{"location":"CHANGELOG/#fix_29","title":"Fix","text":"<ul> <li>bugs with weight_and_biases operators (linear, conv, ...) with new introduced NamedTensor</li> </ul>"},{"location":"CHANGELOG/#added_21","title":"Added","text":"<ul> <li>API to export only specific tensors</li> <li>PEFT export cli support</li> <li>maintain order in NNEF <code>custom_extensions</code> (as some tract extensions are order sensitive)</li> </ul>"},{"location":"CHANGELOG/#0151-2024-10-10","title":"[0.15.1] - 2024-10-10","text":""},{"location":"CHANGELOG/#fix_30","title":"Fix","text":"<ul> <li>edge case of interaction between QTensor and NamedTensor</li> <li>f16 mix and allclose check</li> </ul>"},{"location":"CHANGELOG/#0150-2024-10-09","title":"[0.15.0] - 2024-10-09","text":""},{"location":"CHANGELOG/#change_14","title":"Change","text":"<ul> <li>NNEF <code>variable</code> label values are now same as PyTorch module attributes naming, if Tensor are holded in any (sub-)modules</li> </ul>"},{"location":"CHANGELOG/#0140-2024-10-08","title":"[0.14.0] - 2024-10-08","text":""},{"location":"CHANGELOG/#added_22","title":"Added","text":"<ul> <li>refactor of <code>llm_tract</code> into sub-modules</li> <li>added support for <code>modes</code> IO dump and checks</li> </ul>"},{"location":"CHANGELOG/#fix_31","title":"Fix","text":"<ul> <li><code>intel</code> based <code>mac</code> tract export download correct CLI</li> <li>expand more robust</li> <li>align correctly all dimensional 'int' value as Int64</li> <li>force implicit mixed inputs dtype in PyTorch math operator to add explicit casting in exported graph</li> <li><code>Phi3</code> export correctly</li> </ul>"},{"location":"CHANGELOG/#01316-2024-10-01","title":"[0.13.16] - 2024-10-01","text":""},{"location":"CHANGELOG/#fix_32","title":"Fix","text":"<ul> <li><code>dynamic_axes</code> working for <code>Llama</code> model family</li> </ul>"},{"location":"CHANGELOG/#01315-2024-09-24","title":"[0.13.15] - 2024-09-24","text":""},{"location":"CHANGELOG/#fix_33","title":"Fix","text":"<ul> <li>slice with dyn axis edge case</li> </ul>"},{"location":"CHANGELOG/#01314-2024-09-23","title":"[0.13.14] - 2024-09-23","text":""},{"location":"CHANGELOG/#added_23","title":"Added","text":"<ul> <li>Official support tract <code>0.21.7</code></li> </ul>"},{"location":"CHANGELOG/#01313-2024-09-20","title":"[0.13.13] - 2024-09-20","text":""},{"location":"CHANGELOG/#fix_34","title":"Fix","text":"<ul> <li>Support QTensor for legacy (bellow 2.0), up to 1.12.0 &lt;= torch</li> </ul>"},{"location":"CHANGELOG/#01312-2024-09-18","title":"[0.13.12] - 2024-09-18","text":""},{"location":"CHANGELOG/#fix_35","title":"Fix","text":"<ul> <li>flexible checks</li> </ul>"},{"location":"CHANGELOG/#01311-2024-09-18","title":"[0.13.11] - 2024-09-18","text":""},{"location":"CHANGELOG/#fix_36","title":"Fix","text":"<ul> <li>Split further functionalities &amp; add some arguments as opt-in in LLM cli to add more reusable code</li> </ul>"},{"location":"CHANGELOG/#01310-2024-09-18","title":"[0.13.10] - 2024-09-18","text":""},{"location":"CHANGELOG/#fix_37","title":"Fix","text":"<ul> <li>(missfire) mkdir parents dir if needed while cache tract binary</li> </ul>"},{"location":"CHANGELOG/#0139-2024-09-17","title":"[0.13.9] - 2024-09-17","text":""},{"location":"CHANGELOG/#fix_38","title":"Fix","text":"<ul> <li>mkdir parents dir if needed while cache tract binary</li> </ul>"},{"location":"CHANGELOG/#0138-2024-09-17","title":"[0.13.8] - 2024-09-17","text":""},{"location":"CHANGELOG/#fix_39","title":"Fix","text":"<ul> <li>filter more possible stdout tract (avoid to land in stderr)</li> <li>tract inference target more robust with no subprocess shell=True and no wget needed</li> <li>in case of potential collision while merging graph and sub-graph during torch graph parsing, auto incrementation of variable name is performed</li> </ul>"},{"location":"CHANGELOG/#added_24","title":"Added","text":"<ul> <li><code>aten::linalg_norm</code> basic support for p=1 and p=2 added</li> </ul>"},{"location":"CHANGELOG/#0137-2024-09-16","title":"[0.13.7] - 2024-09-16","text":""},{"location":"CHANGELOG/#fix_40","title":"Fix","text":"<ul> <li><code>export_llm_to_tract</code> API underlying no more need hugging face slug if only local dir.</li> <li><code>export_llm_to_tract</code> log error if IO check wrong.</li> </ul>"},{"location":"CHANGELOG/#0136-2024-09-11","title":"[0.13.6] - 2024-09-11","text":""},{"location":"CHANGELOG/#fix_41","title":"Fix","text":"<ul> <li><code>export_llm_to_tract</code> export cli more modular and reusable fn's</li> </ul>"},{"location":"CHANGELOG/#0135-2024-09-11","title":"[0.13.5] - 2024-09-11","text":""},{"location":"CHANGELOG/#fix_42","title":"Fix","text":"<ul> <li><code>f16</code> export of LLM more stable (LayerNorm handling)</li> <li>more robust <code>export_llm_to_tract</code> export cli (+ full tokenizer, config export)</li> </ul>"},{"location":"CHANGELOG/#0134-2024-09-09","title":"[0.13.4] - 2024-09-09","text":""},{"location":"CHANGELOG/#fix_43","title":"Fix","text":"<ul> <li><code>f16</code> export of LLM export correctly</li> <li><code>Q4_0</code> accurately serialize to tract</li> </ul>"},{"location":"CHANGELOG/#0133-2024-09-05","title":"[0.13.3] - 2024-09-05","text":""},{"location":"CHANGELOG/#change_15","title":"Change","text":"<ul> <li>QTensor inherit now from torch.Tensor and support any weight sharing</li> </ul>"},{"location":"CHANGELOG/#0132-2024-08-27","title":"[0.13.2] - 2024-08-27","text":""},{"location":"CHANGELOG/#fix_44","title":"Fix","text":"<ul> <li>add missing <code>arm64</code> in arch64 for tract downloader</li> </ul>"},{"location":"CHANGELOG/#0131-2024-08-26","title":"[0.13.1] - 2024-08-26","text":""},{"location":"CHANGELOG/#added_25","title":"Added","text":"<ul> <li><code>tract_llm</code> with various tract target support</li> </ul>"},{"location":"CHANGELOG/#change_16","title":"Change","text":"<ul> <li>refactor <code>renaming_scheme</code> -&gt; <code>nnef_variable_naming_scheme</code></li> </ul>"},{"location":"CHANGELOG/#fix_45","title":"Fix","text":"<ul> <li>few remaining <code>nnef_spec_strict</code> replaced</li> <li>logger.warning for unofficially supported inference target fixed</li> </ul>"},{"location":"CHANGELOG/#0130-2024-08-22","title":"[0.13.0] - 2024-08-22","text":""},{"location":"CHANGELOG/#added_26","title":"Added","text":"<ul> <li>Support for explicit <code>InferenceTarget</code> in core function <code>export_model_to_nnef</code> (so far 2 variants: <code>KhronosNNEF</code> and <code>TractNNEF</code>)</li> <li>Added <code>KhronosNNEF</code> test suite based on nnef-tool interpreter</li> <li>In case of <code>TractNNEF</code> binary management is handled internally (no more system wide <code>tract</code> reference)</li> </ul>"},{"location":"CHANGELOG/#change_17","title":"Change","text":"<ul> <li>refactor tract within inference_target</li> <li>refactor module \"primitives\" as \"aten\"</li> <li>refactor class \"NamedItemOrderedSet\" as \"ReactiveNamedItemDict\"</li> <li>updated README in accordance with new exposed API</li> </ul>"},{"location":"CHANGELOG/#0123-2024-08-21","title":"[0.12.3] - 2024-08-21","text":""},{"location":"CHANGELOG/#added_27","title":"Added","text":"<ul> <li>support for all variants of <code>torch.nn.functional.scaled_dot_product_attention</code></li> <li>add GELU with <code>tanh</code> approximation option</li> <li>slice with out of bound reformulation, to allow tract to work (ie. [-100:] on a 50 size dim)</li> <li>new LLM pass: <code>Mistral</code> is passing, <code>Gemma2</code> pass but some IO diff</li> </ul>"},{"location":"CHANGELOG/#0122-2024-08-19","title":"[0.12.2] - 2024-08-19","text":""},{"location":"CHANGELOG/#added_28","title":"Added","text":"<ul> <li>refactor NNEF variable naming in a ir_naming in module aside</li> <li>new NNEF variable naming scheme <code>natural_verbose_camel</code></li> <li>added export IO support for dict/list/tuple of torch.Tensor via flattening</li> <li>added export IO support for other object via constantization (not part of graph <code>external</code>)</li> </ul>"},{"location":"CHANGELOG/#0121-2024-08-09","title":"[0.12.1] - 2024-08-09","text":""},{"location":"CHANGELOG/#added_29","title":"Added","text":"<ul> <li>tract <code>Q4_0</code> support</li> <li>new <code>llm_tract</code> extension installable with <code>pip install torch_to_nnef[llm_tract]</code></li> <li>hold cli <code>export_llm_to_tract</code> for direct LLM export from any huggingface model with optional quant</li> <li>replace <code>scripts</code> dir at root of the project</li> <li>added support for Python 3.12</li> </ul>"},{"location":"CHANGELOG/#removed","title":"Removed","text":"<ul> <li>dropped support for Python 3.8</li> <li>dropped support for unused QTensor formats</li> </ul>"},{"location":"CHANGELOG/#0113-2024-07-26","title":"[0.11.3] - 2024-07-26","text":""},{"location":"CHANGELOG/#added_30","title":"Added","text":"<ul> <li>Tested export for <code>Llama</code>,<code>openELM</code>, <code>Phi</code>  LLM family works</li> <li>Added support aten::ops : <code>tril</code>, <code>repeat_interleave</code>, <code>type_as</code></li> <li>Variable naming scheme: old <code>natural_verbose</code> option renamed <code>raw</code>,  new option <code>natural_verbose</code> means 'as close as possible' to torch Python code</li> <li>Protection against variable naming collision with <code>input_names</code>, <code>output_names</code></li> <li>Updated NNEF <code>extensions</code> to comply to tract expectations</li> </ul>"},{"location":"CHANGELOG/#fix_46","title":"Fix","text":"<ul> <li>Improved support aten::ops : <code>index_</code> multi index gathering, <code>masked_fill</code>, <code>ones_like</code></li> <li>added naming for models unit-tests, 'useful' in case of failures</li> <li>Compliance with tract&gt;0.21.3 (introduced more restrictive definition within NNEF with different notation of scalar between float and TDim/long/int )</li> <li>Substantial performance improvement for internals graph IR (via by example new data-structures:  <code>NamedItemOrderedSet</code>)</li> </ul>"},{"location":"CHANGELOG/#0102-2024-06-21","title":"[0.10.2] - 2024-06-21","text":""},{"location":"CHANGELOG/#fix_47","title":"Fix","text":"<ul> <li>squeeze after getting shape slice to get scalar (specific to tract to get rank 0)</li> </ul>"},{"location":"CHANGELOG/#0101-2024-04-19","title":"[0.10.1] - 2024-04-19","text":""},{"location":"CHANGELOG/#fix_48","title":"Fix","text":"<ul> <li>better dynamic shape handling: remove realized shape from IR and adapt translation of slice accordingly</li> </ul>"},{"location":"CHANGELOG/#0100-2024-04-17","title":"[0.10.0] - 2024-04-17","text":""},{"location":"CHANGELOG/#removed_1","title":"Removed","text":"<ul> <li>drop python 3.7.0 support</li> </ul>"},{"location":"CHANGELOG/#added_31","title":"Added","text":"<ul> <li>added <code>triu</code> export support</li> <li>script to export Llama2</li> </ul>"},{"location":"CHANGELOG/#fix_49","title":"Fix","text":"<ul> <li>Support aten::ops : <code>ones_like</code>, <code>zeros_like</code>, <code>arange</code> with dynamic shape</li> <li>Support aten::ops: <code>expand</code>, <code>masked_fill</code> with dynamic shape (but no tract support)</li> <li>more unit test of primitive</li> <li>fix poetry source pypi</li> </ul>"},{"location":"CHANGELOG/#091-2024-04-04","title":"[0.9.1] - 2024-04-04","text":""},{"location":"CHANGELOG/#removed_2","title":"Removed","text":"<ul> <li>drop python 3.7.0 support</li> <li>updated tract version tested against: 0.19.16, 0.20.22, 0.21.3</li> </ul>"},{"location":"CHANGELOG/#added_32","title":"Added","text":"<ul> <li>(alpha) <code>scripts/generate_qtensor_gguf_matmul.py</code> to generate unit tests with GGUF format for tract</li> <li>(alpha) <code>[gguf]</code> feature gate to support export to GGUF format and quantization</li> <li>(alpha) Support 2 new quantization tensor type (implemented as module for now):</li> <li><code>QTensorGGUF</code> support almost all GGUF data types -&gt; with export prototype working</li> <li><code>QTensorSepParamsWithPack</code> more flexible than GGUF format, with support of classical per group with different sizes, per channel, per weight quantisation scheme at different bit-width 1, 2, 3, 4, 8 (useful for experimentation/accuracy simulation)</li> <li>move <code>[dev]</code> dependencies as a poetry group, to avoid exposition as packaged optional feature</li> <li>new <code>torch_version()</code> and <code>tract_version()</code> utility functions now allows for direct comparison to string version \"<code>X.Y.Z</code>\"</li> <li>Updated all tests packages torch/torch_audio/..., to torch <code>2.2</code> compatible \ud83c\udf89</li> <li>added <code>weight_norm</code> export support</li> </ul>"},{"location":"CHANGELOG/#fix_50","title":"Fix","text":"<ul> <li>support for latest scaled_dot_product_attention aten version (last PyTorch version)</li> <li>quantization of bias as i32 at export for better support in tract (checked accuracy no-regression on bigger model)</li> <li>additional test for quantization with PyTorch different inputs q params activated (since last tract version merged related PR)</li> <li>custom_extractors have been refactored into sub-modules</li> </ul>"},{"location":"CHANGELOG/#0811-2024-03-04","title":"[0.8.11] - 2024-03-04","text":""},{"location":"CHANGELOG/#fix_51","title":"Fix","text":"<ul> <li><code>linear</code>, <code>conv</code>, quantized operators accurately export bias to tract</li> <li><code>activations</code>, quantized operators export output tensor scale / offset</li> </ul>"},{"location":"CHANGELOG/#0810-2024-02-23","title":"[0.8.10] - 2024-02-23","text":""},{"location":"CHANGELOG/#added_33","title":"Added","text":"<ul> <li><code>add</code>, <code>mul</code>, <code>div</code> element wise operators for quantized elements</li> </ul>"},{"location":"CHANGELOG/#fix_52","title":"Fix","text":"<ul> <li><code>deconv</code> with group now export correctly to tract</li> </ul>"},{"location":"CHANGELOG/#089-2024-01-16","title":"[0.8.9] - 2024-01-16","text":""},{"location":"CHANGELOG/#added_34","title":"Added","text":"<ul> <li><code>tract_core_external</code> in case of graph input being not i64, nor f32</li> </ul>"},{"location":"CHANGELOG/#088-2023-11-29","title":"[0.8.8] - 2023-11-29","text":""},{"location":"CHANGELOG/#fix_53","title":"Fix","text":"<ul> <li><code>rnn</code> states can now be manipulated in graph even in <code>multi-layers</code></li> </ul>"},{"location":"CHANGELOG/#087-2023-11-29","title":"[0.8.7] - 2023-11-29","text":""},{"location":"CHANGELOG/#fix_54","title":"Fix","text":"<ul> <li><code>rnn</code> states can now be manipulated in graph</li> <li><code>dynamic_axes</code> with tensor construction such as <code>zeros</code>, <code>ones</code> (and all related variants) now produce correct dynamic matrix</li> </ul>"},{"location":"CHANGELOG/#086-2023-10-27","title":"[0.8.6] - 2023-10-27","text":""},{"location":"CHANGELOG/#fix_55","title":"Fix","text":"<ul> <li><code>tract_core</code> NNEF extension added when using slice with dynamic_axes (to use <code>tract_core_shape_of</code>)</li> <li><code>python 3.7</code> is now authorized again for the package even if no more supported</li> </ul>"},{"location":"CHANGELOG/#added_35","title":"Added","text":"<ul> <li><code>tract_extra</code> is added parameters to tract when running <code>check_same_io_as_tract</code> starting at tract 0.20.20</li> </ul>"},{"location":"CHANGELOG/#085-2023-09-12","title":"[0.8.5] - 2023-09-12","text":""},{"location":"CHANGELOG/#added_36","title":"Added","text":"<ul> <li><code>PyTorch</code> v2 support</li> <li>Python <code>3.7</code> no more tested/supported as it is deprecated</li> <li>Support Python <code>3.8</code> to <code>3.11</code> tested/supported</li> </ul>"},{"location":"CHANGELOG/#084-2023-08-28","title":"[0.8.4] - 2023-08-28","text":""},{"location":"CHANGELOG/#fix_56","title":"Fix","text":"<ul> <li>In case of <code>RNN</code>,<code>GRU</code>,<code>LSTM</code> we expand explicitly state initializers to batch dimensions (helping tract in case of some <code>dynamic_axes</code> graph formulation)</li> <li>Refactor of <code>torch_graph</code> module in sub-modules</li> </ul>"},{"location":"CHANGELOG/#added_37","title":"Added","text":"<ul> <li><code>hstack</code> and <code>vstack</code> support</li> <li><code>unflatten</code> support</li> <li><code>einsum</code> support</li> </ul>"},{"location":"CHANGELOG/#082-2023-08-02","title":"[0.8.2] - 2023-08-02","text":""},{"location":"CHANGELOG/#fix_57","title":"Fix","text":"<ul> <li>slice with end being dynamic (akka max dimension size) given tract export target and dynamic_axes enabled</li> </ul>"},{"location":"CHANGELOG/#081-2023-08-01","title":"[0.8.1] - 2023-08-01","text":""},{"location":"CHANGELOG/#fix_58","title":"Fix","text":"<ul> <li>fail if <code>tract</code> binary not found but <code>check_same_io_as_tract=True</code></li> <li>better tract handling when <code>check_same_io_as_tract</code></li> <li>disable fft's tests for now</li> </ul>"},{"location":"CHANGELOG/#added_38","title":"Added","text":"<ul> <li><code>Llama</code> partial export</li> <li><code>_convolution_mode</code> aten operator (padding same and valid)</li> </ul>"},{"location":"CHANGELOG/#080-2023-05-01","title":"[0.8.0] - 2023-05-01","text":""},{"location":"CHANGELOG/#added_39","title":"Added","text":"<ul> <li>Refactored internals in primitive/quantized with submodule and registries</li> <li><code>relu6</code>, <code>hardswish</code> activations</li> </ul>"},{"location":"CHANGELOG/#fix_59","title":"Fix","text":"<ul> <li>Support tract 0.19.15</li> <li>Support tract 0.20.4</li> </ul>"},{"location":"CHANGELOG/#removed_3","title":"Removed","text":"<ul> <li>deprecated support tract 0.17 (we support only last 3 majors)</li> <li>deprecated support of fft's ops prior to tract 0.20.0</li> </ul>"},{"location":"CHANGELOG/#077-2023-02-20","title":"[0.7.7] - 2023-02-20","text":""},{"location":"CHANGELOG/#added_40","title":"Added","text":"<ul> <li>add <code>narrow</code> support</li> <li>fix <code>copy</code> should not be used for tract</li> <li><code>tile</code> akka expand allow dynamic dimension as repeat</li> </ul>"},{"location":"CHANGELOG/#076-2023-01-25","title":"[0.7.6] - 2023-01-25","text":""},{"location":"CHANGELOG/#added_41","title":"Added","text":"<ul> <li>complex support for <code>abs</code></li> <li><code>log10</code> ops supported</li> <li><code>torchaudio.transform.MelSpectrogram</code> supported out of the box</li> </ul>"},{"location":"CHANGELOG/#075-2023-01-23","title":"[0.7.5] - 2023-01-23","text":""},{"location":"CHANGELOG/#added_42","title":"Added","text":"<ul> <li><code>stft</code>, <code>fft</code>, <code>ifft</code> and basic complex number manipulations, torch now export to nnef with tract core experimental   implementation in 0.19.0</li> </ul>"},{"location":"CHANGELOG/#074-2023-01-18","title":"[0.7.4] - 2023-01-18","text":""},{"location":"CHANGELOG/#fix_60","title":"Fix","text":"<ul> <li>Avoid global log config setting in export module (restrict it to test)</li> </ul>"},{"location":"CHANGELOG/#073-2023-01-12","title":"[0.7.3] - 2023-01-12","text":""},{"location":"CHANGELOG/#fix_61","title":"Fix","text":"<ul> <li><code>aten:Int</code> catched even if not part of a list</li> <li>In case a float or an int is too big it use exponential notation and may trunk   part of the number at serialization by example: <code>torch.finfo(self.dtype).min</code>   (from huggingface transformers lib).</li> </ul>"},{"location":"CHANGELOG/#added_43","title":"Added","text":"<ul> <li><code>embedding</code> operator</li> <li><code>Albert</code> model is passing</li> </ul>"},{"location":"CHANGELOG/#072-2023-01-11","title":"[0.7.2] - 2023-01-11","text":""},{"location":"CHANGELOG/#fix_62","title":"Fix","text":"<ul> <li>dynamic_axes generated stream variables should be better casted to NNEF tensor ref</li> </ul>"},{"location":"CHANGELOG/#071-2023-01-10","title":"[0.7.1] - 2023-01-10","text":""},{"location":"CHANGELOG/#added_44","title":"Added","text":"<ul> <li><code>roll</code>, <code>new_zeros</code>, <code>zeros</code> operators</li> <li><code>pow</code> operator now support negative and scalars as exponent</li> </ul>"},{"location":"CHANGELOG/#fix_63","title":"Fix","text":"<ul> <li><code>rsub</code> &amp; <code>remainder</code> operator with constant should be precomputed output constants</li> <li><code>avg_pool1d</code>, <code>avg_pool2d</code> operators now work as expected</li> </ul>"},{"location":"CHANGELOG/#0610-2022-11-07","title":"[0.6.10] - 2022-11-07","text":""},{"location":"CHANGELOG/#fix_64","title":"Fix","text":"<ul> <li><code>aten:floor_divide</code> new op from torch 1.13 (torch 1.13 is passing)</li> </ul>"},{"location":"CHANGELOG/#069-2022-11-04","title":"[0.6.9] - 2022-11-04","text":""},{"location":"CHANGELOG/#fix_65","title":"Fix","text":"<ul> <li><code>aten:size</code> fix lost context for dyn shapes</li> </ul>"},{"location":"CHANGELOG/#068-2022-10-31","title":"[0.6.8] - 2022-10-31","text":""},{"location":"CHANGELOG/#fix_66","title":"Fix","text":"<ul> <li><code>aten:size</code> expand is now consistant in nameing pattern and should be more   robust</li> </ul>"},{"location":"CHANGELOG/#067-2022-10-31","title":"[0.6.7] - 2022-10-31","text":""},{"location":"CHANGELOG/#fix_67","title":"Fix","text":"<ul> <li><code>aten:size</code> case with negative index is now translated correctly</li> <li><code>...-pre</code> tract version are now handled correctly</li> </ul>"},{"location":"CHANGELOG/#066-2022-10-21","title":"[0.6.6] - 2022-10-21","text":""},{"location":"CHANGELOG/#fix_68","title":"Fix","text":"<ul> <li>Handle case with no tract binary found ( thanks to Theo  )</li> </ul>"},{"location":"CHANGELOG/#065-2022-10-20","title":"[0.6.5] - 2022-10-20","text":""},{"location":"CHANGELOG/#fix_69","title":"Fix","text":"<ul> <li>Missing use of SONOS infra</li> </ul>"},{"location":"CHANGELOG/#064-2022-10-20","title":"[0.6.4] - 2022-10-20","text":""},{"location":"CHANGELOG/#fix_70","title":"Fix","text":"<ul> <li>Push to SONOS repo as well</li> </ul>"},{"location":"CHANGELOG/#063-2022-10-19","title":"[0.6.3] - 2022-10-19","text":""},{"location":"CHANGELOG/#fix_71","title":"Fix","text":"<ul> <li><code>round</code> operator is now following tract core IEE implementation and warn if vanilla NNEF version is used</li> <li><code>ipdb</code> is no more a dependency of this package</li> <li>bump to black formatter v22 (to avoid click raising errors)</li> <li>support tract &gt; v0.18.0 (changed Conv1d bias expected shapes)</li> </ul>"},{"location":"CHANGELOG/#061-2022-09-27","title":"[0.6.1] - 2022-09-27","text":"<ul> <li><code>baddbmm</code> operator is supported</li> </ul>"},{"location":"CHANGELOG/#fix_72","title":"Fix","text":"<ul> <li>all small fixes to have torch_to_nnef works with torch 1.12.0 and beyond (keeping backward compatibility)</li> </ul>"},{"location":"CHANGELOG/#060-2022-09-27","title":"[0.6.0] - 2022-09-27","text":""},{"location":"CHANGELOG/#added_45","title":"Added","text":"<ul> <li><code>nnef_spec_strict</code> option in <code>export</code> allows to export strict the NNEF spec compliant model.</li> <li><code>select</code>, <code>group_norm</code>, <code>erf</code> operators are supported.</li> <li><code>gelu</code> was rewritten with <code>erf</code> fragment for precision.</li> <li><code>ConvTasNet</code> is supported.</li> <li><code>Wav2Vec2</code> encoder is supported.</li> <li><code>VisionTransformer</code> (ViT) is supported.</li> </ul>"},{"location":"CHANGELOG/#fix_73","title":"Fix","text":"<ul> <li>negative index in <code>slice</code> are now handled for fixed dimensions</li> </ul>"},{"location":"CHANGELOG/#change_18","title":"Change","text":"<ul> <li><code>Exceptions</code> are now unified under T2NError</li> </ul>"},{"location":"CHANGELOG/#053-2022-09-08","title":"[0.5.3] - 2022-09-08","text":""},{"location":"CHANGELOG/#change_19","title":"Change","text":"<ul> <li>naming exported file with <code>.nnef</code> is no more required</li> </ul>"},{"location":"CHANGELOG/#052-2022-09-06","title":"[0.5.2] - 2022-09-06","text":""},{"location":"CHANGELOG/#change_20","title":"Change","text":"<ul> <li>update <code>nnef</code> deps with real original dir since poetry now support subdirectory</li> <li>tract v0.17.7 should make the CI tests pass again</li> </ul>"},{"location":"CHANGELOG/#051-2022-08-17","title":"[0.5.1] - 2022-08-17","text":""},{"location":"CHANGELOG/#change_21","title":"Change","text":"<ul> <li>update <code>nnef</code> deps</li> </ul>"},{"location":"CHANGELOG/#050-2022-08-16","title":"[0.5.0] - 2022-08-16","text":""},{"location":"CHANGELOG/#change_22","title":"Change","text":"<ul> <li><code>aten:size</code> is now transformed in <code>tract_core_shape_of</code> which is against NNEF   protocol specification but allow 'more' dynamic network to be expressed</li> <li><code>aten:reshape</code> allow symbolic dims as parameters</li> </ul>"},{"location":"CHANGELOG/#040-2022-07-20","title":"[0.4.0] - 2022-07-20","text":""},{"location":"CHANGELOG/#added_46","title":"Added","text":"<ul> <li><code>tensor.norm</code> with p 1 or 2</li> <li><code>tensor.clamp_min(float)</code> and <code>tensor.clamp_max(float)</code></li> </ul>"},{"location":"CHANGELOG/#fix_74","title":"Fix","text":"<ul> <li>fix nn.MultiHeadAttention case (not self attention) allow to export Transpose</li> </ul>"},{"location":"CHANGELOG/#change_23","title":"Change","text":"<ul> <li>torch quantize op lead to explicit <code>tract_core_cast</code> now</li> </ul>"},{"location":"CHANGELOG/#034-2022-05-06","title":"[0.3.4] - 2022-05-06","text":""},{"location":"CHANGELOG/#fix_75","title":"Fix","text":"<ul> <li>expand can be expressed with negative values and repeat within rank dim</li> <li>Conformer Architecture now export correctly regardless the number of Attention Head</li> </ul>"},{"location":"CHANGELOG/#033-2022-05-02","title":"[0.3.3] - 2022-05-02","text":""},{"location":"CHANGELOG/#fix_76","title":"Fix","text":"<ul> <li>Quantization info are passed correctly in case of type neutral information   like ((un)squeeze, transpose, split).</li> <li>Dequantize is applied as a forced cast</li> </ul>"},{"location":"CHANGELOG/#032-2022-04-29","title":"[0.3.2] - 2022-04-29","text":""},{"location":"CHANGELOG/#fix_77","title":"Fix","text":"<ul> <li> <p>Arity was not properly tracked in some Subgraph expansion when parameter where   flipped during torch optimization process (that modified ordering), this lead   to wrong matching between io of graph and subgraph during recursive process.</p> </li> <li> <p>Div with an int type was not possible to cast implicitly to float by tract, to   avoid rounding behavior missmatch we did had casting wrapper to handle such   usecase properly.</p> </li> </ul>"},{"location":"CHANGELOG/#added_47","title":"Added","text":"<ul> <li>Better collected environment with OS, GCC, python and more package info</li> <li>Export Q8 Conv{1,2}d and Linear</li> <li>In Quantized network use scale/zero point of weight &amp; input for bias export</li> </ul>"},{"location":"CHANGELOG/#031-2022-04-22","title":"[0.3.1] - 2022-04-22","text":""},{"location":"CHANGELOG/#fix_78","title":"Fix","text":"<ul> <li>LogSofmax with negative value #9</li> <li>switch-on cast test</li> </ul>"},{"location":"CHANGELOG/#added_48","title":"Added","text":"<ul> <li><code>dynamic_axes</code> in export API allowing to handle streaming dimensions</li> <li>Added aten::ops : <code>stack</code>, <code>unbind</code>,</li> <li>Filter <code>slice</code> if applied without effect (slice on full range)</li> </ul>"},{"location":"CHANGELOG/#030-2022-04-13","title":"[0.3.0] - 2022-04-13","text":""},{"location":"CHANGELOG/#fix_79","title":"Fix","text":"<ul> <li>Rank expansion done right (<code>TRUnet</code> normalisations options works)</li> <li>TorchTrace optimization may from time to time change signature of <code>nn.Module</code> so we needed to take it into account in <code>torch_to_nnef.torch_graph</code> module.</li> <li>NNEF fragments file now express with their own extensions, this allows for finer   grain export notation</li> <li>macos-latest OS removed from matrix test in CI since we have limited use (  we will re-add it once tract latest version will be out )</li> </ul>"},{"location":"CHANGELOG/#added_49","title":"Added","text":"<ul> <li>Added aten::ops : <code>zeros_like</code>, <code>ones</code>, <code>expand</code>, <code>GLU</code>, <code>split</code>, <code>arange</code>, <code>chunk</code>, <code>layer_norm</code>, <code>trunc</code>, <code>masked_fill</code>, <code>clamp</code>, <code>to</code></li> <li>Ability to export and unit-tested: <code>Deepspeech</code>, <code>Conformer</code></li> <li>Ability to export <code>Wavenet</code>, <code>TDNN-ECAPA</code></li> <li>Added LSTM with <code>projection</code></li> </ul>"},{"location":"CHANGELOG/#022-2022-04-04","title":"[0.2.2] - 2022-04-04","text":""},{"location":"CHANGELOG/#fix_80","title":"Fix","text":"<ul> <li>Fix base TRUNet</li> <li>Expose renaming scheme</li> <li>Add id to unittest for easier debug</li> </ul>"},{"location":"CHANGELOG/#021-2022-03-31","title":"[0.2.1] - 2022-03-31","text":""},{"location":"CHANGELOG/#fix_81","title":"Fix","text":"<ul> <li>try correct parse with release workflow</li> </ul>"},{"location":"CHANGELOG/#020-2022-03-30","title":"[0.2.0] - 2022-03-30","text":""},{"location":"CHANGELOG/#added_50","title":"Added","text":"<ul> <li>Hook system on modules (allowing to avoid jit.trace expansion replaced by custom code )</li> <li>py.test Tract IO complaints added to errors</li> <li>better test representation</li> <li>LSTM/GRU/RNN handled (excepted LSTM with projection)</li> <li>Hard tanh</li> <li>ISO with tract check</li> <li>Logging with log level exposed</li> <li>TRUNet export</li> <li>debug bundling opt-in</li> <li>Numerous operators</li> <li>Q8 errors explorations</li> </ul>"},{"location":"CHANGELOG/#014-2022-03-17","title":"[0.1.4] - 2022-03-17","text":""},{"location":"CHANGELOG/#fixed_1","title":"Fixed","text":"<ul> <li>CI calibration finished</li> </ul>"},{"location":"CHANGELOG/#011-2022-03-17","title":"[0.1.1] - 2022-03-17","text":""},{"location":"CHANGELOG/#added_51","title":"Added","text":"<ul> <li>Support basic models conversion (if there is not quantized layers nor LSTM in it)</li> <li>CI is working with appropriate test suite (PyTorch-&gt;Tract ISO IO checked for ~80 cases)</li> <li>variable renaming scheme to keep NNEF generated files short</li> </ul>"},{"location":"CHANGELOG/#010-2022-02-28","title":"[0.1.0] - 2022-02-28","text":"<ul> <li>First release on Sonos Org.</li> </ul>"},{"location":"backstory/","title":"Backstory","text":"<p>At Sonos, we\u2019ve long been committed to building machine learning solutions that span the full pipeline\u2014from training all the way to inference\u2014delivering smart experiences to millions of users. A core part of this journey has been enabling on-device neural network computation, a challenging but critical step in making our products more responsive, private, and reliable.</p> <p>To support this goal, we developed and open-sourced Tract, a neural network inference engine written entirely in Rust. From the very beginning, Tract has used the NNEF (Neural Network Exchange Format) as its intermediate model representation. NNEF offered the right mix of practical benefits: it loads quickly, is human-readable, and is easy to extend\u2014ideal for fast iteration and debugging. (We go into more detail about our choice in Why NNEF.)</p> <p>Fast forward to early 2022. Like many others working on edge ML, we started experimenting with compression techniques\u2014especially quantization. As models ballooned in size and complexity, the need to optimize for limited on-device resources became urgent. We knew that exporting quantized models was going to be essential to keep up with the scale and performance constraints of modern deployments.</p> <p>However, when we looked at the state of ONNX at the time\u2014especially its approach to quantization via QDQ (Quantize-Dequantize)\u2014we found it lacking. It felt rigid and tacked on, not something we could easily extend or trust to support our long-term needs. So we took a shot at doing it differently.</p> <p>In just two weeks, we built a prototype that exported quantization-aware trained (QAT) models from PyTorch directly into NNEF, fully readable by Tract. The result was eye-opening. We could now debug and extend graph operators without compiling protobuf or navigating through opaque binary formats. Everything was right there in a clean, readable NNEF file. It unlocked a new level of agility and transparency in our workflow, and we haven\u2019t looked back since.</p> <p>What started as a small side experiment turned into a powerful internal capability\u2014one that continues to shape how we build and deploy machine learning models at Sonos.</p>"},{"location":"export_api/","title":"Main export API's","text":"","boost":2},{"location":"export_api/#torch_to_nnef.export","title":"torch_to_nnef.export","text":"","boost":2},{"location":"export_api/#torch_to_nnef.export.export_model_to_nnef","title":"export_model_to_nnef","text":"<pre><code>export_model_to_nnef(model: torch.nn.Module, args, file_path_export: T.Union[Path, str], inference_target: InferenceTarget, input_names: T.Optional[T.List[str]] = None, output_names: T.Optional[T.List[str]] = None, compression_level: int = 0, log_level: int = log.INFO, nnef_variable_naming_scheme: VariableNamingScheme = DEFAULT_VARNAME_SCHEME, check_io_names_qte_match: bool = True, debug_bundle_path: T.Optional[Path] = None, custom_extensions: T.Optional[T.List[str]] = None)\n</code></pre> <p>Main entrypoint of this library.</p> <p>Export any torch.nn.Module to NNEF file format archive</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>a nn.Module that have a <code>.forward</code> function with only tensor arguments and outputs (no tuple, list, dict or objects) Only this function will be serialized</p> required <code>args</code> <p>a flat ordered list of tensors for each forward inputs of <code>model</code> this list can not be of dynamic size (at serialization it will be fixed to quantity of tensor provided) WARNING! tensor size in args will increase export time so take that in consideration for dynamic axes</p> required <code>file_path_export</code> <code>Union[Path, str]</code> <p>a Path to the exported NNEF serialized model archive. It must by convention end with <code>.nnef.tgz</code> suffixes</p> required <code>inference_target</code> <code>InferenceTarget</code> <p>can be <code>torch_to_nnef.TractNNEF</code> or <code>torch_to_nnef.KhronosNNEF</code> for each you can specify version targeted: - KhronosNNEF is the least maintained so far,     and is checked against nnef-tools PyTorch interpreter - TractNNEF is our main focus at SONOS,   it is checked against tract inference engine   among key paramters there is     feature_flags: Optional[Set[str]],     that may contains tract specifics     dynamic_axes: Optional       By default the exported model will have       the shapes of all input and output tensors set       to exactly match those given in args.       To specify axes of tensors as dynamic       (i.e. known only at runtime)       set dynamic_axes to a dict with schema:           KEY (str):             an input or output name. Each name must also             be provided in input_names or output_names.           VALUE (dict or list): If a dict, keys are axis indices             and values are axis names. If a list, each element is             an axis index.</p> required <code>specific_tract_binary_path</code> <p>Optional[Path] ideal to check io against new tract versions</p> required <code>input_names</code> <code>Optional[List[str]]</code> <p>Optional list of names for args, it replaces variable inputs names traced from graph (if set it must have the same size as number of args)</p> <code>None</code> <code>output_names</code> <code>Optional[List[str]]</code> <p>Optional list of names for outputs of <code>model.forward</code>, it replaces variable output names traced from graph (if set it must have the same size as number of outputs)</p> <code>None</code> <code>compression_level</code> <code>int</code> <p>int (&gt;= 0) compression level of tar.gz (higher is more compressed)</p> <code>0</code> <code>log_level</code> <code>int</code> <p>int, logger level for <code>torch_to_nnef</code> following Python standard logging level can be set to: INFO, WARN, DEBUG ...</p> <code>INFO</code> <code>nnef_variable_naming_scheme</code> <code>VariableNamingScheme</code> <p>Possible choices NNEF variables naming schemes are: - \"raw\": Taking variable names from traced graph debugName directly - \"natural_verbose\": that try to provide nn.Module exported   variable naming consistency - \"natural_verbose_camel\": that try to provide nn.Module exported   variable naming consistency but with more consice camelCase   variable pattern - \"numeric\": that try to be as concise as possible</p> <code>DEFAULT_VARNAME_SCHEME</code> <code>check_io_names_qte_match</code> <code>bool</code> <p>(default: True) During the tracing process of the torch graph One or more input provided can be removed if not contributing to generate outputs while check_io_names_qte_match is True we ensure that this input and output quantity remain constant with numbers in <code>input_names</code> and <code>output_names</code>.</p> <code>True</code> <code>debug_bundle_path</code> <code>Optional[Path]</code> <p>Optional[Path] if specified it should create an archive bundle with all needed information to allow easier debug.</p> <code>None</code> <code>custom_extensions</code> <code>Optional[List[str]]</code> <p>Optional[List[str]] allow to add a set of extensions as defined in (https://registry.khronos.org/NNEF/specs/1.0/nnef-1.0.5.html) Useful to set specific extensions like for example: 'extension tract_assert S &gt;= 0' those assertion allows to add limitation on dynamic shapes that are not expressed in traced graph (like for example maximum number of tokens for an LLM)</p> <code>None</code> <p>Examples:</p> <p>For example this function can be used to export as simple perceptron model:</p> <pre><code>&gt;&gt;&gt; import os\n&gt;&gt;&gt; import tarfile\n&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; mod = nn.Sequential(nn.Linear(1, 5), nn.ReLU())\n&gt;&gt;&gt; export_path = tempfile.mktemp(suffix=\".nnef.tgz\")\n&gt;&gt;&gt; inference_target = TractNNEF.latest()\n&gt;&gt;&gt; export_model_to_nnef(\n...   mod,\n...   torch.rand(3, 1),\n...   export_path,\n...   inference_target,\n...   input_names=[\"inp\"],\n...   output_names=[\"out\"]\n... )\n&gt;&gt;&gt; os.chdir(export_path.rsplit(\"/\", maxsplit=1)[0])\n&gt;&gt;&gt; tarfile.open(export_path).extract(\"graph.nnef\")\n&gt;&gt;&gt; \"graph network(inp) -&gt; (out)\" in open(\"graph.nnef\").read()\nTrue\n</code></pre>","boost":2},{"location":"export_api/#torch_to_nnef.export.export_tensors_from_disk_to_nnef","title":"export_tensors_from_disk_to_nnef","text":"<pre><code>export_tensors_from_disk_to_nnef(store_filepath: T.Union[Path, str], output_dir: T.Union[Path, str], filter_key: T.Optional[T.Callable[[str], bool]] = None, fn_check_found_tensors: T.Optional[T.Callable[[T.Dict[str, _Tensor]], bool]] = None) -&gt; T.Dict[str, _Tensor]\n</code></pre> <p>Export any statedict or safetensors file torch.Tensors to NNEF .dat file.</p> <p>Parameters:</p> Name Type Description Default <code>store_filepath</code> <code>Union[Path, str]</code> <p>the filepath that hold the .safetensors , .pt or .bin containing the state dict</p> required <code>output_dir</code> <code>Union[Path, str]</code> <p>directory to dump the NNEF tensor .dat files</p> required <code>filter_key</code> <code>Optional[Callable[[str], bool]]</code> <p>An optional function to filter specific keys to be exported</p> <code>None</code> <code>fn_check_found_tensors</code> <code>Optional[Callable[[Dict[str, _Tensor]], bool]]</code> <p>post checking function to ensure all requested tensors have effectively been dumped</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, _Tensor]</code> <p>a dict of tensor name as key and torch.Tensor values, identical to <code>torch_to_nnef.export.export_tensors_to_nnef</code></p> <p>Examples:</p> <p>Simple filtered example</p> <pre><code>&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; class Mod(nn.Module):\n...     def __init__(self):\n...         super().__init__()\n...         self.a = nn.Linear(1, 5)\n...         self.b = nn.Linear(5, 1)\n...\n...     def forward(self, x):\n...         return self.b(self.a(x))\n&gt;&gt;&gt; mod = Mod()\n&gt;&gt;&gt; pt_path = tempfile.mktemp(suffix=\".pt\")\n&gt;&gt;&gt; nnef_dir = tempfile.mkdtemp(suffix=\"_nnef\")\n&gt;&gt;&gt; torch.save(mod.state_dict(), pt_path)\n&gt;&gt;&gt; def check(ts):\n...     assert all(_.startswith(\"a.\") for _ in ts)\n&gt;&gt;&gt; exported_tensors = export_tensors_from_disk_to_nnef(\n...     pt_path,\n...     nnef_dir,\n...     lambda x: x.startswith(\"a.\"),\n...     check\n... )\n&gt;&gt;&gt; list(exported_tensors.keys())\n['a.weight', 'a.bias']\n</code></pre>","boost":2},{"location":"export_api/#torch_to_nnef.export.export_tensors_to_nnef","title":"export_tensors_to_nnef","text":"<pre><code>export_tensors_to_nnef(name_to_torch_tensors: T.Dict[str, _Tensor], output_dir: Path) -&gt; T.Dict[str, _Tensor]\n</code></pre> <p>Export any torch.Tensors list to NNEF .dat file.</p> <p>Parameters:</p> Name Type Description Default <code>name_to_torch_tensors</code> <code>Dict[str, _Tensor]</code> <p>dict A map of name (that will be used to define .dat filename) and tensor values (that can also be special torch_to_nnef tensors)</p> required <code>output_dir</code> <code>Path</code> <p>directory to dump the NNEF tensor .dat files</p> required <p>Returns:</p> Type Description <code>Dict[str, _Tensor]</code> <p>a dict of tensor name as key and torch.Tensor values, identical to <code>torch_to_nnef.export.export_tensors_to_nnef</code></p> <p>Examples:</p> <p>Simple example</p> <pre><code>&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; class Mod(nn.Module):\n...     def __init__(self):\n...         super().__init__()\n...         self.a = nn.Linear(1, 5)\n...         self.b = nn.Linear(5, 1)\n...\n...     def forward(self, x):\n...         return self.b(self.a(x))\n&gt;&gt;&gt; mod = Mod()\n&gt;&gt;&gt; nnef_dir = tempfile.mkdtemp(suffix=\"_nnef\")\n&gt;&gt;&gt; exported_tensors = export_tensors_to_nnef(\n...     {k: v for k, v in mod.named_parameters() if k.startswith(\"b.\")},\n...     nnef_dir,\n... )\n&gt;&gt;&gt; list(exported_tensors.keys())\n['b.weight', 'b.bias']\n</code></pre>","boost":2},{"location":"export_api/#torch_to_nnef.export.iter_torch_tensors_from_disk","title":"iter_torch_tensors_from_disk","text":"<pre><code>iter_torch_tensors_from_disk(store_filepath: Path, filter_key: T.Optional[T.Callable[[str], bool]] = None) -&gt; T.Iterator[T.Tuple[str, _Tensor]]\n</code></pre> <p>Iter on torch tensors from disk .safetensors, .pt, pth, .bin.</p> <p>Parameters:</p> Name Type Description Default <code>store_filepath</code> <code>Path</code> <p>path to the container file holding PyTorch tensors (.pt, .pth, .bin and .safetensors)</p> required <code>filter_key</code> <code>Optional[Callable[[str], bool]]</code> <p>if set, this function filter over tensor by name stored in those format</p> <code>None</code> <p>Yields:</p> Type Description <code>str</code> <p>provide each tensor that are validated by filter within store filepath</p> <code>_Tensor</code> <p>one at a time as tuple with name first then the torch.Tensor itself</p>","boost":2},{"location":"why_nnef/","title":"Why Use NNEF?","text":""},{"location":"why_nnef/#wait-what-is-nnef","title":"Wait, What Is NNEF?","text":"<p>NNEF stands for Neural Network Exchange Format.</p> <p>Introduced in 2018\u2014just a year after .</p> <p>NNEF addresses the same core challenge as ONNX: providing a standardized way to exchange neural network models across different tools and frameworks.</p> <p>It is specified by the Khronos Group, an open, non-profit consortium of around 170 member organizations, better known for defining major graphics and compute standards such as WebGL, OpenCL, and Vulkan.</p>"},{"location":"why_nnef/#tools-and-ecosystem","title":"Tools and Ecosystem","text":"<p>Beyond the specification itself, Khronos also provides several reference tools to enable partial model conversion (e.g., from TensorFlow or ONNX). However, these tools:</p> <ul> <li> <p>Do not support PyTorch directly,</p> </li> <li> <p>And none offer the extensive support provided by this package.</p> </li> </ul> <p>Note</p> <p>We leverage these Khronos tools for final serialization within <code>torch_to_nnef</code> (thanks Viktor Gyenes &amp; al for their continued support on <code>NNEF-tools</code>).</p>"},{"location":"why_nnef/#nnef-inference-support","title":"NNEF Inference Support","text":"<p>As of today, the only inference engine (excluding full training frameworks) that natively supports NNEF as a first-class format is tract \u2014 the open-source neural inference engine developed by Sonos.</p>"},{"location":"why_nnef/#the-good-what-makes-the-nnef-specification-appealing","title":"The Good: What Makes the NNEF Specification Appealing","text":"<ol> <li> <p>Leverages Existing, Widely-Supported Containers</p> <p>Stop reinventing the wheel\u2014NNEF embraces common container systems. It's efficient, well-supported, and decouples data storage from model structure (think of video formats vs. codecs).</p> <ul> <li>Example: <code>tar</code> is totally fine\u2014and if you want compression, just apply it.</li> <li>Prefer another container format? You're free to use it.</li> </ul> </li> <li> <p>Efficient Tensor Storage</p> <p>Each tensor is stored as a binary <code>.dat</code> blob.</p> <ul> <li>While <code>.npy</code> might seem more standard, <code>.dat</code> offers better extensibility.</li> <li> <p>The format supports custom data types with a:</p> <p>4-byte code indicating the tensor's item-type (Up to 4.2 billion possible custom types!)</p> </li> </ul> </li> <li> <p>Readable Graph Structure</p> <p>The main <code>.nnef</code> file represents the model graph in a simple, declarative, text-based format:</p> <ul> <li>No control flow complexity</li> <li>Easy to read and edit (e.g., jump to definitions in your favorite editor)</li> <li>Flexible and extensible\u2014it's just text.</li> </ul> </li> <li> <p>Separation of Quantization Logic</p> <p>Quantization metadata lives in a separate <code>.quant</code> file:</p> <ul> <li>Defines variables, quantization functions, and parameters</li> <li>Supports advanced schemes (e.g., Q40 per-group) via custom data types</li> </ul> </li> <li> <p>Textual Composition with Pure Functions</p> <p>Neural-network are made of repetition of blocks (group of layers), the text format promotes reusability, avoids repetition, and enables a clean functional structure.</p> </li> </ol>"},{"location":"why_nnef/#the-bad-limitations-of-the-nnef-specification","title":"The Bad: Limitations of the NNEF Specification","text":"<ol> <li> <p>No Reference Implementation or Test Suite</p> <p>Only basic converters exist (TensorFlow/ONNX), and a rudimentary interpreter in PyTorch\u2014nothing production-grade.</p> </li> <li> <p>Image-Centric Design</p> <p>The spec was initially tailored for image inference tasks, limiting its general applicability.</p> </li> <li> <p>Static Tensor Shapes</p> <p>No support for dynamic dimensions.</p> </li> <li> <p>No Built-In Support for Recurrent Layers</p> </li> <li> <p>Undefined or Poorly-Specified Data Types for activations</p> </li> <li> <p>Stagnant Development</p> <p>Last official update: <code>v1.0.5</code> on 2022-02</p> </li> </ol>"},{"location":"why_nnef/#nnef-extensions-in-tract","title":"NNEF Extensions in Tract","text":"<ol> <li> <p>Supports Text and Signal Models</p> <p>Through an extended operator set.</p> </li> <li> <p>Dynamic Shape Support</p> <p>Enabled by symbolic dimensions.</p> </li> <li> <p>Advanced Data Type Handling</p> <p>Fine-grained, low-level types are natively supported.</p> </li> <li> <p>Modular Subgraph Assembly</p> <p>Enables flexible architecture composition.</p> </li> </ol> <p>These extensions are encapsulated under the concept of inference targets in <code>torch_to_nnef</code>, allowing inference engines to define their own \"NNEF flavor\"\u2014while retaining a shared syntax and graph structure and common set of 'specified' operators.</p>"},{"location":"why_nnef/#why-not-onnx-or-other-protocol-buffer-based-formats","title":"Why Not ONNX or Other Protocol Buffer-Based Formats?","text":"<p>Abstract</p> <p>Let's be clear: ONNX is a great standard. It's mature, widely adopted, and works well for many neural network applications.</p> <p>However, ONNX is based on Protocol Buffers, which introduce real limitations\u2014even acknowledged in their own docs:</p> <ol> <li> <p>Not Suitable for Large Data Assets</p> <p>... assume that entire messages can be loaded into memory at once and are not larger than an object graph. For data that exceeds a few megabytes, consider a different solution; when working with larger data, you may effectively end up with several copies of the data due to serialized copies, which can cause surprising spikes in memory usage.</p> </li> <li> <p>Inefficient for Large Float Arrays</p> <p>Protocol buffer messages are less than maximally efficient in both size and speed for many scientific and engineering uses that involve large, multi-dimensional arrays of floating point numbers ...</p> </li> <li> <p>No Built-In Compression</p> </li> </ol>"},{"location":"why_nnef/#opinionated-grievances-specific-to-nn-use-cases","title":"Opinionated Grievances (Specific to NN Use Cases)","text":"<ol> <li> <p>Tightly Coupled Graph &amp; Tensors    Want to patch a model with new PEFT weights or tweak a few parameters? Good luck\u2014everything\u2019s entangled.</p> </li> <li> <p>Unreadable Without Specialized Tools    Tools like TensorBoard or Netron are needed for visualization but difficult to read when more than 10 I/O tensors are linked to an operator (e.g having long residual connection deforms the graph visuals).</p> </li> <li> <p>No Direct Tensor Access    Requires full graph parsing and multi-hop traversal.</p> </li> <li> <p>Quantization definition is not very flexible    Especially for custom formats or precision below Q4.</p> </li> <li> <p>Extensibility is Harder    To add new data formats, you need change of protocol buffer spec, features like <code>symbols</code> definition in tract need to be defined ad-hoc. Adding plain text extensions is easier to do and read (at the cost of loosing code-gen ser/deser from protobuf). Prior PyTorch 2.0, adding custom ops (when it has no-equivalent chain of supported ops) is also tedious and partly unspecified.</p> </li> </ol>"},{"location":"why_nnef/#safetensors","title":"Safetensors","text":"<p>Safetensors is essentially a secure, structured list of tensors stored in binary\u2014plus minimal metadata.</p> <ol> <li> <p>Directly Loadable to Devices</p> </li> <li> <p>Avoids Pickle Security Issues</p> </li> </ol> <p>\ud83d\udd0d But: Its benefits are tied to loading efficiency\u2014not the format itself. It could just as well have been implemented using <code>tar</code>.</p>"},{"location":"why_nnef/#major-drawback","title":"Major Drawback","text":"<ul> <li> <p>No Computation Graph</p> <p>Every model architecture must be re-implemented manually on top of the inference engine\u2014error-prone and wasteful.</p> </li> <li> <p>No Operator Fusion or Optimization Guidance</p> <p>That burden falls entirely on the implementer, per model.</p> </li> </ul>"},{"location":"why_nnef/#gguf","title":"GGUF","text":"<p>GGUF is similar to <code>.safetensors</code>, but includes a lot of quantization format definitions.</p> <ol> <li> <p>Vast choices of Quantization formats</p> <p>Especially the Q40 format, which we've borrowed in <code>tract/torch_to_nnef</code>.</p> </li> <li> <p>Still No Graph Structure</p> <p>Just like <code>.safetensors</code>, GGUF lacks a way to express model computation graphs.</p> </li> </ol>"},{"location":"contributing/add_new_aten_op/","title":"Add new aten / prim / quantized operator","text":"<p>Goals</p> <p>At the end of this tutorial you will be able to:</p> <ol> <li> Contribute a new operator support</li> </ol> <p>Prerequisite</p> <ul> <li> PyTorch and Python basics</li> <li> 10 min to read this page</li> </ul> <p>PyTorch internal representation (IR) contains more than 10<sup>3</sup> operators (and less than 10<sup>4</sup>). Aten is the name of the underling C++ namespace in which most of the PyTorch computational operators are specified. Looking at the core list in the PyTorch IR, it may seems at first there is only: &lt;200 main ops to support (not accounting quantized &amp; prims namespaces). Sadly these external documentations are partial, in order to keep an exhaustive track of what is supported we maintain a generated compatibility list.</p> <p>While the most common operators are already supported in <code>torch_to_nnef</code>, this list is ever expanding, so there is always a need to catch-up when a new model end up adopting one of those.</p> <p>In the development of this library we add operator translation (support) on a per need basis (aka we need to export a new model, ow it misses this and that operator, let's implement it). There is no commitment by SONOS to support them all, but contributions are always welcome.</p> <p>In this tutorial we will share with you how to contribute a new operator.</p>"},{"location":"contributing/add_new_aten_op/#checklist","title":"Checklist","text":"<p>To implement a new operator we need to follow the following steps:</p> <ul> <li> 0. Ensure this operator makes sense in your targeted engine (by example <code>copy</code>, device layout etc) should be <code>nop</code>, implementation detail in most inference engines</li> <li> 1.  Add few unit-test covering the operator in test_primitive.py</li> <li> 2.  Check we obtain as expected the following form of exception:</li> </ul> <pre><code>torch_to_nnef.exceptions.T2NErrorNotImplemented:\n'$operator_aten_name' operator as not yet been translated to NNEF or registred\n</code></pre> <ul> <li> 3.   Implement a translation in Python in one of the <code>torch_to_nnef.op.aten</code></li> <li> 4.  Ensure test-suite pass</li> <li> 5.  Ensure coding guideline are respected</li> <li> 6.  submit the Pull request (PR)</li> </ul> <p>Obviously this list is indicative as in some infortunate cases:</p> <ul> <li>The operator does not exist in targeted inference engine: please link the associated PR from this engine as reference (by example tract)</li> <li>There is a bug between 2. and 3.: in that case maybe you can file an issue or try to debug yourself</li> </ul>"},{"location":"contributing/add_new_aten_op/#step-0-ensure-targeted-inference-engine-compatibility","title":"Step 0. Ensure targeted inference engine compatibility","text":"<p>As we stated in the checklist upper, each targeted inference engine is different (today: KhronosNNEF &amp; TractNNEF are the only included in <code>torch_to_nnef</code>).</p> <p>In the case of <code>tract</code>, the engine:</p> <ul> <li>decides on which device it runs</li> <li>what is the memory layout</li> <li>when copy should happen inside the neural network operation chains.</li> <li>willingly lack operations relative to losses and back-propagation (operations containing <code>backward</code> in their names)</li> <li>does not handle today <code>sparse</code> tensors, and quantization support is partial.</li> <li>was developed first with audio and NLP usecase in-mind so there may be a significant portion of image specific operators that are still missing (implementation is welcome in tract repository side).</li> <li>is not a general purpose linear algebra library, so most specialized operations will certainly be missing (decomposition, ...).</li> </ul> <p>This set of constraint remove a whole class of operators that are used in PyTorch, if you are unsure about the operator you are willing to support just contact directly the maintainers of this project in the discussion section of tract.</p>"},{"location":"contributing/add_new_aten_op/#step-1-adding-unit-tests","title":"Step 1. Adding unit tests","text":"<p>Let's checkout the git project and create a new branch to <code>torch_to_nnef</code> named:  <code>feat/{inference_target}-aten-{aten_operator_name}</code> where inference target is <code>tract</code>,<code>khronos</code> and aten operator is one of this list, still unsupported.</p> <p>After that you can edit the file named: <code>./tests/test_primitive.py</code> and at the end of it after the last <code>test_suite.add</code>, add the following temporary line:</p> <pre><code>test_suite.reset()\n</code></pre> <p>After that you can copy one of our <code>test_suite.add</code> observable upper with proper call to your unsupported torch operation by example:</p> <pre><code>test_suite.add(\n    torch.randn(5, 3),\n    UnaryPrimitive(torch.svd),\n    # filter to specific inference engine to skip from test\n    inference_conditions=skip_khronos_interpreter,\n)\n</code></pre> <p>Side note here: tract has no reason to expose Singular Value Decomposition (this is not part of most neural network, but you can argue in tract discussions if you feel that's a need).</p> <p>After that you can run the test with the command:</p> <pre><code>py.test tests/test_primitive.py::test_primitive_export\n</code></pre> <p>If you run it as such there should be 2 failed test case. Why 2 ? Because given our test suite definition in our test it run on last 2 major versions of tract.</p> <p>What if you want to focus on 1 test case ? just run:</p> <pre><code>T2N_TEST_TRACT_VERSION=\"0.21.13\" py.test tests/test_primitive.py::test_primitive_export\n</code></pre> <p>In this case you will test on 0.21.13 version but you can set it to any version released at the time in tract repository.</p> <p>If you are willing to test a specific custom version of tract instead you can directly specify the path to tract cli binary it as such:</p> <pre><code>T2N_TEST_TRACT_PATH=\"$HOME/.../tract\" py.test tests/test_primitive.py::test_primitive_export\n</code></pre> <p>Finally you may want to focus on non tract inferences and set the:</p> <pre><code>T2N_TEST_SKIP_TRACT=1 py.test tests/test_primitive.py::test_primitive_export\n</code></pre> <p>Other useful environment variable you can activate are:</p> <ul> <li><code>DUMP_DIRPATH={my_dirpath}</code> that will dump all <code>.nnef.tgz</code> from successful tests (useful to create a zoo of use-case), warning that may be a lot</li> <li><code>DEBUG=1</code> that will build and fill a directory <code>failed_tests</code> when you run tests. It will contain all dumps of models that are not passing test suite but still are able to be exported to NNEF (either because of a translation code error or a bug in the targeted inference engine), with ad-hoc information useful to debug.</li> </ul>"},{"location":"contributing/add_new_aten_op/#step-2-un-implemented-check","title":"Step 2. Un-Implemented check","text":"<p>While adding new operators test it may happen you do not observe the error (following example in step 1.)</p> <pre><code>torch_to_nnef.exceptions.T2NErrorNotImplemented:\n'svd' operator as not yet been translated to NNEF or registred\n</code></pre> <p>If that happen you can either file an issue or try to debug yourself.</p>"},{"location":"contributing/add_new_aten_op/#step-3-implement-translation","title":"Step 3. Implement translation","text":"<p>It's now time to add your operator translation !</p> <p>A lot of example exist in the <code>torch_to_nnef.op.aten</code> sub modules. Each sub-module is organized by theme. please try to find the one that is the closest from your operator or put it in <code>other</code> if not.</p> <p>There are mostly 3 kinds of operator mapping</p> <p>1. Those that directly map to NNEF spec and are 1 to 1 tensor transformation in that case just add it in the map in <code>torch_to_nnef.op.aten.unary</code>: <code>GENERIC_UNARY_OUTPUT_ATEN_OP_NAMES</code> or <code>REMAP_ATEN_OP_NAMES</code>.</p> <p>2. Those that need a bit of mapping (sometimes adding few composed operators NNEF side):</p> Example of straight mapping<pre><code>@OP_REGISTRY.register([\"bitwise_or\"]) # (1)!\ndef bitwise_or(node, op_helper, inference_target, **kwargs): # (2)!\n    assert len(node.outputs) == 1\n    if not isinstance(inference_target, TractNNEF): # (3)!\n        raise T2NErrorNotImplemented(inference_target)\n    op_helper.unary_output_op_without_attr( # (4)!\n        nnef_op_type=\"tract_core_bitor\", node=node\n    )\n    return [\"tract_core\"] # (5)!\n</code></pre> <ol> <li>OP_REGISTRY is by convention always declared on top of module it's the registry that accumulate the translations. if you call <code>.register()</code> it will take the name of the function as reference for the operator to translate. if you provide an array like <code>.register([\"a\", \"b\", \"c\"])</code> all aten operators named <code>a</code>, <code>b</code> and <code>c</code> will be mapped here.</li> <li>The complete signature of the function is evolving but as of now is: <code>g: nnef.Graph</code>, <code>node: torch_to_nnef.torch_graph.TorchOp</code>, <code>name_to_tensor: T.Dict[str, nnef.tensor]</code>, <code>null_ref: nnef.tensor</code>, <code>torch_graph: torch_to_nnef.torch_graph.TorchModuleIRGraph</code>, <code>inference_target: torch_to_nnef.inference_target.base.InferenceTarget</code>, <code>aten_op_id: str</code>, <code>op_helper: torch_to_nnef.op.helper.OpHelper</code> obviously a lot of those parameters are often unneeded hence the <code>**kwargs</code>. Basically our goal is always to translate what is in <code>node</code> the best we can in <code>g</code> while keeping <code>name_to_tensor</code> up-to-date. <code>OpHelper</code> is a 'newly' introduced builder to simplify creation of classic translation pattern.</li> <li>Often you may want to support only for specific <code>inference_target</code> Type or Version this is an concrete example of how this can look like</li> <li>Here we use the helper to declare a new operator that will have a single output from a single input named in NNEF graph <code>tract_core_bitor</code></li> <li>By default translation function can return None or empty array but if an array of string is provided, it will automatically try to load the associated fragment in <code>torch_to_nnef.op.fragment</code></li> </ol> <p>Here we added tooltips on each part to explains the best we could.</p> <p>3. <code>nn.Module</code> that are are too complex (explode is too much IR components) or not tracable easily in this case you can use custom operators (If you want to introduce one in core library please contact us before)</p>"},{"location":"contributing/add_new_aten_op/#step-4-test-suite-pass","title":"Step 4. Test suite pass","text":"<p>At this stage you can relaunch test suite as described upper and it should pass.</p> <p>Do not forget to remove the <code>test_suite.reset()</code> and relaunch the full test suite coverage.</p> global cmd<pre><code>py.test\n</code></pre> <p>To ensure nothing breaks due to that new addition.</p>"},{"location":"contributing/add_new_aten_op/#step-5-check-written-code","title":"Step 5. Check written code","text":"<p>There are 3 things to do now run :</p> <ul> <li>ruff for formatting</li> </ul> <pre><code>ruff format\n</code></pre> <ul> <li>ruff check for first lint</li> </ul> <pre><code>ruff check\n</code></pre> <ul> <li>prospector deeper more complete analysis</li> </ul> <p>In term of naming convention try to follow google style. Please conform to those.</p>"},{"location":"contributing/add_new_aten_op/#step-6-submit-the-pull-request-pr","title":"Step 6. submit the Pull request (PR)","text":"<p>You are now ready to create a pull request on our repo, please note that after that all your code will be [MIT/Apache 2] licensed. Also please follow the same prefix naming convention for your PR name and commits than for the branch.</p> <p>Congratulation</p> <p> You made it ! Congratulation and thank you so much for your contribution</p>"},{"location":"contributing/debugging/","title":"Debugging, tips &amp; tricks","text":"<p>Info</p> <p>This section intends to be an 'unordered' collection of debugging tricks for torch_to_nnef internals.</p>"},{"location":"contributing/debugging/#logging","title":"Logging","text":"<p>Setting the log level to debug in <code>torch_to_nnef</code> may help you figure out where things broke in the first place. There is a <code>torch_to_nnef.log</code> exactly for that.</p>"},{"location":"contributing/debugging/#errors","title":"Errors","text":"<p>In <code>torch_to_nnef</code> we try to derive all possible errors from <code>torch_to_nnef.exceptions.T2NError</code>, so it should help to interpret why issue arise, but also control it on upper level control flow.</p>"},{"location":"contributing/debugging/#graph-display","title":"Graph Display","text":"<p>If you end up debugging the internal IR construction it is very helpful to display the representation that are built as they may involve a lot of parameters and operators, be sure to read about this section, that may help you a lot.</p>"},{"location":"contributing/debugging/#nnef-display","title":".NNEF Display","text":"<p>Reading plain text without syntax coloring is painful and may lead to confusion. You can use <code>c++</code> syntax display as a good proxy in most IDE. If you are a vim user you can also use this direct NNEF syntax plugin.</p>"},{"location":"contributing/guidelines/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/guidelines/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/guidelines/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/sonos/torch-to-nnef/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> <li>Ideally share the debug bundle generated from <code>export_model_to_nnef(..., debug_bundle_path=)</code></li> </ul>"},{"location":"contributing/guidelines/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"contributing/guidelines/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"contributing/guidelines/#write-documentation","title":"Write Documentation","text":"<p>torch_to_nnef could always use more documentation, whether as part of the official torch_to_nnef docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/guidelines/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/sonos/torch-to-nnef/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions   are welcome :)</li> </ul>"},{"location":"contributing/guidelines/#get-started","title":"Get Started","text":"<p>Ready to contribute? Here's how to set up <code>torch_to_nnef</code> for local development.</p> <ol> <li>Fork the <code>torch_to_nnef</code> repo on GitHub.</li> <li> <p>Clone your fork locally</p> <pre><code>git clone git@github.com:sonos/torch_to_nnef.git\n</code></pre> </li> <li> <p>Ensure uv is installed.</p> </li> <li> <p>Install dependencies and start your virtualenv:</p> <pre><code>uv pip install --extra test --extra doc --extra dev\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Try to follow this prefix naming convention: * <code>feat/</code>  for features branch (new operators, new handlers ...) * <code>fix/</code>  for bugfixes and hotfixes branch * <code>chore/</code>  for versions bump, CI/CD, packaging, ...</p> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass the    tests, including testing other Python versions, with tox:</p> <pre><code>uv run tox\n</code></pre> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>git add .\ngit commit -m \"Your detailed description of your changes.\"\ngit push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/guidelines/#code-style","title":"Code style","text":"<p>We use the following code tools for formatting and first checks:</p> <ul> <li>ruff for formatting</li> <li>ruff check for first lint</li> <li>prospector deeper more complete analysis</li> </ul> <p>In term of naming convention we try to follow google style, the most we can.</p> <p>I</p>"},{"location":"contributing/guidelines/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated. Put    your new functionality into a function with a docstring, and add the    feature to the list in README.md.</li> <li>The pull request should work for Python 3.9, 3.10, 3.11 and 3.12. Check    https://github.com/sonos/torch-to-nnef/actions    and make sure that the tests pass for all supported Python versions.</li> </ol>"},{"location":"contributing/guidelines/#tips","title":"Tips","text":"<pre><code>uv run pytest tests/test_torch_to_nnef.py\n</code></pre> <p>To run a subset of tests.</p>"},{"location":"contributing/guidelines/#deploying","title":"Deploying","text":"<p>A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run:</p> <pre><code>uv run bump2version patch # possible: major / minor / patch\ngit push\ngit push --tags\n</code></pre> <p>GitHub Actions will then deploy to PyPI if tests pass.</p>"},{"location":"contributing/internal_design/","title":"Internal design","text":"<p>Internals of torch to NNEF export are mostly segmented in 6 steps as shown bellow:</p> figure 1: Export internal process <p>Each of those steps have specific aims and goals.</p> <ol> <li>Aims to make sense of complex inputs and outputs such as dict, dict like object containing tensors or tensors inside containers in containers...</li> <li>Name each tensor after the module it is assigned to (if it's shared across multiple modules first name encountered will be retained)</li> <li>Trace the PyTorch Graph module by module starting from the provided model each sub-module call is solved after this module have been traced. each submodule is colapsed inside it's parent. This tracing build a specific internal representation (IR) in torch to nnef which is NOT torch graph but a simplified version of it that is no more tied to torch cpp internals and with removed useless operators for inference.</li> <li>Translate the torch to nnef internal IR into NNEF depending on inference target selected</li> <li>Save each tensor on disk in .dat and serialize the graph.nnef and graph.quant associated.</li> <li>Allow to perform a serie of test after NNEF model asset has been generated (typically checking output similarities)</li> </ol> figure 2: code structure as of 2025-08 (n\u00b0 correspond to figure 1) <p>Note</p> <p>These steps only apply to <code>torch_to_nnef.export_model_to_nnef</code> export function that exports the graph + the tensors. To observe those: setting log level to info for this lib is helpful, a proposed default logger is available in <code>torch_to_nnef.log.init_log</code></p>"},{"location":"contributing/internal_design/#1-auto-wrapper","title":"1. Auto wrapper","text":"<p>The auto wrapper is available at <code>torch_to_nnef.model_wrapper</code>. In essence, this step tries hard to make sense of the input and output provided by the user as input parameters by 'flattening' and extracting from complex data-structures a proper list of tensor to be passed. Some example can be seen in our multi inputs/outputs tutorial. Still note that as of today the graph is traced statically with Python primitive constantized. Also raw objects passed in <code>forward</code> function are not supported yet (uncertainty about the order in which tensors found in it should be passed).</p>"},{"location":"contributing/internal_design/#2-tensor-naming","title":"2. Tensor naming","text":"<p>This replaces each tensor in the graph (code can be found in <code>torch_to_nnef.tensor.named</code>) by a named tensor holding the name it will have in the different intermediate representations. This is helpful to keep consistent tensor naming between the PyTorch parameters/buffers name and NNEF archive we build. Allowing confident reference between the 2 worlds. In practice this tensor acts just like a classical <code>torch.Tensor</code> so it can even be used beyond <code>torch_to_nnef</code> usecase, if you want to name tensors.</p>"},{"location":"contributing/internal_design/#3-internal-ir-representation","title":"3. Internal IR representation","text":"<p>While tracing the graph recursively you may debug its parsed representation as follows: let's imagine you set a breakpoint in <code>torch_to_nnef.torch_graph.ir_graph.TorchModuleIRGraph.parse</code> method you could call <code>self.tracer.torch_graph</code> to observe the PyTorch representation:</p> <pre><code>graph(%self.1 : __torch__.torchvision.models.alexnet.___torch_mangle_39.AlexNet,\n      %x.1 : Float(1, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cpu)):\n  %classifier : __torch__.torch.nn.modules.container.___torch_mangle_38.Sequential = prim::GetAttr[name=\"classifier\"](%self.1)\n  %avgpool : __torch__.torch.nn.modules.pooling.___torch_mangle_30.AdaptiveAvgPool2d = prim::GetAttr[name=\"avgpool\"](%self.1)\n  %features : __torch__.torch.nn.modules.container.___torch_mangle_29.Sequential = prim::GetAttr[name=\"features\"](%self.1)\n  %394 : Tensor = prim::CallMethod[name=\"forward\"](%features, %x.1)\n  %395 : Tensor = prim::CallMethod[name=\"forward\"](%avgpool, %394)\n  %277 : int = prim::Constant[value=1]() # /Users/julien.balian/SONOS/src/torch-to-nnef/.venv/lib/python3.12/site-packages/torchvision/models/alexnet.py:50:0\n  %278 : int = prim::Constant[value=-1]() # /Users/julien.balian/SONOS/src/torch-to-nnef/.venv/lib/python3.12/site-packages/torchvision/models/alexnet.py:50:0\n  %input.19 : Float(1, 9216, strides=[9216, 1], requires_grad=0, device=cpu) = aten::flatten(%395, %277, %278) # /Users/julien.balian/SONOS/src/torch-to-nnef/.venv/lib/python3.12/site-packages/torchvision/models/alexnet.py:50:0\n  %396 : Tensor = prim::CallMethod[name=\"forward\"](%classifier, %input.19)\n  return (%396)\n</code></pre> <p><code>self.printall()</code> and observe the current torch to NNEF representation:</p> <pre><code>___________________________________[PyTorch JIT Graph '&lt;class 'torchvision.models.alexnet.AlexNet'&gt;']___________________________________\ninputs: (AlexNet_x_1: torch.float32@[1, 3, 224, 224])\n\n        Static Constants:\n                int AlexNet_277 := 1\n                int AlexNet_278 := -1\n\n        Static Tensor:\n\n        Blob TorchScript:\n\n        List:\n\n        TupleTensors:\n\n        Directed Acyclic Graph:\n                 None AlexNet_394 := prim::CallMethod&lt;Sequential.forward&gt;( AlexNet_x_1 )\n                 None AlexNet_395 := prim::CallMethod&lt;AdaptiveAvgPool2d.forward&gt;( AlexNet_394 )\n                 torch.float32 AlexNet_input_19 := aten::flatten( AlexNet_395, AlexNet_277, AlexNet_278 )\n                 None AlexNet_396 := prim::CallMethod&lt;Sequential.forward&gt;( AlexNet_input_19 )\n\noutputs: (AlexNet_396: None@None)\n____________________________________________________________________________________________________\n</code></pre> <p>Since the process is recursive you can see this representation evolve as each submodule gets parsed.</p> <p>Also if you want to learn more the representation data structure we use you can look at the <code>torch_to_nnef.torch_graph.ir_data</code> and <code>torch_to_nnef.torch_graph.ir_op</code>.</p> <p>This step is crucial in order to get an accurate representation of the Graph. A lot of thing can go wrong and this interface with some internal part of <code>PyTorch</code> which aren't guarantied as stable. This is one of the reason we have a dedicated IR in <code>torch_to_nnef</code>. When code breaks in this part, a good understanding of PyTorch internals is often required, and due to the lack of documentation, reading their source code is necessary.</p>"},{"location":"contributing/internal_design/#4-nnef-translation","title":"4. NNEF translation","text":"<p>This step is probably one that need the most code, but that's often rather straightforward. It's responsible to mapping between our internal representation and the NNEF graph. Adding a new operator is a rather simple process as long as the 2 engines (PyTorch and the inference target) share similar operator to composes. But since there are so much operators in <code>PyTorch</code> there is a lot of mapping to do. In some case when there is too much discrepancy between the engines it may be worth proposing to reify the operation in the targeted inference engine.</p>"},{"location":"contributing/internal_design/#5-nnef-dump","title":"5. NNEF dump","text":"<p>This step is rather simple. It uses a modernized version of the dump logic proposed by Khronos group in their package <code>nnef_tools</code>, with few extensions around custom <code>.dat</code> format serialization (code is available here).</p>"},{"location":"contributing/supported_operators/","title":"Supported aten operators","text":"<p>Note</p> <p>This table and page are auto generated from 'a script' that dig into PyTorch. Version targetted is:  'v2.7.1'. file was generated the 28 Jul 2025.</p> <p>Warning</p> <p>Take these informations with a grain of salt as this is referencing operators that may never appear in torch IR graph traced by <code>torch_to_nnef</code> (because remapped to others more generic). Also some  uncommon operators are very rare in models, hence support may be lacking.  SONOS only maintains operators 'per need basis', but contributions are always wecome see how.</p> <p>'is core' column refers to this PyTorch IR documentation page</p> <p>We filter-out from from observed operators 'backward' and 'sym' one's which are unwanted in inference engine. Also in place operations are merged with memory allocated activations as this is inference implementation detail.</p> TractNNEFONNX <p>Total matched operators in <code>torch_to_nnef</code> compared to:</p> <ul> <li>core PyTorch opset:</li> </ul> <p> <p>114/138</p> </p> <ul> <li>and support from full <code>aten::</code>:</li> </ul> <p> <p>186/862</p> </p> <p>(total registered aten operators in t2n being 217)</p> aten name aliases can in-place is core translated abs absolute \u2705 \u2705 \u2705 acos arccos \u2705 \u2705 \u2705 acosh arccosh \u2705 \u2705 \u2705 adaptive_avg_pool1d \u274c \u2705 \u2705 add \u2705 \u2705 \u2705 addmm \u2705 \u2705 \u2705 alias \u274c \u2705 \u2705 amax \u274c \u2705 \u2705 amin \u274c \u2705 \u2705 any \u274c \u2705 \u2705 arange \u274c \u2705 \u2705 argmax \u274c \u2705 \u2705 argmin \u274c \u2705 \u2705 as_strided \u2705 \u2705 \u274c asin arcsin \u2705 \u2705 \u2705 asinh arcsinh \u2705 \u2705 \u2705 atan arctan \u2705 \u2705 \u2705 atan2 arctan2 \u2705 \u2705 \u2705 atanh arctanh \u2705 \u2705 \u2705 avg_pool1d \u274c \u2705 \u2705 avg_pool2d \u274c \u2705 \u2705 avg_pool3d \u274c \u2705 \u2705 bitwise_and \u2705 \u2705 \u2705 bitwise_not \u2705 \u2705 \u2705 bitwise_or \u2705 \u2705 \u2705 bitwise_xor \u2705 \u2705 \u2705 bmm \u274c \u2705 \u2705 cat concat, concatenate \u274c \u2705 \u2705 ceil \u2705 \u2705 \u2705 clamp clip \u2705 \u2705 \u2705 clone \u274c \u2705 \u2705 col2im \u274c \u2705 \u274c constant_pad_nd \u274c \u2705 \u2705 convolution \u274c \u2705 \u2705 copy \u2705 \u2705 \u2705 cos \u2705 \u2705 \u2705 cosh \u2705 \u2705 \u2705 cumsum \u2705 \u2705 \u274c diagonal \u274c \u2705 \u274c div divide, true_divide \u2705 \u2705 \u2705 elu \u2705 \u2705 \u2705 embedding \u274c \u2705 \u2705 empty \u274c \u2705 \u274c empty_strided \u274c \u2705 \u274c eq \u2705 \u2705 \u2705 erf special_erf \u2705 \u2705 \u2705 exp \u2705 \u2705 \u2705 expand \u274c \u2705 \u2705 expm1 special_expm1 \u2705 \u2705 \u2705 fill \u2705 \u2705 \u2705 flip \u274c \u2705 \u274c floor \u2705 \u2705 \u2705 fmod \u2705 \u2705 \u2705 full \u274c \u2705 \u2705 full_like \u274c \u2705 \u2705 gather \u274c \u2705 \u2705 ge greater_equal \u2705 \u2705 \u2705 gelu \u2705 \u2705 \u2705 grid_sampler_2d \u274c \u2705 \u274c gt greater \u2705 \u2705 \u2705 hardtanh \u2705 \u2705 \u2705 index \u274c \u2705 \u2705 index_put \u2705 \u2705 \u274c index_select \u274c \u2705 \u2705 isinf \u274c \u2705 \u274c isnan \u274c \u2705 \u274c le less_equal \u2705 \u2705 \u2705 leaky_relu \u2705 \u2705 \u2705 log \u2705 \u2705 \u2705 log10 \u2705 \u2705 \u2705 log1p special_log1p \u2705 \u2705 \u2705 log2 \u2705 \u2705 \u2705 logical_and \u2705 \u2705 \u2705 logical_not \u2705 \u2705 \u2705 logical_or \u2705 \u2705 \u2705 logical_xor \u2705 \u2705 \u2705 lt less \u2705 \u2705 \u2705 masked_scatter \u2705 \u2705 \u274c max \u274c \u2705 \u2705 max_pool2d_with_indices \u274c \u2705 \u274c max_pool3d_with_indices \u274c \u2705 \u274c maximum \u274c \u2705 \u2705 mean \u274c \u2705 \u2705 min \u274c \u2705 \u2705 minimum \u274c \u2705 \u2705 mm \u274c \u2705 \u2705 mul multiply \u2705 \u2705 \u2705 native_dropout \u274c \u2705 \u2705 native_group_norm \u274c \u2705 \u2705 native_layer_norm \u274c \u2705 \u2705 ne not_equal \u2705 \u2705 \u2705 neg negative \u2705 \u2705 \u2705 nonzero \u274c \u2705 \u274c permute \u274c \u2705 \u2705 pow \u2705 \u2705 \u2705 prod \u274c \u2705 \u2705 rand \u274c \u2705 \u274c randn \u274c \u2705 \u274c randperm \u274c \u2705 \u274c reciprocal \u2705 \u2705 \u2705 reflection_pad1d \u274c \u2705 \u2705 reflection_pad2d \u274c \u2705 \u2705 reflection_pad3d \u274c \u2705 \u2705 relu \u2705 \u2705 \u2705 remainder \u2705 \u2705 \u2705 repeat \u274c \u2705 \u2705 replication_pad2d \u274c \u2705 \u2705 replication_pad3d \u274c \u2705 \u2705 round special_round \u2705 \u2705 \u2705 rsqrt \u2705 \u2705 \u2705 scalar_tensor \u274c \u2705 \u2705 scatter \u2705 \u2705 \u2705 scatter_add \u2705 \u2705 \u274c scatter_reduce \u2705 \u2705 \u274c select \u274c \u2705 \u2705 select_scatter \u274c \u2705 \u274c sigmoid special_expit \u2705 \u2705 \u2705 sign \u2705 \u2705 \u2705 sin \u2705 \u2705 \u2705 sinh \u2705 \u2705 \u2705 slice \u274c \u2705 \u2705 slice_scatter \u274c \u2705 \u274c sort \u274c \u2705 \u2705 split_with_sizes \u274c \u2705 \u2705 sqrt \u2705 \u2705 \u2705 squeeze \u2705 \u2705 \u2705 sub subtract \u2705 \u2705 \u2705 sum \u274c \u2705 \u2705 tan \u2705 \u2705 \u2705 tanh \u2705 \u2705 \u2705 topk \u274c \u2705 \u2705 trunc fix \u2705 \u2705 \u2705 unsqueeze \u2705 \u2705 \u2705 upsample_bilinear2d \u274c \u2705 \u274c upsample_nearest2d \u274c \u2705 \u274c var \u274c \u2705 \u2705 view \u274c \u2705 \u2705 where \u274c \u2705 \u2705 adaptive_avg_pool2d \u274c - \u2705 adaptive_avg_pool3d \u274c - \u2705 adaptive_max_pool1d \u274c - \u2705 adaptive_max_pool2d \u274c - \u2705 adaptive_max_pool3d \u274c - \u2705 addbmm \u2705 - \u274c addcdiv \u274c - \u274c addcmul \u274c - \u274c addmv \u2705 - \u274c addr \u2705 - \u274c affine_grid_generator \u274c - \u274c alias_copy \u274c - \u274c align_as \u274c - \u274c align_tensors \u274c - \u274c align_to \u274c - \u274c all \u274c - \u2705 all_gather_into_tensor \u274c - \u274c all_reduce \u274c - \u274c allclose \u274c - \u274c alpha_dropout \u2705 - \u274c aminmax \u274c - \u274c angle \u274c - \u274c append \u274c - \u274c argsort \u274c - \u2705 argwhere \u274c - \u274c as_strided_copy \u274c - \u274c as_strided_scatter \u274c - \u274c as_tensor \u274c - \u274c atleast_1d \u274c - \u274c atleast_2d \u274c - \u274c atleast_3d \u274c - \u274c baddbmm \u2705 - \u2705 bartlett_window \u274c - \u274c batch_norm \u274c - \u2705 batch_norm_elemt \u274c - \u274c batch_norm_gather_stats \u274c - \u274c batch_norm_gather_stats_with_counts \u274c - \u274c batch_norm_stats \u274c - \u274c batch_norm_update_stats \u274c - \u274c bernoulli \u2705 - \u274c bilinear \u274c - \u274c bin \u274c - \u274c binary_cross_entropy \u274c - \u274c binary_cross_entropy_with_logits \u274c - \u274c bincount \u274c - \u274c binomial \u274c - \u274c bitwise_left_shift \u2705 - \u274c bitwise_right_shift \u2705 - \u274c blackman_window \u274c - \u274c block_diag \u274c - \u274c broadcast_tensors \u274c - \u274c broadcast_to \u274c - \u274c bucketize \u274c - \u274c can_cast \u274c - \u274c capitalize \u274c - \u274c cartesian_prod \u274c - \u274c cauchy \u2705 - \u274c ccol_indices \u274c - \u274c ccol_indices_copy \u274c - \u274c cdist \u274c - \u274c celu \u274c - \u274c center \u274c - \u274c chain_matmul \u274c - \u274c chalf \u274c - \u274c channel_shuffle \u274c - \u274c cholesky \u274c - \u274c cholesky_inverse \u274c - \u274c cholesky_solve \u274c - \u274c choose_qparams_optimized \u274c - \u274c chr \u274c - \u274c chunk \u274c - \u2705 clamp_max \u2705 - \u2705 clamp_min \u2705 - \u2705 clear \u274c - \u274c coalesce \u274c - \u274c col_indices \u274c - \u274c col_indices_copy \u274c - \u274c column_stack \u274c - \u274c combinations \u274c - \u274c complex \u274c - \u274c confirmed_by_owner \u274c - \u274c conj \u274c - \u274c conj_physical \u2705 - \u274c contiguous \u274c - \u2705 conv \u274c - \u274c conv1d \u274c - \u2705 conv2d \u274c - \u2705 conv3d \u274c - \u2705 conv_depthwise3d \u274c - \u274c conv_tbc \u274c - \u274c conv_transpose1d \u274c - \u274c conv_transpose2d \u274c - \u274c conv_transpose3d \u274c - \u274c convolution_overrideable \u274c - \u274c convrelu \u274c - \u274c copy_sparse_to_sparse \u2705 - \u274c copy_to \u274c - \u274c copysign \u2705 - \u274c corrcoef \u274c - \u274c cosine_embedding_loss \u274c - \u274c cosine_similarity \u274c - \u274c count \u274c - \u274c count_nonzero \u274c - \u274c cpu \u274c - \u274c cross \u274c - \u274c cross_entropy_loss \u274c - \u274c crow_indices \u274c - \u274c crow_indices_copy \u274c - \u274c ctc_loss \u274c - \u274c cuda \u274c - \u274c cudnn_affine_grid_generator \u274c - \u274c cudnn_batch_norm \u274c - \u274c cudnn_convolution \u274c - \u274c cudnn_convolution_add_relu \u274c - \u274c cudnn_convolution_relu \u274c - \u274c cudnn_convolution_transpose \u274c - \u274c cudnn_grid_sampler \u274c - \u274c cudnn_is_acceptable \u274c - \u274c cummax \u274c - \u274c cummin \u274c - \u274c cumprod \u2705 - \u274c cumulative_trapezoid \u274c - \u274c data \u274c - \u274c deg2rad \u274c - \u274c degrees \u274c - \u274c dense_dim \u274c - \u274c dequantize \u274c - \u2705 detach \u2705 - \u2705 detach_copy \u274c - \u274c device \u274c - \u274c diag \u274c - \u274c diag_embed \u274c - \u274c diagflat \u274c - \u274c diagonal_copy \u274c - \u274c diagonal_scatter \u274c - \u274c dict \u274c - \u274c diff \u274c - \u274c digamma special_digamma, special_psi \u2705 - \u274c dim \u274c - \u274c dist \u274c - \u274c divmod \u274c - \u274c dot \u274c - \u274c dropout \u2705 - \u2705 dsplit \u274c - \u274c dstack \u274c - \u274c dtype \u274c - \u274c eig \u274c - \u274c einsum \u274c - \u2705 element_size \u274c - \u274c embedding_bag \u274c - \u274c embedding_renorm \u2705 - \u274c empty_like \u274c - \u2705 empty_permuted \u274c - \u274c empty_quantized \u274c - \u274c enable_grad \u274c - \u274c endswith \u274c - \u274c equal \u274c - \u274c erfc special_erfc \u2705 - \u274c erfinv special_erfinv \u2705 - \u274c exp2 special_exp2 \u2705 - \u274c expand_as \u274c - \u274c expand_copy \u274c - \u274c expandtabs \u274c - \u274c exponential \u2705 - \u274c extend \u274c - \u274c eye \u274c - \u274c fabs \u274c - \u274c factorial \u274c - \u274c fake_quantize_per_channel_affine \u274c - \u274c fake_quantize_per_channel_affine_cachemask \u274c - \u274c fake_quantize_per_tensor_affine \u274c - \u274c fake_quantize_per_tensor_affine_cachemask \u274c - \u274c feature_alpha_dropout \u2705 - \u274c feature_dropout \u2705 - \u274c fft_fftfreq \u274c - \u274c fft_ihfft2 \u274c - \u274c fft_ihfftn \u274c - \u274c fft_irfftn \u274c - \u274c fft_rfftfreq \u274c - \u274c fft_rfftn \u274c - \u274c fill_diagonal_ \u274c - \u274c find \u274c - \u274c flatten \u274c - \u2705 flatten_dense_tensors \u274c - \u274c fliplr \u274c - \u274c flipud \u274c - \u274c float_power \u2705 - \u274c floor_divide \u2705 - \u2705 floordiv \u274c - \u274c fmax \u274c - \u274c fmin \u274c - \u274c foo \u274c - \u274c fork \u274c - \u274c format \u274c - \u274c frac \u274c - \u274c fractional_max_pool2d \u274c - \u274c fractional_max_pool3d \u274c - \u274c frexp \u274c - \u274c frobenius_norm \u274c - \u274c from_file \u274c - \u274c fused_moving_avg_obs_fake_quant \u274c - \u274c gamma \u274c - \u274c gcd \u2705 - \u274c geometric \u2705 - \u274c geqrf \u274c - \u274c get \u274c - \u274c get_autocast_dtype \u274c - \u274c get_device \u274c - \u274c get_gradients \u274c - \u274c get_pool_ceil_padding \u274c - \u274c getelem \u274c - \u274c glu \u274c - \u2705 glu_jvp \u274c - \u274c grad \u274c - \u274c grid_sampler \u274c - \u274c grid_sampler_3d \u274c - \u274c group_norm \u274c - \u2705 gru \u274c - \u274c gru_cell \u274c - \u274c hamming_window \u274c - \u274c hann_window \u274c - \u274c hardshrink \u2705 - \u274c hardsigmoid \u2705 - \u274c hardswish \u2705 - \u2705 has_torch_function \u274c - \u274c hash \u274c - \u274c heaviside \u274c - \u274c hex \u274c - \u274c hinge_embedding_loss \u274c - \u274c histc \u274c - \u274c histogram \u274c - \u274c histogramdd \u274c - \u274c hsplit \u274c - \u274c hspmm \u274c - \u274c hstack \u274c - \u2705 huber_loss \u274c - \u274c hypot \u2705 - \u274c i0 special_i0 \u2705 - \u274c igamma special_gammainc \u2705 - \u274c igammac special_gammaincc \u2705 - \u274c iinfo \u274c - \u274c im2col \u274c - \u274c imag \u274c - \u274c index_add \u274c - \u274c index_copy \u274c - \u274c index_fill \u274c - \u274c index_put_impl_ \u274c - \u274c index_reduce \u2705 - \u274c indices \u274c - \u274c indices_copy \u274c - \u274c initial_seed \u274c - \u274c inner \u274c - \u274c insert \u274c - \u274c instance_norm \u274c - \u274c int_repr \u274c - \u274c is_autocast_cpu_enabled \u274c - \u274c is_autocast_enabled \u274c - \u274c is_coalesced \u274c - \u274c is_complex \u274c - \u274c is_conj \u274c - \u274c is_contiguous \u274c - \u274c is_cuda \u274c - \u274c is_floating_point \u274c - \u274c is_grad_enabled \u274c - \u274c is_leaf \u274c - \u274c is_non_overlapping_and_dense \u274c - \u274c is_nonzero \u274c - \u274c is_owner \u274c - \u274c is_pinned \u274c - \u274c is_same_size \u274c - \u274c is_scripting \u274c - \u274c is_set_to \u274c - \u274c is_signed \u274c - \u274c is_strides_like_format \u274c - \u274c isalnum \u274c - \u274c isalpha \u274c - \u274c isclose \u274c - \u274c isdecimal \u274c - \u274c isdigit \u274c - \u274c isfinite \u274c - \u274c isidentifier \u274c - \u274c isin \u274c - \u274c islower \u274c - \u274c isneginf \u274c - \u274c isnumeric \u274c - \u274c isposinf \u274c - \u274c isprintable \u274c - \u274c isreal \u274c - \u274c isspace \u274c - \u274c istft \u274c - \u274c istitle \u274c - \u274c isupper \u274c - \u274c item \u274c - \u274c items \u274c - \u274c join \u274c - \u274c kaiser_window \u274c - \u274c keys \u274c - \u274c kl_div \u274c - \u274c kron \u274c - \u274c kthvalue \u274c - \u274c l1_loss \u274c - \u274c layer_norm \u274c - \u2705 lcm \u2705 - \u274c ldexp \u2705 - \u274c len \u274c - \u274c lerp \u2705 - \u274c lgamma \u2705 - \u274c lift \u274c - \u274c lift_fresh \u274c - \u274c lift_fresh_copy \u274c - \u274c linalg_cholesky_ex \u274c - \u274c linalg_cond \u274c - \u274c linalg_cross \u274c - \u274c linalg_det det \u274c - \u274c linalg_diagonal \u274c - \u274c linalg_eig \u274c - \u274c linalg_eigh \u274c - \u274c linalg_eigvals \u274c - \u274c linalg_eigvalsh \u274c - \u274c linalg_householder_product orgqr \u274c - \u274c linalg_inv inverse \u274c - \u274c linalg_inv_ex \u274c - \u274c linalg_ldl_factor_ex \u274c - \u274c linalg_ldl_solve \u274c - \u274c linalg_lstsq \u274c - \u274c linalg_lu \u274c - \u274c linalg_lu_factor_ex \u274c - \u274c linalg_lu_solve \u274c - \u274c linalg_matrix_exp matrix_exp \u274c - \u274c linalg_matrix_norm \u274c - \u274c linalg_matrix_power matrix_power \u274c - \u274c linalg_matrix_rank \u274c - \u274c linalg_norm \u274c - \u2705 linalg_pinv \u274c - \u274c linalg_qr \u274c - \u274c linalg_slogdet \u274c - \u274c linalg_solve \u274c - \u274c linalg_solve_triangular \u274c - \u274c linalg_svd \u274c - \u274c linalg_tensorinv \u274c - \u274c linalg_tensorsolve \u274c - \u274c linalg_vector_norm \u274c - \u2705 linear \u274c - \u2705 linspace \u274c - \u274c list \u274c - \u274c list_with_default \u274c - \u274c ljust \u274c - \u274c local_value \u274c - \u274c log_normal \u2705 - \u274c log_sigmoid \u274c - \u274c log_sigmoid_forward \u274c - \u274c log_softmax special_log_softmax \u274c - \u2705 logaddexp \u274c - \u274c logaddexp2 \u274c - \u274c logcumsumexp \u274c - \u274c logdet \u274c - \u274c logit special_logit \u274c - \u274c logspace \u274c - \u274c logsumexp special_logsumexp \u274c - \u274c lower \u274c - \u274c lstm \u274c - \u274c lstm_cell \u274c - \u274c lstrip \u274c - \u274c lstsq \u274c - \u274c lu_solve \u274c - \u274c lu_unpack \u274c - \u274c mH adjoint \u274c - \u274c mT \u274c - \u274c manual_seed \u274c - \u274c margin_ranking_loss \u274c - \u274c masked_fill \u2705 - \u2705 masked_select \u274c - \u274c mathremainder \u274c - \u274c matmul linalg_matmul \u274c - \u2705 matrix_H \u274c - \u274c matrix_rank \u274c - \u274c max_pool1d \u274c - \u2705 max_pool1d_with_indices \u274c - \u274c max_pool2d \u274c - \u2705 max_pool3d \u274c - \u2705 max_unpool2d \u274c - \u274c max_unpool3d \u274c - \u274c median \u274c - \u274c meshgrid \u274c - \u274c miopen_batch_norm \u274c - \u274c miopen_convolution \u274c - \u274c miopen_convolution_add_relu \u274c - \u274c miopen_convolution_relu \u274c - \u274c miopen_convolution_transpose \u274c - \u274c miopen_depthwise_convolution \u274c - \u274c miopen_rnn \u274c - \u274c mish \u274c - \u274c mkldnn_adaptive_avg_pool2d \u274c - \u274c mkldnn_convolution \u274c - \u274c mkldnn_linear \u274c - \u274c mkldnn_max_pool2d \u274c - \u274c mkldnn_max_pool3d \u274c - \u274c mkldnn_reorder_conv2d_weight \u274c - \u274c mkldnn_reorder_conv3d_weight \u274c - \u274c mkldnn_rnn_layer \u274c - \u274c mode \u274c - \u274c modf \u274c - \u274c movedim moveaxis \u274c - \u274c mps_linear \u274c - \u274c mse_loss \u274c - \u274c msort \u274c - \u274c multi_margin_loss \u274c - \u274c multilabel_margin_loss \u274c - \u274c multilabel_margin_loss_forward \u274c - \u274c multinomial \u274c - \u274c mv \u274c - \u274c mvlgamma special_multigammaln \u274c - \u274c nan_to_num \u2705 - \u274c nanmean \u274c - \u274c nanmedian \u274c - \u274c nanquantile \u274c - \u274c nansum \u274c - \u274c narrow \u274c - \u2705 narrow_copy \u274c - \u274c native_batch_norm \u274c - \u274c native_channel_shuffle \u274c - \u274c native_multi_head_self_attention \u274c - \u274c native_norm \u274c - \u274c neq \u274c - \u274c nested_to_padded_tensor \u274c - \u274c new_empty \u274c - \u274c new_empty_strided \u274c - \u274c new_full \u274c - \u274c new_ones \u274c - \u274c new_zeros \u274c - \u2705 nextafter \u2705 - \u274c nll_loss \u274c - \u274c nll_loss2d \u274c - \u274c nll_loss_forward \u274c - \u274c nll_loss_nd \u274c - \u274c node \u274c - \u274c nonzero_numpy \u274c - \u274c nonzero_static \u274c - \u274c norm \u274c - \u2705 norm_except_dim \u274c - \u274c normal \u2705 - \u274c normal_functional \u274c - \u274c nuclear_norm \u274c - \u274c numel \u274c - \u2705 numpy_T \u274c - \u274c oct \u274c - \u274c one_hot \u274c - \u274c ones \u274c - \u2705 ones_like \u274c - \u2705 op \u274c - \u274c op_name \u274c - \u274c ord \u274c - \u274c ormqr \u274c - \u274c outer ger \u274c - \u274c output_nr \u274c - \u274c owner \u274c - \u274c owner_name \u274c - \u274c pad \u274c - \u2705 pad_sequence \u274c - \u274c pairwise_distance \u274c - \u274c partition \u274c - \u274c pdist \u274c - \u274c percentFormat \u274c - \u274c permute_copy \u274c - \u274c pin_memory \u274c - \u274c pinv \u274c - \u274c pinverse \u274c - \u274c pixel_shuffle \u274c - \u274c pixel_unshuffle \u274c - \u274c pointwise_placeholder \u274c - \u274c poisson \u274c - \u274c poisson_nll_loss \u274c - \u274c polar \u274c - \u274c polygamma special_polygamma \u2705 - \u274c pop \u274c - \u274c popitem \u274c - \u274c positive \u274c - \u274c prelu \u274c - \u2705 promote_types \u274c - \u274c put \u2705 - \u274c q_per_channel_axis \u274c - \u274c q_per_channel_scales \u274c - \u274c q_per_channel_zero_points \u274c - \u274c q_scale \u274c - \u274c q_zero_point \u274c - \u274c qr \u274c - \u274c qscheme \u274c - \u274c quantile \u274c - \u274c quantize \u274c - \u274c quantize_per_channel \u274c - \u274c quantize_per_tensor \u274c - \u2705 quantize_per_tensor_dynamic \u274c - \u274c quantized_batch_norm \u274c - \u274c quantized_gru \u274c - \u274c quantized_lstm \u274c - \u274c quantized_max_pool1d \u274c - \u274c quantized_max_pool2d \u274c - \u274c quantized_max_pool3d \u274c - \u274c rad2deg \u274c - \u274c radians \u274c - \u274c rand_like \u274c - \u274c randint \u274c - \u274c randint_like \u274c - \u274c randn_like \u274c - \u274c random \u2705 - \u274c range \u274c - \u274c ravel \u274c - \u274c real \u274c - \u274c record_stream \u274c - \u274c reduce_scatter_tensor \u274c - \u274c refine_names \u274c - \u274c relu6 \u2705 - \u2705 remove \u274c - \u274c rename \u2705 - \u274c renorm \u274c - \u274c repeat_interleave \u274c - \u2705 replace \u274c - \u274c replication_pad1d \u274c - \u2705 requires_grad_ \u274c - \u274c reshape \u274c - \u2705 reshape_as \u274c - \u274c resize \u2705 - \u274c resize_as_ \u274c - \u274c resize_as_sparse \u2705 - \u274c resolve_conj \u274c - \u274c resolve_neg \u274c - \u274c result_type \u274c - \u274c retain_grad \u274c - \u274c retains_grad \u274c - \u274c reverse \u274c - \u274c rfind \u274c - \u274c rindex \u274c - \u274c rjust \u274c - \u274c rnn_relu \u274c - \u274c rnn_relu_cell \u274c - \u274c rnn_tanh \u274c - \u274c rnn_tanh_cell \u274c - \u274c roll \u274c - \u2705 rot90 \u274c - \u274c row_indices \u274c - \u274c row_indices_copy \u274c - \u274c rowwise_prune \u274c - \u274c rpartition \u274c - \u274c rrelu \u274c - \u274c rrelu_with_noise \u2705 - \u274c rrelu_with_noise_functional \u274c - \u274c rsplit \u274c - \u274c rstrip \u274c - \u274c rsub \u274c - \u2705 save \u274c - \u274c scaled_dot_product_attention \u274c - \u2705 searchsorted \u274c - \u274c seed \u274c - \u274c segment_reduce \u274c - \u274c select_copy \u274c - \u274c selu \u2705 - \u2705 set \u2705 - \u274c set_data \u274c - \u274c set_grad_enabled \u274c - \u274c set_source_Tensor_storage_offset \u274c - \u274c setdefault \u274c - \u274c sgn \u274c - \u274c signbit \u274c - \u274c silu \u2705 - \u2705 sinc special_sinc \u274c - \u274c size \u274c - \u2705 slice_copy \u274c - \u274c slice_inverse \u274c - \u274c slow_conv3d \u274c - \u274c slow_conv3d_forward \u274c - \u274c slow_conv_dilated2d \u274c - \u274c slow_conv_dilated3d \u274c - \u274c slow_conv_transpose2d \u274c - \u274c slow_conv_transpose3d \u274c - \u274c smm \u274c - \u274c smooth_l1_loss \u274c - \u274c soft_margin_loss \u274c - \u274c softmax special_softmax \u274c - \u2705 softplus \u274c - \u2705 softshrink \u274c - \u274c solve \u274c - \u274c sorted \u274c - \u274c sparse_compressed_tensor \u274c - \u274c sparse_coo_tensor \u274c - \u274c sparse_dim \u274c - \u274c sparse_mask \u274c - \u274c sparse_resize \u2705 - \u274c sparse_resize_and_clear \u2705 - \u274c sparse_sampled_addmm \u274c - \u274c special_airy_ai \u274c - \u274c special_bessel_j0 \u274c - \u274c special_bessel_j1 \u274c - \u274c special_bessel_y0 \u274c - \u274c special_bessel_y1 \u274c - \u274c special_chebyshev_polynomial_t \u274c - \u274c special_chebyshev_polynomial_u \u274c - \u274c special_chebyshev_polynomial_v \u274c - \u274c special_chebyshev_polynomial_w \u274c - \u274c special_entr \u274c - \u274c special_erfcx \u274c - \u274c special_hermite_polynomial_h \u274c - \u274c special_hermite_polynomial_he \u274c - \u274c special_i0e \u274c - \u274c special_i1 \u274c - \u274c special_i1e \u274c - \u274c special_laguerre_polynomial_l \u274c - \u274c special_legendre_polynomial_p \u274c - \u274c special_log_ndtr \u274c - \u274c special_modified_bessel_i0 \u274c - \u274c special_modified_bessel_i1 \u274c - \u274c special_modified_bessel_k0 \u274c - \u274c special_modified_bessel_k1 \u274c - \u274c special_ndtr \u274c - \u274c special_ndtri \u274c - \u274c special_scaled_modified_bessel_k0 \u274c - \u274c special_scaled_modified_bessel_k1 \u274c - \u274c special_shifted_chebyshev_polynomial_t \u274c - \u274c special_shifted_chebyshev_polynomial_u \u274c - \u274c special_shifted_chebyshev_polynomial_v \u274c - \u274c special_shifted_chebyshev_polynomial_w \u274c - \u274c special_spherical_bessel_j0 \u274c - \u274c special_xlog1py \u274c - \u274c special_zeta \u274c - \u274c split \u274c - \u274c split_copy \u274c - \u274c split_with_sizes_copy \u274c - \u274c splitlines \u274c - \u274c square \u2705 - \u274c sspaddmm \u274c - \u274c stack \u274c - \u2705 startswith \u274c - \u274c std \u274c - \u274c std_mean \u274c - \u274c stft \u274c - \u2705 storage_offset \u274c - \u274c str \u274c - \u274c stride \u274c - \u274c strip \u274c - \u274c sum_to \u274c - \u274c svd \u274c - \u274c swapcase \u274c - \u274c symbolic_b \u274c - \u274c symeig \u274c - \u274c t \u2705 - \u274c take \u274c - \u274c take_along_dim \u274c - \u274c tanhshrink \u274c - \u274c tensor \u274c - \u274c tensor_split \u274c - \u274c tensordot \u274c - \u274c test \u274c - \u274c test_symbol \u274c - \u274c test_vartype \u274c - \u274c test_vartype2 \u274c - \u274c thnn_conv2d \u274c - \u274c thnn_conv2d_forward \u274c - \u274c threshold \u274c - \u274c tile \u274c - \u274c title \u274c - \u274c to \u274c - \u2705 to_dense \u274c - \u274c to_here \u274c - \u274c to_mkldnn \u274c - \u274c to_padded_tensor \u274c - \u274c trace \u274c - \u274c transpose swapaxes, swapdims \u2705 - \u2705 transpose_copy \u274c - \u274c trapezoid \u274c - \u274c trapz \u274c - \u274c triangular_solve \u274c - \u274c tril \u274c - \u2705 tril_indices \u274c - \u274c triplet_margin_loss \u274c - \u274c triu \u274c - \u2705 triu_indices \u274c - \u274c type_as \u274c - \u2705 unbind \u274c - \u2705 unbind_copy \u274c - \u274c unflatten \u274c - \u2705 unflatten_dense_tensors \u274c - \u274c unfold \u274c - \u274c unfold_copy \u274c - \u274c uniform \u2705 - \u274c unique_consecutive \u274c - \u274c unique_dim \u274c - \u274c unique_dim_consecutive \u274c - \u274c unknown \u274c - \u274c unsafe_chunk \u274c - \u274c unsafe_split \u274c - \u274c unsafe_split_with_sizes \u274c - \u274c unsqueeze_copy \u274c - \u274c update \u274c - \u274c upper \u274c - \u274c upsample \u274c - \u274c upsample_bicubic2d \u274c - \u274c upsample_linear1d \u274c - \u274c upsample_nearest1d \u274c - \u274c upsample_nearest3d \u274c - \u274c upsample_trilinear3d \u274c - \u274c values \u274c - \u274c values_copy \u274c - \u274c vander \u274c - \u274c var_mean \u274c - \u274c vdot \u274c - \u274c view_as \u274c - \u274c view_as_complex \u274c - \u2705 view_as_complex_copy \u274c - \u274c view_as_real \u274c - \u2705 view_as_real_copy \u274c - \u274c view_copy \u274c - \u274c view_expand_placeholder \u274c - \u274c vsplit \u274c - \u274c vstack row_stack \u274c - \u2705 wait \u274c - \u274c wait_tensor \u274c - \u274c warn \u274c - \u274c warns \u274c - \u274c wrapped_linear_prepack \u274c - \u274c wrapped_quantized_linear_prepacked \u274c - \u274c xlogy special_xlogy \u274c - \u274c zero \u2705 - \u274c zeros \u274c - \u2705 zeros_like \u274c - \u2705 zfill \u274c - \u274c <p>Total matched operators in builtin PyTorch <code>ONNX</code> support based on this page compared to:</p> <ul> <li>core PyTorch opset:</li> </ul> <p> <p>123/138</p> </p> <ul> <li>and support from full <code>aten::</code>:</li> </ul> <p> <p>320/862</p> </p> <p>(total registered aten operators in t2n being 348)</p> aten name aliases can in-place is core translated abs absolute \u2705 \u2705 \u2705 acos arccos \u2705 \u2705 \u2705 acosh arccosh \u2705 \u2705 \u274c adaptive_avg_pool1d \u274c \u2705 \u2705 add \u2705 \u2705 \u2705 addmm \u2705 \u2705 \u2705 alias \u274c \u2705 \u2705 amax \u274c \u2705 \u2705 amin \u274c \u2705 \u2705 any \u274c \u2705 \u2705 arange \u274c \u2705 \u2705 argmax \u274c \u2705 \u2705 argmin \u274c \u2705 \u2705 as_strided \u2705 \u2705 \u2705 asin arcsin \u2705 \u2705 \u2705 asinh arcsinh \u2705 \u2705 \u274c atan arctan \u2705 \u2705 \u2705 atan2 arctan2 \u2705 \u2705 \u2705 atanh arctanh \u2705 \u2705 \u274c avg_pool1d \u274c \u2705 \u2705 avg_pool2d \u274c \u2705 \u2705 avg_pool3d \u274c \u2705 \u2705 bitwise_and \u2705 \u2705 \u2705 bitwise_not \u2705 \u2705 \u2705 bitwise_or \u2705 \u2705 \u2705 bitwise_xor \u2705 \u2705 \u274c bmm \u274c \u2705 \u2705 cat concat, concatenate \u274c \u2705 \u2705 ceil \u2705 \u2705 \u2705 clamp clip \u2705 \u2705 \u2705 clone \u274c \u2705 \u2705 col2im \u274c \u2705 \u2705 constant_pad_nd \u274c \u2705 \u2705 convolution \u274c \u2705 \u2705 copy \u2705 \u2705 \u274c cos \u2705 \u2705 \u2705 cosh \u2705 \u2705 \u274c cumsum \u2705 \u2705 \u2705 diagonal \u274c \u2705 \u2705 div divide, true_divide \u2705 \u2705 \u2705 elu \u2705 \u2705 \u2705 embedding \u274c \u2705 \u2705 empty \u274c \u2705 \u2705 empty_strided \u274c \u2705 \u274c eq \u2705 \u2705 \u2705 erf special_erf \u2705 \u2705 \u2705 exp \u2705 \u2705 \u2705 expand \u274c \u2705 \u2705 expm1 special_expm1 \u2705 \u2705 \u274c fill \u2705 \u2705 \u2705 flip \u274c \u2705 \u2705 floor \u2705 \u2705 \u2705 fmod \u2705 \u2705 \u2705 full \u274c \u2705 \u2705 full_like \u274c \u2705 \u2705 gather \u274c \u2705 \u2705 ge greater_equal \u2705 \u2705 \u2705 gelu \u2705 \u2705 \u2705 grid_sampler_2d \u274c \u2705 \u274c gt greater \u2705 \u2705 \u2705 hardtanh \u2705 \u2705 \u2705 index \u274c \u2705 \u2705 index_put \u2705 \u2705 \u2705 index_select \u274c \u2705 \u2705 isinf \u274c \u2705 \u2705 isnan \u274c \u2705 \u2705 le less_equal \u2705 \u2705 \u2705 leaky_relu \u2705 \u2705 \u2705 log \u2705 \u2705 \u2705 log10 \u2705 \u2705 \u2705 log1p special_log1p \u2705 \u2705 \u2705 log2 \u2705 \u2705 \u2705 logical_and \u2705 \u2705 \u2705 logical_not \u2705 \u2705 \u2705 logical_or \u2705 \u2705 \u2705 logical_xor \u2705 \u2705 \u2705 lt less \u2705 \u2705 \u2705 masked_scatter \u2705 \u2705 \u2705 max \u274c \u2705 \u2705 max_pool2d_with_indices \u274c \u2705 \u2705 max_pool3d_with_indices \u274c \u2705 \u2705 maximum \u274c \u2705 \u2705 mean \u274c \u2705 \u2705 min \u274c \u2705 \u2705 minimum \u274c \u2705 \u2705 mm \u274c \u2705 \u2705 mul multiply \u2705 \u2705 \u2705 native_dropout \u274c \u2705 \u2705 native_group_norm \u274c \u2705 \u274c native_layer_norm \u274c \u2705 \u2705 ne not_equal \u2705 \u2705 \u2705 neg negative \u2705 \u2705 \u2705 nonzero \u274c \u2705 \u2705 permute \u274c \u2705 \u2705 pow \u2705 \u2705 \u2705 prod \u274c \u2705 \u2705 rand \u274c \u2705 \u2705 randn \u274c \u2705 \u2705 randperm \u274c \u2705 \u274c reciprocal \u2705 \u2705 \u2705 reflection_pad1d \u274c \u2705 \u2705 reflection_pad2d \u274c \u2705 \u2705 reflection_pad3d \u274c \u2705 \u2705 relu \u2705 \u2705 \u2705 remainder \u2705 \u2705 \u2705 repeat \u274c \u2705 \u2705 replication_pad2d \u274c \u2705 \u2705 replication_pad3d \u274c \u2705 \u2705 round special_round \u2705 \u2705 \u2705 rsqrt \u2705 \u2705 \u2705 scalar_tensor \u274c \u2705 \u2705 scatter \u2705 \u2705 \u2705 scatter_add \u2705 \u2705 \u2705 scatter_reduce \u2705 \u2705 \u2705 select \u274c \u2705 \u2705 select_scatter \u274c \u2705 \u274c sigmoid special_expit \u2705 \u2705 \u2705 sign \u2705 \u2705 \u2705 sin \u2705 \u2705 \u2705 sinh \u2705 \u2705 \u274c slice \u274c \u2705 \u2705 slice_scatter \u274c \u2705 \u274c sort \u274c \u2705 \u2705 split_with_sizes \u274c \u2705 \u2705 sqrt \u2705 \u2705 \u2705 squeeze \u2705 \u2705 \u2705 sub subtract \u2705 \u2705 \u2705 sum \u274c \u2705 \u2705 tan \u2705 \u2705 \u2705 tanh \u2705 \u2705 \u2705 topk \u274c \u2705 \u2705 trunc fix \u2705 \u2705 \u274c unsqueeze \u2705 \u2705 \u2705 upsample_bilinear2d \u274c \u2705 \u2705 upsample_nearest2d \u274c \u2705 \u2705 var \u274c \u2705 \u2705 view \u274c \u2705 \u2705 where \u274c \u2705 \u2705 adaptive_avg_pool2d \u274c - \u2705 adaptive_avg_pool3d \u274c - \u2705 adaptive_max_pool1d \u274c - \u2705 adaptive_max_pool2d \u274c - \u2705 adaptive_max_pool3d \u274c - \u2705 addbmm \u2705 - \u274c addcdiv \u274c - \u274c addcmul \u274c - \u2705 addmv \u2705 - \u274c addr \u2705 - \u274c affine_grid_generator \u274c - \u2705 alias_copy \u274c - \u274c align_as \u274c - \u274c align_tensors \u274c - \u274c align_to \u274c - \u274c all \u274c - \u2705 all_gather_into_tensor \u274c - \u274c all_reduce \u274c - \u274c allclose \u274c - \u274c alpha_dropout \u2705 - \u2705 aminmax \u274c - \u2705 angle \u274c - \u274c append \u274c - \u2705 argsort \u274c - \u2705 argwhere \u274c - \u274c as_strided_copy \u274c - \u274c as_strided_scatter \u274c - \u274c as_tensor \u274c - \u2705 atleast_1d \u274c - \u2705 atleast_2d \u274c - \u2705 atleast_3d \u274c - \u2705 baddbmm \u2705 - \u2705 bartlett_window \u274c - \u274c batch_norm \u274c - \u2705 batch_norm_elemt \u274c - \u274c batch_norm_gather_stats \u274c - \u274c batch_norm_gather_stats_with_counts \u274c - \u274c batch_norm_stats \u274c - \u274c batch_norm_update_stats \u274c - \u274c bernoulli \u2705 - \u2705 bilinear \u274c - \u274c bin \u274c - \u274c binary_cross_entropy \u274c - \u274c binary_cross_entropy_with_logits \u274c - \u2705 bincount \u274c - \u274c binomial \u274c - \u274c bitwise_left_shift \u2705 - \u2705 bitwise_right_shift \u2705 - \u2705 blackman_window \u274c - \u274c block_diag \u274c - \u274c broadcast_tensors \u274c - \u2705 broadcast_to \u274c - \u2705 bucketize \u274c - \u2705 can_cast \u274c - \u274c capitalize \u274c - \u274c cartesian_prod \u274c - \u274c cauchy \u2705 - \u274c ccol_indices \u274c - \u274c ccol_indices_copy \u274c - \u274c cdist \u274c - \u2705 celu \u274c - \u2705 center \u274c - \u274c chain_matmul \u274c - \u274c chalf \u274c - \u274c channel_shuffle \u274c - \u274c cholesky \u274c - \u274c cholesky_inverse \u274c - \u274c cholesky_solve \u274c - \u274c choose_qparams_optimized \u274c - \u274c chr \u274c - \u274c chunk \u274c - \u2705 clamp_max \u2705 - \u2705 clamp_min \u2705 - \u2705 clear \u274c - \u274c coalesce \u274c - \u274c col_indices \u274c - \u274c col_indices_copy \u274c - \u274c column_stack \u274c - \u274c combinations \u274c - \u274c complex \u274c - \u274c confirmed_by_owner \u274c - \u274c conj \u274c - \u274c conj_physical \u2705 - \u2705 contiguous \u274c - \u2705 conv \u274c - \u274c conv1d \u274c - \u2705 conv2d \u274c - \u2705 conv3d \u274c - \u2705 conv_depthwise3d \u274c - \u274c conv_tbc \u274c - \u2705 conv_transpose1d \u274c - \u2705 conv_transpose2d \u274c - \u2705 conv_transpose3d \u274c - \u2705 convolution_overrideable \u274c - \u274c convrelu \u274c - \u274c copy_sparse_to_sparse \u2705 - \u274c copy_to \u274c - \u274c copysign \u2705 - \u274c corrcoef \u274c - \u274c cosine_embedding_loss \u274c - \u274c cosine_similarity \u274c - \u2705 count \u274c - \u274c count_nonzero \u274c - \u274c cpu \u274c - \u274c cross \u274c - \u2705 cross_entropy_loss \u274c - \u2705 crow_indices \u274c - \u274c crow_indices_copy \u274c - \u274c ctc_loss \u274c - \u274c cuda \u274c - \u274c cudnn_affine_grid_generator \u274c - \u274c cudnn_batch_norm \u274c - \u274c cudnn_convolution \u274c - \u274c cudnn_convolution_add_relu \u274c - \u274c cudnn_convolution_relu \u274c - \u274c cudnn_convolution_transpose \u274c - \u274c cudnn_grid_sampler \u274c - \u274c cudnn_is_acceptable \u274c - \u274c cummax \u274c - \u274c cummin \u274c - \u274c cumprod \u2705 - \u274c cumulative_trapezoid \u274c - \u274c data \u274c - \u274c deg2rad \u274c - \u274c degrees \u274c - \u274c dense_dim \u274c - \u274c dequantize \u274c - \u2705 detach \u2705 - \u2705 detach_copy \u274c - \u274c device \u274c - \u274c diag \u274c - \u274c diag_embed \u274c - \u274c diagflat \u274c - \u274c diagonal_copy \u274c - \u274c diagonal_scatter \u274c - \u274c dict \u274c - \u274c diff \u274c - \u274c digamma special_digamma, special_psi \u2705 - \u274c dim \u274c - \u2705 dist \u274c - \u274c divmod \u274c - \u274c dot \u274c - \u2705 dropout \u2705 - \u2705 dsplit \u274c - \u274c dstack \u274c - \u274c dtype \u274c - \u274c eig \u274c - \u274c einsum \u274c - \u2705 element_size \u274c - \u274c embedding_bag \u274c - \u2705 embedding_renorm \u2705 - \u2705 empty_like \u274c - \u2705 empty_permuted \u274c - \u274c empty_quantized \u274c - \u274c enable_grad \u274c - \u274c endswith \u274c - \u274c equal \u274c - \u274c erfc special_erfc \u2705 - \u274c erfinv special_erfinv \u2705 - \u274c exp2 special_exp2 \u2705 - \u274c expand_as \u274c - \u2705 expand_copy \u274c - \u274c expandtabs \u274c - \u274c exponential \u2705 - \u274c extend \u274c - \u274c eye \u274c - \u2705 fabs \u274c - \u274c factorial \u274c - \u274c fake_quantize_per_channel_affine \u274c - \u2705 fake_quantize_per_channel_affine_cachemask \u274c - \u274c fake_quantize_per_tensor_affine \u274c - \u2705 fake_quantize_per_tensor_affine_cachemask \u274c - \u274c feature_alpha_dropout \u2705 - \u2705 feature_dropout \u2705 - \u2705 fft_fftfreq \u274c - \u274c fft_ihfft2 \u274c - \u274c fft_ihfftn \u274c - \u274c fft_irfftn \u274c - \u274c fft_rfftfreq \u274c - \u274c fft_rfftn \u274c - \u274c fill_diagonal_ \u274c - \u274c find \u274c - \u274c flatten \u274c - \u2705 flatten_dense_tensors \u274c - \u274c fliplr \u274c - \u274c flipud \u274c - \u274c float_power \u2705 - \u274c floor_divide \u2705 - \u2705 floordiv \u274c - \u2705 fmax \u274c - \u274c fmin \u274c - \u274c foo \u274c - \u274c fork \u274c - \u274c format \u274c - \u274c frac \u274c - \u274c fractional_max_pool2d \u274c - \u274c fractional_max_pool3d \u274c - \u274c frexp \u274c - \u274c frobenius_norm \u274c - \u2705 from_file \u274c - \u274c fused_moving_avg_obs_fake_quant \u274c - \u274c gamma \u274c - \u274c gcd \u2705 - \u274c geometric \u2705 - \u274c geqrf \u274c - \u274c get \u274c - \u274c get_autocast_dtype \u274c - \u274c get_device \u274c - \u274c get_gradients \u274c - \u274c get_pool_ceil_padding \u274c - \u274c getelem \u274c - \u274c glu \u274c - \u2705 glu_jvp \u274c - \u274c grad \u274c - \u274c grid_sampler \u274c - \u2705 grid_sampler_3d \u274c - \u274c group_norm \u274c - \u2705 gru \u274c - \u2705 gru_cell \u274c - \u274c hamming_window \u274c - \u274c hann_window \u274c - \u2705 hardshrink \u2705 - \u2705 hardsigmoid \u2705 - \u2705 hardswish \u2705 - \u2705 has_torch_function \u274c - \u274c hash \u274c - \u274c heaviside \u274c - \u274c hex \u274c - \u274c hinge_embedding_loss \u274c - \u274c histc \u274c - \u274c histogram \u274c - \u274c histogramdd \u274c - \u274c hsplit \u274c - \u274c hspmm \u274c - \u274c hstack \u274c - \u2705 huber_loss \u274c - \u274c hypot \u2705 - \u274c i0 special_i0 \u2705 - \u274c igamma special_gammainc \u2705 - \u274c igammac special_gammaincc \u2705 - \u274c iinfo \u274c - \u274c im2col \u274c - \u2705 imag \u274c - \u274c index_add \u274c - \u2705 index_copy \u274c - \u2705 index_fill \u274c - \u2705 index_put_impl_ \u274c - \u274c index_reduce \u2705 - \u274c indices \u274c - \u274c indices_copy \u274c - \u274c initial_seed \u274c - \u274c inner \u274c - \u274c insert \u274c - \u2705 instance_norm \u274c - \u2705 int_repr \u274c - \u274c is_autocast_cpu_enabled \u274c - \u274c is_autocast_enabled \u274c - \u274c is_coalesced \u274c - \u274c is_complex \u274c - \u274c is_conj \u274c - \u274c is_contiguous \u274c - \u274c is_cuda \u274c - \u274c is_floating_point \u274c - \u2705 is_grad_enabled \u274c - \u274c is_leaf \u274c - \u274c is_non_overlapping_and_dense \u274c - \u274c is_nonzero \u274c - \u274c is_owner \u274c - \u274c is_pinned \u274c - \u2705 is_same_size \u274c - \u274c is_scripting \u274c - \u274c is_set_to \u274c - \u274c is_signed \u274c - \u274c is_strides_like_format \u274c - \u274c isalnum \u274c - \u274c isalpha \u274c - \u274c isclose \u274c - \u274c isdecimal \u274c - \u274c isdigit \u274c - \u274c isfinite \u274c - \u2705 isidentifier \u274c - \u274c isin \u274c - \u274c islower \u274c - \u274c isneginf \u274c - \u274c isnumeric \u274c - \u274c isposinf \u274c - \u274c isprintable \u274c - \u274c isreal \u274c - \u274c isspace \u274c - \u274c istft \u274c - \u274c istitle \u274c - \u274c isupper \u274c - \u274c item \u274c - \u2705 items \u274c - \u274c join \u274c - \u274c kaiser_window \u274c - \u274c keys \u274c - \u274c kl_div \u274c - \u2705 kron \u274c - \u274c kthvalue \u274c - \u274c l1_loss \u274c - \u274c layer_norm \u274c - \u2705 lcm \u2705 - \u274c ldexp \u2705 - \u274c len \u274c - \u2705 lerp \u2705 - \u2705 lgamma \u2705 - \u274c lift \u274c - \u2705 lift_fresh \u274c - \u274c lift_fresh_copy \u274c - \u274c linalg_cholesky_ex \u274c - \u274c linalg_cond \u274c - \u274c linalg_cross \u274c - \u2705 linalg_det det \u274c - \u2705 linalg_diagonal \u274c - \u274c linalg_eig \u274c - \u274c linalg_eigh \u274c - \u274c linalg_eigvals \u274c - \u274c linalg_eigvalsh \u274c - \u274c linalg_householder_product orgqr \u274c - \u274c linalg_inv inverse \u274c - \u274c linalg_inv_ex \u274c - \u274c linalg_ldl_factor_ex \u274c - \u274c linalg_ldl_solve \u274c - \u274c linalg_lstsq \u274c - \u274c linalg_lu \u274c - \u274c linalg_lu_factor_ex \u274c - \u274c linalg_lu_solve \u274c - \u274c linalg_matrix_exp matrix_exp \u274c - \u274c linalg_matrix_norm \u274c - \u2705 linalg_matrix_power matrix_power \u274c - \u274c linalg_matrix_rank \u274c - \u274c linalg_norm \u274c - \u2705 linalg_pinv \u274c - \u274c linalg_qr \u274c - \u274c linalg_slogdet \u274c - \u274c linalg_solve \u274c - \u274c linalg_solve_triangular \u274c - \u274c linalg_svd \u274c - \u274c linalg_tensorinv \u274c - \u274c linalg_tensorsolve \u274c - \u274c linalg_vector_norm \u274c - \u2705 linear \u274c - \u2705 linspace \u274c - \u2705 list \u274c - \u2705 list_with_default \u274c - \u274c ljust \u274c - \u274c local_value \u274c - \u274c log_normal \u2705 - \u274c log_sigmoid \u274c - \u2705 log_sigmoid_forward \u274c - \u274c log_softmax special_log_softmax \u274c - \u2705 logaddexp \u274c - \u274c logaddexp2 \u274c - \u274c logcumsumexp \u274c - \u274c logdet \u274c - \u2705 logit special_logit \u274c - \u2705 logspace \u274c - \u274c logsumexp special_logsumexp \u274c - \u2705 lower \u274c - \u274c lstm \u274c - \u2705 lstm_cell \u274c - \u2705 lstrip \u274c - \u274c lstsq \u274c - \u274c lu_solve \u274c - \u274c lu_unpack \u274c - \u274c mH adjoint \u274c - \u274c mT \u274c - \u274c manual_seed \u274c - \u274c margin_ranking_loss \u274c - \u274c masked_fill \u2705 - \u2705 masked_select \u274c - \u2705 mathremainder \u274c - \u274c matmul linalg_matmul \u274c - \u2705 matrix_H \u274c - \u274c matrix_rank \u274c - \u274c max_pool1d \u274c - \u2705 max_pool1d_with_indices \u274c - \u2705 max_pool2d \u274c - \u2705 max_pool3d \u274c - \u2705 max_unpool2d \u274c - \u274c max_unpool3d \u274c - \u274c median \u274c - \u274c meshgrid \u274c - \u2705 miopen_batch_norm \u274c - \u274c miopen_convolution \u274c - \u274c miopen_convolution_add_relu \u274c - \u274c miopen_convolution_relu \u274c - \u274c miopen_convolution_transpose \u274c - \u274c miopen_depthwise_convolution \u274c - \u274c miopen_rnn \u274c - \u274c mish \u274c - \u2705 mkldnn_adaptive_avg_pool2d \u274c - \u274c mkldnn_convolution \u274c - \u274c mkldnn_linear \u274c - \u274c mkldnn_max_pool2d \u274c - \u274c mkldnn_max_pool3d \u274c - \u274c mkldnn_reorder_conv2d_weight \u274c - \u274c mkldnn_reorder_conv3d_weight \u274c - \u274c mkldnn_rnn_layer \u274c - \u274c mode \u274c - \u274c modf \u274c - \u274c movedim moveaxis \u274c - \u2705 mps_linear \u274c - \u274c mse_loss \u274c - \u2705 msort \u274c - \u274c multi_margin_loss \u274c - \u274c multilabel_margin_loss \u274c - \u274c multilabel_margin_loss_forward \u274c - \u274c multinomial \u274c - \u2705 mv \u274c - \u2705 mvlgamma special_multigammaln \u274c - \u274c nan_to_num \u2705 - \u2705 nanmean \u274c - \u274c nanmedian \u274c - \u274c nanquantile \u274c - \u274c nansum \u274c - \u274c narrow \u274c - \u2705 narrow_copy \u274c - \u274c native_batch_norm \u274c - \u274c native_channel_shuffle \u274c - \u274c native_multi_head_self_attention \u274c - \u274c native_norm \u274c - \u274c neq \u274c - \u274c nested_to_padded_tensor \u274c - \u274c new_empty \u274c - \u2705 new_empty_strided \u274c - \u274c new_full \u274c - \u2705 new_ones \u274c - \u2705 new_zeros \u274c - \u2705 nextafter \u2705 - \u274c nll_loss \u274c - \u2705 nll_loss2d \u274c - \u2705 nll_loss_forward \u274c - \u274c nll_loss_nd \u274c - \u2705 node \u274c - \u274c nonzero_numpy \u274c - \u2705 nonzero_static \u274c - \u274c norm \u274c - \u2705 norm_except_dim \u274c - \u274c normal \u2705 - \u2705 normal_functional \u274c - \u274c nuclear_norm \u274c - \u274c numel \u274c - \u2705 numpy_T \u274c - \u2705 oct \u274c - \u274c one_hot \u274c - \u2705 ones \u274c - \u2705 ones_like \u274c - \u2705 op \u274c - \u274c op_name \u274c - \u274c ord \u274c - \u274c ormqr \u274c - \u274c outer ger \u274c - \u2705 output_nr \u274c - \u274c owner \u274c - \u274c owner_name \u274c - \u274c pad \u274c - \u2705 pad_sequence \u274c - \u274c pairwise_distance \u274c - \u2705 partition \u274c - \u274c pdist \u274c - \u274c percentFormat \u274c - \u274c permute_copy \u274c - \u274c pin_memory \u274c - \u274c pinv \u274c - \u274c pinverse \u274c - \u274c pixel_shuffle \u274c - \u2705 pixel_unshuffle \u274c - \u2705 pointwise_placeholder \u274c - \u274c poisson \u274c - \u274c poisson_nll_loss \u274c - \u274c polar \u274c - \u274c polygamma special_polygamma \u2705 - \u274c pop \u274c - \u2705 popitem \u274c - \u274c positive \u274c - \u274c prelu \u274c - \u2705 promote_types \u274c - \u274c put \u2705 - \u274c q_per_channel_axis \u274c - \u274c q_per_channel_scales \u274c - \u274c q_per_channel_zero_points \u274c - \u274c q_scale \u274c - \u274c q_zero_point \u274c - \u274c qr \u274c - \u274c qscheme \u274c - \u274c quantile \u274c - \u274c quantize \u274c - \u274c quantize_per_channel \u274c - \u274c quantize_per_tensor \u274c - \u2705 quantize_per_tensor_dynamic \u274c - \u274c quantized_batch_norm \u274c - \u274c quantized_gru \u274c - \u274c quantized_lstm \u274c - \u274c quantized_max_pool1d \u274c - \u274c quantized_max_pool2d \u274c - \u274c quantized_max_pool3d \u274c - \u274c rad2deg \u274c - \u274c radians \u274c - \u274c rand_like \u274c - \u2705 randint \u274c - \u2705 randint_like \u274c - \u2705 randn_like \u274c - \u2705 random \u2705 - \u274c range \u274c - \u274c ravel \u274c - \u274c real \u274c - \u274c record_stream \u274c - \u274c reduce_scatter_tensor \u274c - \u274c refine_names \u274c - \u274c relu6 \u2705 - \u2705 remove \u274c - \u274c rename \u2705 - \u274c renorm \u274c - \u274c repeat_interleave \u274c - \u2705 replace \u274c - \u274c replication_pad1d \u274c - \u2705 requires_grad_ \u274c - \u274c reshape \u274c - \u2705 reshape_as \u274c - \u2705 resize \u2705 - \u274c resize_as_ \u274c - \u274c resize_as_sparse \u2705 - \u274c resolve_conj \u274c - \u2705 resolve_neg \u274c - \u2705 result_type \u274c - \u274c retain_grad \u274c - \u274c retains_grad \u274c - \u274c reverse \u274c - \u274c rfind \u274c - \u274c rindex \u274c - \u274c rjust \u274c - \u274c rnn_relu \u274c - \u2705 rnn_relu_cell \u274c - \u274c rnn_tanh \u274c - \u2705 rnn_tanh_cell \u274c - \u274c roll \u274c - \u2705 rot90 \u274c - \u274c row_indices \u274c - \u274c row_indices_copy \u274c - \u274c rowwise_prune \u274c - \u274c rpartition \u274c - \u274c rrelu \u274c - \u2705 rrelu_with_noise \u2705 - \u274c rrelu_with_noise_functional \u274c - \u274c rsplit \u274c - \u274c rstrip \u274c - \u274c rsub \u274c - \u2705 save \u274c - \u274c scaled_dot_product_attention \u274c - \u2705 searchsorted \u274c - \u274c seed \u274c - \u274c segment_reduce \u274c - \u274c select_copy \u274c - \u274c selu \u2705 - \u2705 set \u2705 - \u274c set_data \u274c - \u274c set_grad_enabled \u274c - \u274c set_source_Tensor_storage_offset \u274c - \u274c setdefault \u274c - \u274c sgn \u274c - \u274c signbit \u274c - \u274c silu \u2705 - \u2705 sinc special_sinc \u274c - \u274c size \u274c - \u2705 slice_copy \u274c - \u274c slice_inverse \u274c - \u274c slow_conv3d \u274c - \u274c slow_conv3d_forward \u274c - \u274c slow_conv_dilated2d \u274c - \u274c slow_conv_dilated3d \u274c - \u274c slow_conv_transpose2d \u274c - \u274c slow_conv_transpose3d \u274c - \u274c smm \u274c - \u274c smooth_l1_loss \u274c - \u274c soft_margin_loss \u274c - \u274c softmax special_softmax \u274c - \u2705 softplus \u274c - \u2705 softshrink \u274c - \u2705 solve \u274c - \u274c sorted \u274c - \u274c sparse_compressed_tensor \u274c - \u274c sparse_coo_tensor \u274c - \u274c sparse_dim \u274c - \u274c sparse_mask \u274c - \u274c sparse_resize \u2705 - \u274c sparse_resize_and_clear \u2705 - \u274c sparse_sampled_addmm \u274c - \u274c special_airy_ai \u274c - \u274c special_bessel_j0 \u274c - \u274c special_bessel_j1 \u274c - \u274c special_bessel_y0 \u274c - \u274c special_bessel_y1 \u274c - \u274c special_chebyshev_polynomial_t \u274c - \u274c special_chebyshev_polynomial_u \u274c - \u274c special_chebyshev_polynomial_v \u274c - \u274c special_chebyshev_polynomial_w \u274c - \u274c special_entr \u274c - \u274c special_erfcx \u274c - \u274c special_hermite_polynomial_h \u274c - \u274c special_hermite_polynomial_he \u274c - \u274c special_i0e \u274c - \u274c special_i1 \u274c - \u274c special_i1e \u274c - \u274c special_laguerre_polynomial_l \u274c - \u274c special_legendre_polynomial_p \u274c - \u274c special_log_ndtr \u274c - \u274c special_modified_bessel_i0 \u274c - \u274c special_modified_bessel_i1 \u274c - \u274c special_modified_bessel_k0 \u274c - \u274c special_modified_bessel_k1 \u274c - \u274c special_ndtr \u274c - \u274c special_ndtri \u274c - \u274c special_scaled_modified_bessel_k0 \u274c - \u274c special_scaled_modified_bessel_k1 \u274c - \u274c special_shifted_chebyshev_polynomial_t \u274c - \u274c special_shifted_chebyshev_polynomial_u \u274c - \u274c special_shifted_chebyshev_polynomial_v \u274c - \u274c special_shifted_chebyshev_polynomial_w \u274c - \u274c special_spherical_bessel_j0 \u274c - \u274c special_xlog1py \u274c - \u274c special_zeta \u274c - \u274c split \u274c - \u2705 split_copy \u274c - \u274c split_with_sizes_copy \u274c - \u274c splitlines \u274c - \u274c square \u2705 - \u2705 sspaddmm \u274c - \u274c stack \u274c - \u2705 startswith \u274c - \u274c std \u274c - \u2705 std_mean \u274c - \u2705 stft \u274c - \u2705 storage_offset \u274c - \u274c str \u274c - \u274c stride \u274c - \u274c strip \u274c - \u274c sum_to \u274c - \u274c svd \u274c - \u274c swapcase \u274c - \u274c symbolic_b \u274c - \u274c symeig \u274c - \u274c t \u2705 - \u2705 take \u274c - \u2705 take_along_dim \u274c - \u274c tanhshrink \u274c - \u274c tensor \u274c - \u2705 tensor_split \u274c - \u2705 tensordot \u274c - \u2705 test \u274c - \u274c test_symbol \u274c - \u274c test_vartype \u274c - \u274c test_vartype2 \u274c - \u274c thnn_conv2d \u274c - \u274c thnn_conv2d_forward \u274c - \u274c threshold \u274c - \u2705 tile \u274c - \u2705 title \u274c - \u274c to \u274c - \u2705 to_dense \u274c - \u274c to_here \u274c - \u274c to_mkldnn \u274c - \u274c to_padded_tensor \u274c - \u274c trace \u274c - \u274c transpose swapaxes, swapdims \u2705 - \u2705 transpose_copy \u274c - \u274c trapezoid \u274c - \u274c trapz \u274c - \u274c triangular_solve \u274c - \u274c tril \u274c - \u2705 tril_indices \u274c - \u274c triplet_margin_loss \u274c - \u274c triu \u274c - \u2705 triu_indices \u274c - \u274c type_as \u274c - \u2705 unbind \u274c - \u2705 unbind_copy \u274c - \u274c unflatten \u274c - \u2705 unflatten_dense_tensors \u274c - \u274c unfold \u274c - \u2705 unfold_copy \u274c - \u274c uniform \u2705 - \u274c unique_consecutive \u274c - \u274c unique_dim \u274c - \u2705 unique_dim_consecutive \u274c - \u274c unknown \u274c - \u274c unsafe_chunk \u274c - \u2705 unsafe_split \u274c - \u2705 unsafe_split_with_sizes \u274c - \u2705 unsqueeze_copy \u274c - \u274c update \u274c - \u274c upper \u274c - \u274c upsample \u274c - \u274c upsample_bicubic2d \u274c - \u2705 upsample_linear1d \u274c - \u2705 upsample_nearest1d \u274c - \u2705 upsample_nearest3d \u274c - \u2705 upsample_trilinear3d \u274c - \u2705 values \u274c - \u274c values_copy \u274c - \u274c vander \u274c - \u274c var_mean \u274c - \u2705 vdot \u274c - \u274c view_as \u274c - \u2705 view_as_complex \u274c - \u274c view_as_complex_copy \u274c - \u274c view_as_real \u274c - \u274c view_as_real_copy \u274c - \u274c view_copy \u274c - \u274c view_expand_placeholder \u274c - \u274c vsplit \u274c - \u274c vstack row_stack \u274c - \u2705 wait \u274c - \u274c wait_tensor \u274c - \u274c warn \u274c - \u274c warns \u274c - \u274c wrapped_linear_prepack \u274c - \u274c wrapped_quantized_linear_prepacked \u274c - \u274c xlogy special_xlogy \u274c - \u274c zero \u2705 - \u2705 zeros \u274c - \u2705 zeros_like \u274c - \u2705 zfill \u274c - \u274c"},{"location":"tutos/1_getting_started/","title":"1. Getting Started","text":"<p>Goals</p> <p>At the end of this tutorial you will know:</p> <ol> <li> How to export an image model with <code>torch_to_nnef</code></li> <li> The basic commands to check your model is correct within tract</li> <li> How to create a minimal Python program that perform inference with tract</li> <li> (Bonus) How to create a minimal rust binary that perform inference from the cli with tract</li> </ol> <p>Prerequisite</p> <ul> <li> PyTorch and Python basics</li> <li> Basic rust knowledge (for the Bonus)</li> <li> 15 min to read this page</li> </ul>"},{"location":"tutos/1_getting_started/#step-1-select-an-image-classifier-and-an-image","title":"Step 1.  Select an image classifier and an image","text":"<p>Let's start by simply selecting a model to export from the well known torchvision.</p> <ul> <li>Create a virtual env, install dependencies and assets:</li> </ul> Setup<pre><code>mkdir getting_started_py\ncd getting_started_py\npython3 -m venv .venv\nsource .venv/bin/activate\npip install -U pip\npip install torch==2.7.0 \\\n    torchvision==0.22.0 \\\n    torch_to_nnef\nwget https://upload.wikimedia.org/wikipedia/commons/5/55/Grace_Hopper.jpg\ntouch export.py\n</code></pre> <p>Let write inside export python file: <code>export.py</code> to get PyTorch model &amp; input data and perform inference with the given image.</p> export.py (part 1)<pre><code>import torch\nfrom torchvision import models as vision_mdl\nfrom torchvision.io import read_image\n\nmy_image_model = vision_mdl.vit_b_16(pretrained=True) # (1)!\n\nimg = read_image(\"./Grace_Hopper.jpg\")\nclassification_task = vision_mdl.ViT_B_16_Weights.IMAGENET1K_V1 # (2)!\ninput_data_sample = classification_task.transforms()(\n    img.unsqueeze(0)\n)\n\nwith torch.no_grad():\n    predicted_index = my_image_model(\n        input_data_sample\n    ).argmax(1).tolist()[0]\n    print(\n        \"class id:\",\n        predicted_index,\n        \"label: \",\n        classification_task.meta[\"categories\"][\n            predicted_index\n        ],\n    )\n</code></pre> <ol> <li>Selected model is documented here</li> <li>The classification task is documented here</li> </ol> <p>Running the file:</p> output<pre><code>class id: 652 label:  military uniform\n</code></pre> <p>The class index predicted with PyTorch (<code>652</code>) needs to be aligned with tract prediction we will develop.</p>"},{"location":"tutos/1_getting_started/#step-2-export-to-nnef","title":"Step 2. Export to NNEF","text":"<p>Let's continue the <code>export.py</code> by calling the main export function from this package:</p> export.py (part 2)<pre><code>from pathlib import Path\nfrom torch_to_nnef import export_model_to_nnef, TractNNEF\n\nfile_path_export = Path(\"vit_b_16.nnef.tgz\")\nexport_model_to_nnef( # (1)!\n    # any nn.Module\n    model=my_image_model,\n    # list of model arguments\n    # (here simply an example of tensor image)\n    args=input_data_sample,\n    # filepath to dump NNEF archive\n    file_path_export=file_path_export,\n    # inference engine to target\n    inference_target=TractNNEF( # (2)!\n        # tract version (to ensure compatible operators)\n        version=\"0.21.13\",\n        # default False\n        # (tract cli will be installed on the machine on fly)\n        # and correctness of output compared to PyTorch for the\n        # provided model and input will be performed\n        check_io=True,\n    ),\n    input_names=[\"input\"],\n    output_names=[\"output\"],\n    # create a debug bundle in case model export work\n    # but NNEF fail in tract\n    # (either due to load error or precision mismatch)\n    debug_bundle_path=Path(\"./debug.tgz\"),\n)\nprint(f\"exported {file_path_export.absolute()}\")\n</code></pre> <ol> <li>Full function documentation available here</li> <li>Full Class documentation available here</li> </ol> <p>And that's it if we now run our little snippet (full code here)</p> <pre><code>source.venv/bin/activate\npython export.py\n</code></pre> <p>We should now observe the following output:</p> <pre><code>.../site-packages/torch/__init__.py:2132: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n  assert condition, message\naten::size replaced by constant traced value (follows NNEF spec).Keeping dynamism would require dynamic_axes specified.\nexported ./vit_b_16.nnef.tgz\n</code></pre> <p>But wait there are 2 tracing warnings here:</p> <ul> <li> <p>The first is inherent to tracing mechanism happening inside <code>torch_to_nnef</code> indeed behind the scene     we use PyTorch jit.trace.     It is only able to capture torch control flows, so all Python manipulations     that do not happen in PyTorch internals are 'solidified' into a set of fixed values.     This also happens if you export a PyTorch model to ONNX with their internal tool.</p> </li> <li> <p>The second is interesting, it highlights a loss of model expressiveness because we did not specify     that one of the input dimension is in fact the batch size, a parameter that may vary. We will show how     to solve that in the next tutorial. (spoiler: we use same API as ONNX export to inform <code>dynamic_axes</code>)</p> </li> </ul> <p>Finally last line indicates that the model has been correctly exported on disk at: <code>.../vit_b_16.nnef.tgz</code>.</p>"},{"location":"tutos/1_getting_started/#step-3-tract-cli-checks","title":"Step 3.  tract cli checks","text":"<p>We will now check with the tract cli that everything is working as expected.</p> <p>Let's first display the help of the command line we downloaded when we checked io between tract and PyTorch (in step 2.)</p> Setup<pre><code>alias tract=$HOME/.cache/svc/tract/0.21.13/tract\ntract --help\n</code></pre> <p>If you did skip this steps you can always download manually the cli from the tract release page, or run <code>cargo install tract</code> (which will compile it for your system).</p> <p>This command line is pretty dense so we will only use part of it today.</p> <p>Let's first load and dump a profile of our model:</p> Dump model properties with tract<pre><code>tract ./vit_b_16.nnef.tgz \\\n    --nnef-tract-core \\\n    -O \\\n    dump \\\n    --allow-random-input \\\n    --profile\n</code></pre> <p>Here a lot is happening:</p> <ul> <li>tract loads the NNEF registry relative to core operators</li> <li>it then loads the model</li> <li>it declutters and optimizes it  (thanks to the <code>-O</code>)</li> <li>the <code>--allow-random-input</code> avoids us to provide a concrete input example</li> <li>the <code>--profile</code> informs the command-line that we want to observe the speed of it</li> </ul> <p>Output in stdout is composed of following sections in order:</p> <p>The graph of computation (after decluttering and optimization) with each operation speed:</p> Graph display<pre><code>  0.000 ms/i  0.0%  \u250f 0 Source input\n                    \u2503   \u2501\u2501\u2501 1,3,224,224,F32\n....\n  0.000 ms/i  0.0%  \u2523 686 OptMatMulPack output_linear_output.pack_b\n                    \u2503   \u2501\u2501\u2501 1,Opaque \ud83d\udd0d DynPackedOpaqueFact { k: Val(768), mn: Val(1), packers: [PackedF32[1]@128+1] }\n  0.046 ms/i  0.0%  \u2523\u253b 688 OptMatMul output_linear_output\n                        \u2501\u2501\u2501 1,1000,F32\n</code></pre> <p>This already tell us about how the network is composed and which specialized operators kernels were selected. (this display is from an ARM CPU) Then we have the list of custom properties that have been exported by <code>torch_to_nnef</code>:</p> Exported properties<pre><code>* export_cmd: ,String docs/examples/getting_started.py\n* export_date: ,String 2025-07-08 ...\n* exported_py_class: ,String VisionTransformer\n* hostname: ,String ...\n* os: ,String ...arm64 Darwin\n* py_version: ,String 3.12.9 ...\n* torch_to_nnef_version: ,String 0.18.6\n* torch_version: ,String 2.6.0\n* tract_stage: ,String optimized\n* tract_target_version: ,String 0.21.13\n* transformers_version: ,String 4.49.0\n* user: ,String tuto\n</code></pre> <p>These are metadata that are automatically set when exporting models. This often come handy during debugging sessions. You can set custom ones with the <code>specific_properties</code> parameter in <code>TractNNEF</code> init.</p> <p>Finally the aggregated per operator kind performance is shown:</p> Performance per operator kind<pre><code> * OptMatMul               74 nodes:  90.859 ms/i 65.6%\n * OptMatMulPack           97 nodes:  12.779 ms/i  9.2%\n * Softmax                 12 nodes:  10.345 ms/i  7.5%\n...\n</code></pre> <p>With percentage of time spent (again aggregated per operator kind) and you get the total time spent to run the network:</p> Total performance<pre><code>Entire network performance: 138.525 ms/i\n</code></pre> <p>On classical networks, matrix multiplication operations should dominate the compute time.</p> <p>Info</p> <p>This command only displays time to run the inference (model load and optimization are not accounted for).</p> <p>GPU usage</p> <p>If you have a recent Apple Silicon device try the same command adding <code>--metal</code> before the <code>dump</code> and observe the speed difference.</p>"},{"location":"tutos/1_getting_started/#step-5-tract-inference-with-python","title":"Step 5.  tract inference with Python","text":"<p>We just created a great NNEF model, and it has been checked during export to get same output for same input between PyTorch and tract (thanks to the <code>check_io=True</code> option). That said you may now wish to interact with it to perform a fully fledged evaluation of the model (to ensure this new inference engine does not get imprecise results on some specific samples).</p> <p>For this purpose we need to install a new package in our activated <code>venv</code>:</p> add tract python package<pre><code>pip install \"tract&lt;0.22,&gt;=0.21\"\n</code></pre> <p>Let's now create a new python file called <code>run.py</code>:</p> <p>Let's read our example image again with torch vision and transform it in <code>numpy</code> feature matrix, this part is specific to the image classification, and could be done with any tool you wish (this is not <code>tract</code> or <code>torch_to_nnef</code> related).</p> run.py (part 1)<pre><code>import tract\nimport numpy as np\nfrom torchvision import models as vision_mdl\nfrom torchvision.io import read_image\n\nimg = read_image(\"./Grace_Hopper.jpg\")\nclassification_task = vision_mdl.ViT_B_16_Weights.IMAGENET1K_V1\ninput_data_sample = classification_task.transforms()(\n    img.unsqueeze(0)\n).numpy()\n</code></pre> <p>Now we can load the <code>NNEF</code> model with tract, declutter and optimize it:</p> run.py (part 2)<pre><code>model = (\n    tract.nnef() #(1)!\n    .with_tract_core()\n    .model_for_path(\"./vit_b_16.nnef.tgz\")\n    .into_optimized()\n    .into_runnable()\n)\n</code></pre> <ol> <li>documentation available here</li> </ol> <p>Finally we can run the inference for the provided input and extract predicted result:</p> run.py (part 3)<pre><code>result = model.run([input_data_sample])\nconfidences = result[0].to_numpy()\nprediced_index = np.argmax(confidences)\nprint(\n    \"class id:\",\n    predicted_index,\n    \"label: \",\n    classification_task.meta[\"categories\"][\n        predicted_index\n    ],\n)\n</code></pre> <p>And that's it, we can now run our little snippet (full code here).</p> <p>Congratulation</p> <p>Your first export with <code>torch_to_nnef</code> is now done and you ran a successful standalone tract based inference with it. This is sufficent if you intend to use python only.</p>"},{"location":"tutos/1_getting_started/#step-6-making-a-minimal-rust-program","title":"Step 6.  Making a minimal rust program","text":"<p>Ok, we have our model asset we confirmed it run well from tract cli and Python, now let's integrate it in a rust program.</p> <p>We will build it step by step, but note that the code is very similar to this tract example.</p> minimal setup rust binary and download a dummy image<pre><code>cd ..\ncargo init getting_started_rs\ncd getting_started_rs\ncargo add tract-core tract-nnef image\ncp ../getting_started_py/vit_b_16.nnef.tgz ./\ncp ../getting_started_py/Grace_Hopper.jpg ./\n</code></pre> <p>Now compile the project:</p> check project compile<pre><code>cargo run --release\n</code></pre> <p>We should observe <code>Hello, world!</code> in stdout. Let's write the core interesting parts in <code>src/main.rs</code>:</p> <p>Add the prelude from tract_nnef</p> main.rs (part 1)<pre><code>use tract_nnef::prelude::*;\n</code></pre> <p>Replace the type signature of the main (for simplicity of this example):</p> main.rs (part 2)<pre><code>fn main() -&gt; TractResult&lt;()&gt; {\n    println!(\"Hello, world!\");\n    Ok(())\n}\n</code></pre> <p>Inside the main replace println with:</p> main.rs (part 3)<pre><code>    let model = tract_nnef::nnef()\n        .with_tract_core()\n        .model_for_path(\"./vit_b_16.nnef.tgz\")?\n        // optimize the model\n        .into_optimized()?\n        // make the model runnable and fix its inputs and outputs\n        .into_runnable()?;\n</code></pre> <p>This code is responsible to load, declutter and optimize the model. Prepare an image to be ingested by the neural network:</p> main.rs (part 4)<pre><code>    // open image, resize it and make a Tensor out of it\n    let image = image::open(\"Grace_Hopper.jpg\")?.to_rgb8();\n    // scale to model input dimension\n    let resized = image::imageops::resize(\n        &amp;image,\n        224,\n        224,\n        ::image::imageops::FilterType::Triangle\n    );\n    // normalization step\n    let image = tract_ndarray::Array4::from_shape_fn(\n        (1, 3, 224, 224),\n        |(_, c, x, y)| {\n            let mean = [0.485, 0.456, 0.406][c];\n            let std = [0.229, 0.224, 0.225][c];\n            (resized[(x as _, y as _)][c] as f32 / 255.0 - mean) / std\n        }\n    )\n    .into_tensor();\n</code></pre> <p>Notice that tract uses ndarray to manipulate tensors with its user facing API.</p> <p>This tensor is now ready to be run with our tract model:</p> main.rs (part 5)<pre><code>    // run the model on the input\n    let result = model.run(tvec!(image.into()))?;\n</code></pre> <p>Let's now get the index of classified class for the image and print it:</p> main.rs (part 6)<pre><code>    // find and display the max value with its index\n    let best = result[0]\n        .to_array_view::&lt;f32&gt;()?\n        .iter()\n        .cloned()\n        .zip(0..)\n        .max_by(|a, b| a.0.partial_cmp(&amp;b.0).unwrap());\n\n    println!(\"result: {best:?}\");\n</code></pre> <p>That's it our code is complete, (your code should now look like this)</p> <p>You can now rebuild and run the code with cargo:</p> compile &amp; run completed project<pre><code>cargo run --release\n</code></pre> <p>This should display to you</p> <pre><code>result: Some((9.439479, 652))\n</code></pre> <p>Congratulation</p> <p> you made it ! You first exported the network with <code>torch_to_nnef</code> and ran a successful standalone rust cli command with tract based inference in it.</p>"},{"location":"tutos/1_getting_started/#demo-1-image-classifier","title":"Demo 1:  Image classifier","text":"<p>Using the knowledge you acquired during this tutorial and a bit of extra for WASM in rust.  We demo the use of a small <code>Efficient NET B0</code> image neural network running in your browser (smaller than ViT to ensure a fast download of the asset for you - 22Mo for the model - ).</p> <p> </p> <p>Note</p> <p>This model is not trained by SONOS so prediction accuracy is responsibility of original torchvision authors. Inference performance is descent, but little to no effort was made to make tract WASM efficient (no SIMD WASM, no WebGPU kernels), this demo is for demonstration purpose.</p> <p>Curious to read the code behind it ? Just look at our example directory here and this raw page content.</p>"},{"location":"tutos/1_getting_started/#demo-2-yolo-human-pose-estimator","title":"Demo 2:  Yolo Human Pose Estimator","text":"<p>In the same logic here is a slightly more modern model.</p> <p> </p> <p>Again the example directory here and this raw page content.</p>"},{"location":"tutos/2_nnef_archive/","title":"2. NNEF archive composition","text":"<p>Goals</p> <p>At the end of this tutorial you will know:</p> <ol> <li> The basics of .nnef format</li> <li> The reference Khronos specification</li> </ol> <p>Prerequisite</p> <ul> <li> Understanding what a neural network is</li> <li> 5 min to read this page</li> </ul> <p>The NNEF generated by our getting-started (and all <code>torch_to_nnef</code> export) respects most of the format defined by the   specification.</p> <p>In fact this package uses their tools to perform the last serialization part of the export. That's why, most informations you may need to understand this format is available in their specification. This page provides a quick glimpse with emphasis on key elements/differences, to allow you to easily navigate it.</p>"},{"location":"tutos/2_nnef_archive/#inside-the-archive","title":"Inside the archive","text":"<p>As shown in the 1st tutorial, export step creates a <code>${MY_MODEL_NAME}.nnef.tgz</code> archive. This is really just a classical <code>tar</code> archive with <code>gzip</code> compression option. You can in fact control the compression ratio with the <code>compression_level: int</code> parameter of the export function (0 meaning no compression). Compression allows to trade space on disk and network transfers against time to load the model during the first time (decompression).</p> <p>Info</p> <p>tract itself is agnostic to specific archive or 'compression' format and as long as you provide a <code>graph.nnef</code> text file and related <code>.dat</code> files it should be possible to run it through tract. In fact tract is even able to take the model from a bit stream given proper interface call.</p> <p>Since it's just an archive you can just extract it:</p> <pre><code>mkdir vit_b_16_nnef_dir\ncd vit_b_16_nnef_dir\ntar -xvzf ../vit_b_16.nnef.tgz\nls -l\n</code></pre> <p>You should see a list of <code>.dat</code> binary files each corresponds to a specific 'parameter' tensor from the ViT neural network exported, and a <code>graph.nnef</code> that contains the textual representation of the graph.</p>"},{"location":"tutos/2_nnef_archive/#graphnnef","title":"graph.nnef","text":"<p>Let's start by looking at the <code>graph.nnef</code>.</p> graph.nnef<pre><code>version 1.0;\n\nextension tract_registry tract_core;\n\nfragment tract_core_properties(\n) -&gt; (properties: (string, tensor&lt;scalar&gt;)[])\n{\n  properties = [\n    (\"tract_target_version\", \"0.21.13\"),\n    (\"torch_to_nnef_version\", \"0.18.6\"),\n//...\n    (\"export_cmd\", \"getting_started.py\")\n  ];\n}\n\nfragment tract_gelu( x: tensor&lt;scalar&gt; ) -&gt; ( y: tensor&lt;scalar&gt; )\n{\n    y = 0.5 * x * ( 1.0 + tract_core_erf(x * 0.7071067811865475));\n}\n\n# ...\n\ngraph network(input) -&gt; (output)\n{\n    input = tract_core_external(shape = [1, 3, 224, 224], datum_type = 'f32');\n    class_token = variable&lt;scalar&gt;(label = 'class_token', shape = [1, 1, 768]);\n# ...\n    output = linear(\n      select0,\n      heads_head_weight,\n      heads_head_bias_aligned_rank_expanded\n    );\n\n}\n</code></pre> <p>First we observe the <code>extension</code>s, in our case some of the operators used later are specific to tract, so the tract registry called <code>tract_core</code> is needed. In tract there are various registry for different purposes: <code>tract_core</code> (all classical operators in tract), <code>tract_transformers</code> that holds some operators specific to transformers, <code>tract_onnx</code> (operators very specific happening in ONNX), <code>tract_extra</code> that is specific to peculiar operators such as <code>exponential unit norm</code>, <code>tract_resource</code> to load custom variables inside your graph... Those are added automatically by <code>torch_to_nnef</code> except if you use custom operators.</p> <p>After that we see a set of <code>fragment</code>s you can think of those as pure functions, for most of them (there are few exceptions like if there is <code>scan</code> operator but that is a good approximate). These fragments allow to compose graph to 'compile' into smaller reusable blocks. <code>tract_gelu</code> is interesting because it is replaced on the fly by <code>gelu</code> specific operator if it exists in selected registries and for your hardware.</p> <p>Finally there is the <code>network</code> which is the main entry point that will describe the inference computations to perform from inputs to outputs by calling operators and fragments, and assigning the result in temporary <code>variables</code>.</p> <ul> <li><code>tract_core_external</code> is like <code>external</code> from NNEF original spec but with data type specification,     allowing fine grained definition of what is expected.</li> <li><code>variable&lt;scalar&gt;</code> is the standard declaration to load a parameter tensor as described in NNEF spec. label value attribute directly matches with the <code>${label_value}.dat</code> file that will be loaded.</li> </ul> <p>Info</p> <p>As of today graph is mostly 'flat' with exception of few fragments. <code>torch.nn.Module</code> structure is not maintained in the final NNEF, so there is repetitions in the control flow expressed here.</p> <p>Nothing, prevent us to further factorize the graph in the future, beyond simplicity of this package codebase, as this graph representation is NOT the final optimized graph that tract will run but rather the blueprint.</p>"},{"location":"tutos/2_nnef_archive/#dat-files","title":".dat files","text":"<p>We follow the <code>.dat</code> format specification defined in the Khronos spec, including support for q8 quantization. We also leverage the flexibility left to define new formats, for example: <code>Q4_0</code> .dat files in a format that is close to isolated GGML_TYPE_Q4_0, and can be exported to tract with our package seamlessly as explained in the quantization tutorial.</p>"},{"location":"tutos/3_multi_inputs_outputs/","title":"3. Model with multiple inputs or outputs","text":"<p>Goals</p> <p>At the end of this tutorial you will know:</p> <ol> <li> How to export multi io neural network</li> <li> Data type limitations of the NNEF representation</li> </ol> <p>Prerequisite</p> <ul> <li> PyTorch and Python basics</li> <li> 10 min to read this page</li> </ul> <p>A lot of neural network models require more than 1 input or output. In that case we support no less than classical ONNX export.</p>"},{"location":"tutos/3_multi_inputs_outputs/#how-to-export","title":"How to export ?","text":"<p>To exemplify this,  let's simply try to export a classical Language model called Albert (from 'ALBERT: A lite BERT for self-supervised learning of language representations',  2020) with the <code>transformers</code> library.</p> <p>First let's create a dir and install few dependencies:</p> setup<pre><code>mkdir multi_io_py\ncd multi_io_py\npython3 -m venv .venv\nsource .venv/bin/activate\npip install -U pip\npip install torch==2.7.0 \\\n    transformers==4.53.2 \\\n    sentencepiece==0.2.0 \\\n    torch_to_nnef\ntouch export_albert.py\n</code></pre> <p>We are now ready to start, load the model and prepare an input sample:</p> load model and input sample in ('export_albert.py' part 1)<pre><code>tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\ninputs = tokenizer(\"Hello, I am happy\", return_tensors=\"pt\")\nalbert_model = AlbertModel.from_pretrained(\"albert-base-v2\")\n</code></pre>"},{"location":"tutos/3_multi_inputs_outputs/#using-basic-export-api","title":"Using basic export API","text":"<p>What would happen if we used the same call as previously in the getting started tutorial ?</p> <p>Let's not forget that <code>inputs</code> generated from the tokenizer are put in a Python object  <code>BatchEncoding</code>. It contains the tensors that we will use in the <code>forward</code> pass of this network in the following attributes: <code>input_ids</code>, <code>attention_mask</code>, <code>token_type_ids</code>. So we need to add those attributes in  <code>args</code> and refer to them in <code>input_names</code> of export API.</p> <p>Let's try together:</p> simple approach ('export_albert.py' part 2)<pre><code>from pathlib import Path\nfrom torch_to_nnef import export_model_to_nnef, TractNNEF\n\nfile_path_export = Path(\"albert_v2.nnef.tgz\")\ninput_names = [\n    'input_ids', 'attention_mask', 'token_type_ids'\n]\nexport_model_to_nnef(\n    model=albert_model,\n    # here we can not simply write\n    # args=inputs.values()\n    # because order of values\n    # is different than .forward parameters !\n    args=[inputs[k] for k in input_names],\n    file_path_export=file_path_export,\n    inference_target=TractNNEF(\n        version=\"0.21.13\",\n        check_io=True,\n    ),\n    input_names=input_names,\n    output_names=[\"output\"],\n    debug_bundle_path=Path(\"./debug.tgz\"),\n)\n</code></pre> <p>Warning</p> <p>This export is for demonstration of multi inputs outputs only ! The dynamic dimensions specification is missing which creates a limited sub-optimal exported NNEF model.</p> <p>If you run this script you should get a model very close to the definition in transformers library with the graph.nnef signature that look like this:</p> nnef graph signature<pre><code>graph network(\n    input_ids,\n    token_type_ids,\n    attention_mask\n) -&gt; (output_last_hidden_state, output_pooler_output)\n</code></pre>"},{"location":"tutos/3_multi_inputs_outputs/#wait-but-what-did-just-happen-to-the-outputs","title":"Wait but what did just happen to the outputs ?","text":"<ul> <li>This transformer model return in Python a special object:  <code>BaseModelOutputWithPooling</code></li> <li><code>torch_to_nnef</code> export is called with the <code>output_names</code> specified with 1 element named <code>output</code></li> </ul>"},{"location":"tutos/3_multi_inputs_outputs/#so-how-come-do-we-get-2-outputs","title":"So how come do we get 2 outputs ?","text":"<p>It turns out <code>torch_to_nnef</code> tries hard to make sense of the provided inputs and outputs.</p> <p>In this case, the output object has been partially filled because of the inputs provided to the model. In the upper snippet, we did not add parameters for the <code>AlbertModel.forward</code> method: <code>output_attentions</code> or <code>output_hidden_states</code>: to <code>True</code>. So all the graph traced and exported use the control-flow not collecting those outputs.</p> <p>Warning</p> <p>This is one of the key limitation of the NNEF export, since it is based on internal Graph representation in PyTorch it doesn't really know more than PyTorch. All the control-flow existing Python side are unknown. This is why selecting correctly your input so that the correct trace and outputs end up being exported is very important.</p> <p>That's also why conditional sub-model execution is not embededable directly to NNEF (think Mixture Of Experts for example). But fear not we have solutions for that.</p> <p>Ok now that's a bit clearer \ud83d\ude2e\u200d\ud83d\udca8, but why output names differ from those in Python modeling ?</p> <p>Well we requested the first output object to be named <code>output</code>. So all its internal torch tensors are mapped and are automatically prefixed like this: <code>output_{internal_object_key}</code> in NNEF.</p>"},{"location":"tutos/3_multi_inputs_outputs/#io-specification","title":"IO Specification","text":"<p>A bit lost about what is and is not possible to export as Input/Outputs ?</p> <p>Input(s) provided in export parameter: <code>args</code> can be:</p> <ul> <li>a simple <code>torch.Tensor</code></li> <li>a list of supported elements</li> </ul> <p>Those supported elements being:</p> <ul> <li>a <code>torch.Tensor</code></li> <li>a dict of <code>torch.Tensor</code></li> <li>a list or a tuple of <code>torch.Tensor</code></li> <li>An object that mimic a dict by implementing <code>__getitem__</code> magic function</li> <li>Containers (dict, list, tuples, mimic object) can themselves embed sub-containers that     contains <code>torch.Tensor</code></li> </ul> <p>Important:</p> <ul> <li>Python Primitives (boolean, integer, float, string values) are passed but the trace and export will 'constantize' them so they will not be variable anymore in NNEF.</li> <li>List, Tuple and dict are as well fixed in length and keys possible at export time.</li> </ul> <p>Variable Python primitive in NNEF</p> <p>To work-around Python primitives constantization you can transform those into <code>torch.Tensor</code>. This will only work on primitive that does not change the control-flow.</p> <p>Outputs have the same object flexibility.</p> <p>Also, if some names are not provided in <code>input_names</code> and <code>output_names</code> they will be automatically generated with following template <code>input_{}</code> and <code>output_{}</code> where the content of the brackets depends on indexes and keys.</p>"},{"location":"tutos/3_multi_inputs_outputs/#selection-of-inputs-and-outputs-to-export","title":"Selection of inputs and outputs to export","text":"<p>Ok that's nice, we should now start to better understand what's possible to do with simple <code>torch_to_nnef</code> export call.</p> <p>What about if you want something that only exports the <code>last_hidden_states</code> ?</p> <p>In that case you can simply wrap the model into a new <code>nn.Module</code> like so:</p> basic model wrapping<pre><code>import torch\n\nclass ALBERTModelWrapper(torch.nn.Module):\n    def __init__(self, transformer_model: torch.nn.Module):\n        super().__init__()\n        self.transformer_model = transformer_model\n\n    def forward(self, *args):\n        outputs = self.transformer_model(*args)\n        last_hidden_states = outputs.last_hidden_state\n        return last_hidden_states\n\nwrapped_model = ALBERTModelWrapper(albert_model)\n</code></pre> <p>You can now export this wrapped model with the same API call we specified upper. The same logic would apply if you wish to ignore some <code>inputs</code>, you can set those in the <code>__init__</code> and reference them in the <code>forward</code> pass.</p> <p>A concrete example of this is available in the <code>torch_to_nnef</code> codebase with regard to LLM wrappers that need to handle the KV-cache properly: here.</p>"},{"location":"tutos/3_multi_inputs_outputs/#working-around-limitations","title":"Working around limitations","text":""},{"location":"tutos/3_multi_inputs_outputs/#dynamic-number-of-input-or-output-is-not-supported-out-of-the-box","title":"Dynamic number of input or output is not supported out of the box","text":"<p>Given your network has a variable number of input or outputs with the same shape you can envision to wrap your network inside a <code>torch.nn.Module</code> that will concatenate those into a single tensor of variable size.</p> <p>If the shapes are of varying size preventing the direct concatenation, you can pad your tensors before and add another tensor responsible to keep track of your padding values.</p> <p>This again will only work if the underlying PyTorch IR graph is not altered by the different inputs.</p> <p>While suboptimal in RAM and compute, it should allow to express any possible network in that respect.</p>"},{"location":"tutos/4_dynamic_axes/","title":"4. Dynamic axes","text":"<p>Goals</p> <p>At the end of this tutorial you will know:</p> <ol> <li> How to specify inputs tensor of variable lengths in neural network at export time</li> <li> What is tract pulsification and why is this very powerful ?</li> </ol> <p>Prerequisite</p> <ul> <li> PyTorch and Python basics</li> <li> 15 min to read this page</li> </ul> <p>Numerous neural networks act on dimensions that aren't known quantity at export time. Batch size is a common example that is ideally selected at runtime according to the user need. Time dimension is another case were dimension may accumulate over a runtime session, and change between sessions. Also some neural network applied on image support varying resolutions.</p> <p>In this tutorial we will see how to specify this dynamism inside <code>NNEF</code> at export, and the special case of time dimension for stateful neural networks.</p>"},{"location":"tutos/4_dynamic_axes/#simple-case-batch-dimension-only","title":"Simple case: batch dimension only","text":"<p>If we think of our getting_started example earlier, after export: the model generated is having a fixed batch dimension of 1 sample. Let's fix this by declaring this dimension as dynamic at export time:</p> setting streaming dimensions correctly<pre><code>from pathlib import Path\nfrom torch_to_nnef import export_model_to_nnef, TractNNEF\n\nfile_path_export = Path(\"vit_b_16_batchable.nnef.tgz\")\nexport_model_to_nnef(\n    model=my_image_model,\n    args=input_data_sample,\n    file_path_export=file_path_export,\n    inference_target=TractNNEF(\n        version=\"0.21.13\",\n        check_io=True,\n        # here we use the first input_names we define\n        # and request the first dimension: 0 to have\n        # the varying dimensions \"B\" (for batch)\n        dynamic_axes={\"input\": {0: \"B\"}},\n    ),\n    input_names=[\"input\"],\n    output_names=[\"output\"],\n    debug_bundle_path=Path(\"./debug.tgz\"),\n)\n</code></pre> <p>After running this script you should have a new asset: <code>vit_b_16_batchable.nnef.tgz</code>. Looking at the <code>graph.nnef</code> there are 2 new obvious things:</p> <ul> <li>A new extension at the beginning of the file with the introduced symbol</li> </ul> <pre><code>extension tract_symbol B;\n</code></pre> <ul> <li>The input external introduces this symbol in the requested dimensions:</li> </ul> <pre><code>input = tract_core_external(shape = [B, 3, 224, 224], datum_type = 'f32');\n</code></pre> <p>More subtly a lot of operations now don't assume shape is static but instead built from this variable shape:</p> <pre><code>input_shape = tract_core_shape_of(input);\ninput_shape_sliced0 = slice(input_shape, axes = [0], begin = [0], end = [1], stride = [1]);\ninput_dim0 = squeeze(input_shape_sliced0, axes = [0]);\n# ...\nx_reshape0 = reshape(conv_proj__x__convolution0, shape = [input_dim0, 768, mul0]);\n</code></pre> <p>If you run tract with the exported model:</p> <pre><code>tract ./vit_b_16_batchable.nnef.tgz --nnef-tract-core -O dump\n</code></pre> <p>You should observe that the batch dimension flows from the input to the last output of the graph:</p> <pre><code>    \u2501\u2501\u2501 B,1000,F32\n</code></pre> <p>That's great but how can you do profiling ?</p> <pre><code>tract ./vit_b_16_batchable.nnef.tgz --nnef-tract-core -O dump --allow-random-input --profile\n</code></pre> <p>leads to the following stderr:</p> <pre><code>[... ERROR tract] Expected concrete shape, found: B,3,224,224,F32\n</code></pre> <p>That's expected as you now need to concretize this symbol before profiling anything. You can do that before or after the 'dump' keyword but be careful this has a different meaning:</p> <ul> <li>if before this means the compiled graph by tract in memory will be of concretized dimensions you provided</li> <li>if after this means the compiled graph by tract in memory will offer dynamic dimensions to be defined at runtime per session: The way to specify it is with the <code>--set B=3</code> where 3 can be whatever whole number upper or equal to 1.</li> </ul> <p>So running:</p> <pre><code>tract ./vit_b_16_batchable.nnef.tgz --nnef-tract-core -O \\\n    dump --set B=3 --allow-random-input --profile\n</code></pre> <p>You should be able to observe as previously a nice evaluation of network speed and its breakdown.</p> <p>Congratulation</p> <p>You made your first dynamic network export with <code>torch_to_nnef</code>  !</p>"},{"location":"tutos/4_dynamic_axes/#streaming-audio-with-stateful-model","title":"Streaming Audio with stateful model","text":"<p>Imagine that you want to add one of these symbol to the time dimension.</p> <p>We will for this purpose build a very simple audio network that will have to predict that events occured every 4 frames in an infinite stream of frames.</p> <p>We already did 2 examples with transformers like architecture (ViT &amp; BERT), while a Conformer would work fine, let's instead go old school with a RNN stack from the DeepSpeech paper from 2014 with some convolution on top.</p> a custom audio model with streaming<pre><code>from pathlib import Path\nimport torch\nimport torchaudio\n\nclass CustomDeepSpeech(torch.nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.pre = torch.nn.Sequential(\n            torch.nn.BatchNorm1d(64),\n            torch.nn.Conv1d(64, 128, kernel_size=3, stride=2),\n            torch.nn.ReLU(),\n            torch.nn.Conv1d(128, 128, kernel_size=5),\n            torch.nn.ReLU(),\n        )\n        self.maxpool = torch.nn.MaxPool1d(2)\n        self.deepspeech = torchaudio.models.DeepSpeech(128, n_hidden=256)\n\n    def forward(self, x):\n        x = x.permute([0, 2, 1])\n        x = self.pre(x)\n        x = self.maxpool(x)\n        x = x.permute([0, 2, 1])\n        x = x.unsqueeze(1)\n        return self.deepspeech(x)\n</code></pre> <p>We can instantiate a non trained model with it and export it with the following command:</p> export audio model with streaming dimension<pre><code>file_path_export = Path(\"custom_deepspeech.nnef.tgz\")\ncustom_deepspeech = CustomDeepSpeech()\ninput = torch.rand(7, 100, 64)\nexport_model_to_nnef(\n    model=custom_deepspeech,\n    args=input,\n    file_path_export=file_path_export,\n    inference_target=TractNNEF(\n        dynamic_axes={\n            \"melbank\": {0: \"B\", 1: \"S\"},\n        },\n        version=\"0.21.13\",\n        check_io=True,\n    ),\n    input_names=[\"melbank\"],\n    output_names=[\"output\"],\n    debug_bundle_path=Path(\"./debug.tgz\"),\n    custom_extensions=[\n        \"tract_assert S &gt;= 1\",\n    ],\n)\n</code></pre> <p>After running the script we look at it from a tract perspective by dumping it with the classical command:</p> <pre><code>tract ./custom_deepspeech.nnef.tgz --nnef-tract-core -O dump\n</code></pre> <p>We observe a peculiar output dimension:</p> <pre><code>    \u2501\u2501\u2501 B,-3+(S+3)/4,40,F32\n</code></pre> <p>While batch dimension is fine, the temporal one is different.</p>"},{"location":"tutos/4_dynamic_axes/#what-did-just-happen","title":"What did just happen ?","text":"<p>In reality this is quite normal, since some operations in the neural network happen on the time dimension the streaming dimensions outputed is an expression based on S:</p> <p>Since the first convolution has a kernel of 3 we need at least 3 frames to fill our receptive field, hence the -3. Since there is a stride of 2 and a max-pooling of 2: we divide original S by 4 .</p> <p>tract is able to manage this state of receptive field and the caching of RNN state for you transparently. To achieve that we need to pulse the network: Pulsing is a concept specific to tract. It's choosing the 'time' step at which you wish your network to operate.  By example for this neural network you can select any pulse value that would be a multiple of 4. Due to its internal structure we discussed upper.  As an example we select 8:</p> <pre><code>tract custom_deepspeech.nnef.tgz \\\n    --nnef-tract-core \\\n    --pulse S=8 \\\n    dump \\\n    --nnef custom_deepspeech_pulse8.nnef.tgz\n</code></pre> <p>By calling this command you create a new NNEF asset that just replaced your streaming dimensions S by 8. If you look at this newly generated graph.nnef, you will also observe several novelties:</p> <ul> <li>a new extension is added:</li> </ul> <pre><code>extension tract_registry tract_pulse;\n</code></pre> <ul> <li>The introduction of new tract properties:</li> </ul> <pre><code>  (\"pulse.delay\", tract_core_cast([3], to = \"i64\")),\n  (\"pulse.input_axes\", tract_core_cast([1], to = \"i64\")),\n  (\"pulse.output_axes\", tract_core_cast([1], to = \"i64\")),\n</code></pre> <ul> <li>And some novel operators are set:</li> </ul> <pre><code>tract_pulse_delay(\n    pre___0__batch_norm0_batch_normalization_output_14,\n    axis = 2,\n    delay = 1,\n    overlap = 1\n);\n</code></pre> <p>They explicitly state the delay expected after the operation at this point of the graph. Now each time you will call this loaded model within the same state: it will expect to receive the next 8 frames of melbanks. And as explained earlier the state caching is managed internally by tract  .</p> <p>Info</p> <p>There is only 1 possible pulse dimensions within a tract model</p>"},{"location":"tutos/4_dynamic_axes/#nlp-stateless-model-with-dynamic-batch-and-token-dimension","title":"NLP: stateless model with dynamic batch and token dimension","text":"<p>In a prior example in tutorial on multiple input outputs we recommended to avoid using the provided code as such. Let's remedy to the snippet to make a better Albert in NNEF:</p> fixed Albert export with dynamic dimensions<pre><code>from pathlib import Path\nfrom torch_to_nnef import export_model_to_nnef, TractNNEF\n\nfile_path_export = Path(\"albert_v2_dyn.nnef.tgz\")\ninput_names = ['input_ids', 'attention_mask', 'token_type_ids']\nexport_model_to_nnef(\n    model=albert_model,\n    args=[inputs[k] for k in input_names],\n    file_path_export=file_path_export,\n    inference_target=TractNNEF(\n        # here we have to specify the symbols for all inputs\n        # and all dimensions\n        # same symbol is applied several time because:\n        # it's the same dimension.\n        dynamic_axes={\n            \"input_ids\": {0: \"B\", 1: \"S\"},\n            \"attention_mask\": {0: \"B\", 1: \"S\"},\n            \"token_type_ids\": {0: \"B\", 1: \"S\"}\n        },\n        version=\"0.21.13\",\n        check_io=True,\n    ),\n    input_names=input_names,\n    output_names=[\"output\"],\n    debug_bundle_path=Path(\"./debug.tgz\"),\n    # here we are adding some constraints to our introduced\n    # S symbol to help tract reason about what it can do\n    # with this symbol\n    custom_extensions=[\n        \"tract_assert S &gt;= 1\",\n        # we constrain arbitrary our model to be at max 32k tokens\n        # of context length\n        \"tract_assert S &lt;= 32000\",\n    ]\n)\n</code></pre> <p>Great, now at least we have a bit of dynamism, our newly exported model:</p> <ul> <li>can handle multiple queries at once (with single batch)</li> <li>can ingest varying number of tokens</li> </ul> <p>But is it enough to make it complete ?</p> <p>Likely not, because we would like by example to cache previously computed tokens to speed-up inference.</p> <p>To do that we need to introduce a new set of input for KV cache and a new set of output for the updated KV cache. This is not managed as an internal state of tract because we use the HuggingFace <code>transformers</code> library that design states to be held aside a stateless model, and the update happen at each forward pass by providing proper states arguments manually.</p> <p>The past KV-Cache tensors in graph inputs will need a new symbol that we can call <code>P</code> for past and that will lead to following set of additional constraints:</p> <pre><code>...\n    custom_extensions=[\n        \"tract_assert P &gt;= 0\",\n        \"tract_assert S &gt;= 1\",\n        \"tract_assert S+P &lt; \"\n        f\"{self.model_infos.max_position_embeddings}\",\n        # information about processing modes\n        \"tract_assert tg: S==1\",  # text generation\n        \"tract_assert pp: P==0\",  # prompt processing\n    ],\n...\n</code></pre> <p>Here again we introduce a new notation the modes:</p> <ul> <li>Each mode may have a different set of constraints (hence be optimized differently by tract).</li> </ul> <p>To avoid each new user of this library to define these cumbersome settings we provide a dedicated set of helpers for Languages models as we will see in the next section</p>"},{"location":"tutos/4_dynamic_axes/#demo-vad-with-tract-running-in-browser","title":"Demo:  VAD with tract running in browser","text":"<p>As an example of what we just learned we propose a simple VAD running live in this web-page.</p> <p> </p> <p>Note</p> <p>This model is not trained by SONOS so prediction accuracy is responsibility of original nemo authors. Inference performance is descent, but little to no effort was made to make tract WASM efficient (no SIMD WASM, no WebGPU kernels), this demo is for demonstration purpose.</p> <p>Curious to read the code behind it ? Just look at our example directory here and this raw page content.</p>"},{"location":"tutos/5_llm/","title":"5. Large Language Models Support","text":"<p>Goals</p> <p>At the end of this tutorial you will know:</p> <ol> <li> How to export causal Large Language Models</li> <li> Current status of this library with regard to LLM</li> </ol> <p>Prerequisite</p> <ul> <li> PyTorch and Python basics</li> <li> 10 min to read this page</li> </ul> <p>Since 2020, Large Language Models have gathered significant attention in the industry to the point where every product start to integrate them. tract have been polishing for this special networks since late 2023, and the inference engine is now competitive with state of the art on Apple Silicon and more recently on Nvidia's GPUs. In the industry most players use the <code>transformers</code> library and a lot of the HuggingFace ecosystem to specify their models in PyTorch. This make this library the most up to date source of Model architecture and pre-trained weights. To ease the export and experiments with such models <code>torch_to_nnef</code> (this library), has added a dedicated set of modules that we will now present to you.</p> <p>In this part we will only present ability to export to the <code>tract</code> inference engine.</p>"},{"location":"tutos/5_llm/#exporting-a-transformers-pre-trained-model","title":"Exporting a transformers pre-trained model","text":"<p>If you only want to export a model already trained, available on huggingface hub and compatible with the <code>transformers</code> library like for example: <code>meta-llama/Llama-3.2-1B-Instruct</code> for chat or text generation purpose. There is no need for you to learn the api's of <code>torch_to_nnef</code>, we have a nice easy to use command line for you (once <code>torch_to_nnef</code> is installed):</p> torch_to_nnef LLM cli<pre><code>t2n_export_llm_to_tract -e . --help\n</code></pre> <p>Which should output something like</p> <pre><code>usage: t2n_export_llm_to_tract [-h] -e EXPORT_DIRPATH [-s MODEL_SLUG] [-dt {f32,f16,bf16}] [-idt {f32,f16,bf16}] [-mp] [--compression-registry COMPRESSION_REGISTRY] [-d LOCAL_DIR]\n                               [-f32-attn] [-f32-lin-acc] [-f32-norm] [--num-logits-to-keep NUM_LOGITS_TO_KEEP] [--device-map DEVICE_MAP] [-tt {exact,approximate,close,very,super,ultra}]\n                               [-n {raw,natural_verbose,natural_verbose_camel,numeric}] [--tract-specific-path TRACT_SPECIFIC_PATH] [--tract-specific-version TRACT_SPECIFIC_VERSION] [-td]\n                               [-dwtac] [-sgts SAMPLE_GENERATION_TOTAL_SIZE] [-iaed] [-nv] [-v]\n                               [-c {min_max_q4_0,min_max_q4_0_with_embeddings,min_max_q4_0_with_embeddings_99,min_max_q4_0_all}]\n...\n</code></pre> <p>Ok, there is a lot of options here, instead let's do a concrete export of the <code>meta-llama/Llama-3.2-1B-Instruct</code> we mentioned earlier:</p> <pre><code>t2n_export_llm_to_tract \\\n    -s \"meta-llama/Llama-3.2-1B-Instruct\" \\\n    -dt f16 \\\n    -e $HOME/llama32_1B_f16 \\\n    --dump-with-tokenizer-and-conf \\\n    --tract-check-io-tolerance ultra\n</code></pre> <p>On a modern laptop with HuggingFace model already cached locally it should take around 50 seconds to export to NNEF. Tips: if you have <code>rich</code> installed as dependency, logs will be displayed in color and more elegantly.</p> <p>Here we export the llama 3.2 referenced from PyTorch where the model is mostly stored in <code>float16</code> temporary activations in <code>bfloat16</code> to tract where almost all will be in <code>float16</code> (given our <code>-dt</code> request, excepted for normalization kept in <code>f32</code>), we also check conformance between tract and PyTorch on a generic text (in english) and observe in the line of the log that it match:</p> <pre><code>IO bit match between tract and PyTorch for ...\n</code></pre> <p>Looking at what we just exported we see in the folder just created <code>$HOME/llama32_1B_f16</code>:</p> <pre><code>[2.3G]  $HOME/llama32_1B_f16\n\u251c\u2500\u2500 [2.3G]  model\n\u2502   \u251c\u2500\u2500 [2.2K]  config.json\n\u2502   \u2514\u2500\u2500 [2.3G]  model.nnef.tgz\n\u251c\u2500\u2500 [  78]  modes.json\n\u251c\u2500\u2500 [4.0M]  tests\n\u2502   \u251c\u2500\u2500 [838K]  export_io.npz\n\u2502   \u251c\u2500\u2500 [902K]  prompt_io.npz\n\u2502   \u251c\u2500\u2500 [1.1M]  prompt_with_past_io.npz\n\u2502   \u2514\u2500\u2500 [1.2M]  text_generation_io.npz\n\u2514\u2500\u2500 [ 16M]  tokenizer\n    \u251c\u2500\u2500 [3.7K]  chat_template.jinja\n    \u251c\u2500\u2500 [ 296]  special_tokens_map.json\n    \u251c\u2500\u2500 [ 49K]  tokenizer_config.json\n    \u2514\u2500\u2500 [ 16M]  tokenizer.json\n</code></pre> <p>The most important file being the NNEF dump of the model of 2.3Go.</p> <p>If we look at the signature of generated model we should see something like this:</p> <pre><code>graph network(\n    input_ids,\n    in_cache_key_0, in_cache_value_0,\n    ...,\n    in_cache_key_15, in_cache_value_15)\n\n-&gt; (\n    outputs,\n    out_cache_key_0, out_cache_value_0,\n    ...,\n    out_cache_key_15, out_cache_value_15\n)\n</code></pre> <p>To run such model you can for example use this crate of tract.</p> <p>work in progress</p> <p>This cli is still early stage, we intend to support embedding &amp; classification in a near future, as well as other modalities model like Visual and Audio LM.</p> <p>This same cli allows you to export a model that you would have fine-tuned yourself and saved with <code>.save_pretrained</code> by replacing the <code>-s {HUGGING_FACE_SLUG}</code> by a <code>-d {MY_LOCAL_DIR_PATH_TO_TRANSFORMERS_MODEL_WEIGHTS}</code>, if you did your finetuning with PEFT you can just add <code>-mp</code> to merge the PEFT weights before export (in-case this is your wish: this will allow faster inference but remove ability to have multiple 'PEFT finetuning' sharing same base exported model).</p>"},{"location":"tutos/5_llm/#quantize-your-model","title":"Quantize your model","text":"<p>Quantization of models is essential to get the best model on limited resource devices. It is also very simple to apply opt-in at export time with this command line:</p> <ul> <li><code>--compression-registry</code> that control the registry that contains the quantization methods available. It can be any dict from installed modules     including modules from external packages (different from <code>torch_to_nnef</code>).</li> <li><code>--compression-method</code> that select the quantization method to apply, as a toy example you can export the linear layers of a model in Q40 (that means: 4bit symmetric quantization with a granularity per group of 32 elements, totaling 4.5bpw) with simple <code>min_max_q4_0</code>. If you wish to leverage best quantization techniques we recommend you to read our tutorial on Quantization and export to implement your own (SONOS has a closed source package doing just that).</li> </ul>"},{"location":"tutos/5_llm/#export-a-model-that-does-not-fit-in-ram","title":"Export a model that does not fit in RAM","text":"<p>You want to go big, but you find that renting an instance will hundreds of Go of RAM just to export a model is ridiculous ? We agree ! The CLI described upper provide a convenient solution if you have a descent SSD disk just add:</p> <pre><code>--device-map t2n_offload_disk\n</code></pre> <p>to your prior command like for example:</p> <pre><code>t2n_export_llm_to_tract \\\n    --device-map t2n_offload_disk \\\n    -s \"Qwen/Qwen3-8B\" \\\n    -dt f16 \\\n    -f32-attn \\\n    -e $HOME/qwen3_8B \\\n    --dump-with-tokenizer-and-conf \\\n    --tract-check-io-tolerance ultra\n</code></pre> <p>And pouf done. It will be a bit slower because SSD are slower than RAM but hey exporting Qwen3 8B in f16 takes around 4min for a 16Go stored model (this trade is fine for most big models). See our offloaded tensor tutorial to learn more about how to leverage this further (even in your PyTorch based apps).</p>"},{"location":"tutos/5_llm/#export-a-model-from-different-library","title":"Export a model from different library","text":"<p>As long as your model can be serialized into the <code>torch.jit</code> internal intermediate representation (which is the case of almost all neural-networks, whole or parts). This library should be able to do the heavy lifting of the translation to NNEF for you.</p> <p>Here are few key considerations to take before starting to support a non transformers (here we are speaking of the package, not the other architectures like Mamba, RWKV, ...) language model:</p> <ul> <li>How past states of your neural network is managed inside the library, is it like transformers an external design that pass as input and output of your neural network main module all states (like KV-cache) ?</li> <li>If not is it easy to transform the library internal modeling to approach this architecture ?</li> </ul> <p>If you can answer yes to one of those 2 questions congratulation, you should be able to easily adapt these transformers specific torch_to_nnef modules.</p> <p>Else if state management is internal to specific modules you will likely need to write custom operator exporter to express those IO at export time or add specific operators in tract to manage it.</p> <p>In all cases, prior tutorials should be able to help you toward your goal especially with regard to <code>dynamic axes</code> and basic api.</p> <p>Community</p> <p>If you release a custom LLM NNEF export for a different library than <code>transformers</code> based on <code>torch_to_nnef</code> Please reach to us we would love to hear your feedback \ud83d\ude0a</p>"},{"location":"tutos/5_llm/#demo-llm-poetry-generator","title":"Demo:  LLM Poetry generator","text":"<p>Using the knowledge you acquired during this tutorial and a bit of extra for WASM in rust.  We demo the use a minimal Large Language model named <code>Smollm 125m</code> running in your browser (total experiment is &lt;100 mo download).</p> <p> </p> <p>Note</p> <p>This model is not trained by SONOS so generation accuracy is responsibility of original HuggingFace authors. Inference performance is descent, but little to no effort was made to make tract WASM efficient, this demo is for demonstration purpose.</p> <p>Curious to read the code behind it ? Just look at our example directory here and this raw page content.</p>"},{"location":"tutos/6_quantization/","title":"6. Quantization","text":"<p>Goals</p> <p>At the end of this tutorial you will be able to:</p> <ol> <li> Use quantization interfaces in <code>torch_to_nnef</code></li> <li> Define your own quantization library on top of <code>torch_to_nnef</code></li> </ol> <p>Prerequisite</p> <ul> <li> PyTorch and Python basics</li> <li> Understanding of what is quantization for Neural network</li> <li> 10 min to read this page</li> </ul> Illustration by Maarten Grootendorst <p>Quantization is a set of techniques that allow to reduce significantly the model size, and in case of memory-bound computations during model inference: speed up model as well. These techniques reduce the 'size' needed to store the numerical values representing the parameters of the neural network.</p> <p>In order to make those techniques efficient, the inference engine that runs the neural network needs to have in most cases, specific kernels to support the quantization scheme selected.</p> <p><code>torch_to_nnef</code> primary support today being <code>tract</code>, the quantization presented here are all targeting this inference engine.</p> <p>To date tract support 2 kind of quantization:</p> <ul> <li>Q40: almost identical to GGUF Q40, it targets weights only (not activations) where matmul and embedding/gathering operations, transform those into float activations.</li> <li>8 bit asymmetric per tensor quantization built-in in PyTorch that can target weights and activations and allow integer only arithmetic</li> </ul> <p>Let's take a look at each in turn starting by Q40.</p>"},{"location":"tutos/6_quantization/#custom-tensor-quantization-support","title":"Custom Tensor quantization support","text":""},{"location":"tutos/6_quantization/#q40-llm-export-example","title":"Q40 LLM Export example","text":"<p>For LLM as we explained in prior tutorial quantization is as simple as adding the <code>-c</code> (or <code>--compression-method</code>) option with <code>min_max_q4_0_all</code>.</p> <pre><code>t2n_export_llm_to_tract \\\n    -s \"meta-llama/Llama-3.2-1B-Instruct\" \\\n    -dt f16 \\\n    -f32-attn \\\n    -e $HOME/llama32_1B_q40 \\\n    --dump-with-tokenizer-and-conf \\\n    --tract-check-io-tolerance ultra \\\n    -c \"min_max_q4_0_all\"\n</code></pre> <p>It should take around the same time to export (quantization time being compensated by less content to dump on disk).</p> <p>Ok that's nice, but where does this registry come from ?</p> <p>The registry location is defined with the <code>--compression-registry</code> which by default point to:</p>"},{"location":"tutos/6_quantization/#torch_to_nnef.compress.DEFAULT_COMPRESSION","title":"torch_to_nnef.compress.DEFAULT_COMPRESSION  <code>module-attribute</code>","text":"<pre><code>DEFAULT_COMPRESSION = {'min_max_q4_0': quantize_weights_min_max_Q4_0, 'min_max_q4_0_with_embeddings': partial(quantize_weights_min_max_Q4_0, to_quantize_module_classes=(Linear, Embedding)), 'min_max_q4_0_with_embeddings_99': partial(partial(quantize_weights_min_max_Q4_0, percentile=0.99), to_quantize_module_classes=(Linear, Embedding)), 'min_max_q4_0_all': partial(quantize_weights_min_max_Q4_0, to_quantize_module_classes=(Linear, Embedding, Conv1d, Conv2d))}\n</code></pre>"},{"location":"tutos/6_quantization/#defining-your-own-llm-quantization-registry","title":"Defining your own LLM quantization registry","text":"<p>Anyone can create a new registry as long as it follows those rules:</p> <ul> <li>accessible as a global variable dict</li> <li>with as key a string that reference the compression to apply</li> <li>as value a function that has the following signature:</li> </ul> <pre><code>def my_quantization_function(\n    model, # your torch.nn.Module / full model to be quantized\n    # huggingface tokenizer or equivalent\n    tokenizer,\n    # may be usefull to dump compression evaluations results\n    # or some specific metrics\n    export_dirpath,\n    # original trained model location\n    # may be usefull to perform internal evaluations of reference\n    # when more data than just llm torch is available\n    local_dir,\n):\n    pass\n</code></pre> <p>A typical function will transform some model tensors (parameters, buffers, ...) into <code>torch_to_nnef.tensor.QTensor</code> a concrete QTensor that supports NNEF export today being:</p> <ul> <li> </li> </ul> <p>QTensor today only support export to <code>Q40</code> (that means: 4bit symmetric quantization with a granularity per group of 32 elements, totaling 4.5bpw).</p> <p>A <code>QTensor</code> is a Python object that behaves and should be used as a classical <code>torch.Tensor</code> with few exceptions: it can not hold any gradient, it can not be modified, it contains internals objects necessary to its definition like:</p> <ul> <li>A blob of binary data (the compressed information) named <code>u8_blob</code></li> <li>A <code>torch_to_nnef.tensor.quant.Qscheme</code> which define how to quantize/ dequantize the blob from u8 (like QScalePerGroupF16)</li> <li>A list of U8Compressor that can act on the u8 blob and compress it further by for example applying bit-packing to it. Say each represented element is specified in 4 bit (16 value represented) without compressor we waste 4 bit per element because each element take 8bit (here we ignore the attached quantization information that add up to the size). Also Compressor are not necessary just bit-packing that can be any kind of classical compression algorithm (Huffman, Lzma, ...) as long as the compression is lossless.</li> </ul> <p>Each access to the QTensor for torch operations will make it be decompressed on the fly saving RAM allocation when unused. This QTensor will also be identified by <code>torch_to_nnef</code> at export time and translated to requested <code>.dat</code> based on the specific method:</p> <pre><code>def write_in_file(\n        self,\n        dirpath: T.Union[str, Path],\n        label: str,\n        inference_target: InferenceTarget = TractNNEF.latest(),\n ):\n    pass\n</code></pre> <p>Each subclass will define how to dump it (for example for tract Q40).</p> <p>The transformation from a float tensor to a Q40 QTensor can be done through a step we call tensor quantization which may be as simple as a min and max calibration as shown in the function fp_to_tract_q4_0_with_min_max_calibration, but all compatible techniques can be applied like GPTQ, AWQ, ... (those are just not part of <code>torch_to_nnef</code> package which intend to just provide common primitive to be easily exportable).</p> <p>A concrete example of <code>my_quantization_function</code> can be found compress module here.</p> <p>Today Q40 is supported by tract on matmul, convolutions and embeddings operations. The <code>min_max_q4_0_all</code> will try to apply it to all those encountered modules within a model.</p>"},{"location":"tutos/6_quantization/#torch_to_nnef.tensor.quant.qtract.QTensorTractScaleOnly","title":"torch_to_nnef.tensor.quant.qtract.QTensorTractScaleOnly","text":"<pre><code>QTensorTractScaleOnly(*args, specific_machine: T.Optional[str] = None, **kwargs)\n</code></pre> <p>               Bases: <code>QTensorTract</code></p> <p>Tract data format it serializes to: Q4_0.</p>"},{"location":"tutos/6_quantization/#torch_to_nnef.tensor.quant.qtract.QTensorTractScaleOnly.decompress","title":"decompress","text":"<pre><code>decompress()\n</code></pre> <p>Tract dequantization depends on hardware.</p> <p>Typically dequantization happen with ops in f16 on ARM and f32 (scale directly casted) on others so we overwrite the function to be consistant with tract.</p>"},{"location":"tutos/6_quantization/#q40-for-non-llm-network","title":"Q40 for non-llm network","text":"<p>By reading the previous section you should now understand that beyond specific calibration which is not part of this library all of what was explained apply to all neural network parameters used in matmul (nn.Linear, F.linear, ...), conv (Conv1d, Conv2d), and embeddings (gather operator). In fact you can just reuse as is the compress method we referenced upper on any neural network defined in PyTorch it should just work.</p>"},{"location":"tutos/6_quantization/#q40-use-specific-quantization-method","title":"Q40 Use specific quantization method","text":"<p>Ok min-max is cool, but quality it provides in Q40 is bad, how do I implement my own quantization (even with prior sections, I feel confused) ?</p> <p>Let's take an example step by step:</p> <ol> <li> <p>We will use Q40 so no need to redefine the QTensor nor the QScheme, we will just have to create a new tensor quantization function register</p> </li> <li> <p>Let's create a custom register on our own module <code>super_quant.py</code> where we will implement a scale grid search based on Mean Square Error calibration for the demo.</p> </li> <li> <p>we first copy almost same function as <code>quantize_weights_min_max_Q4_0</code> and rename it <code>quantize_weights_grid_mse_Q40</code> and adapt it slightly</p> </li> </ol> super_quant.py<pre><code>from functools import partial\nimport logging\nimport typing as T\nimport torch\nfrom torch import nn\nfrom torch_to_nnef.compress import offloaded_tensor_qtensor\nfrom torch_to_nnef.exceptions import T2NErrorImpossibleQuantization\nfrom torch_to_nnef.tensor.offload import OffloadedTensor\nfrom torch_to_nnef.tensor.quant import QTensor\nfrom torch_to_nnef.tensor.updater import ModTensorUpdater\n\nLOGGER = logging.getLogger(__name__)\n\ndef fp_to_tract_q4_0_with_grid_mse_calibration(weight, grid_size=100, maxshrink=0.8):\n    # TODO: implementation\n    pass\n\n\ndef quantize_weights_grid_mse_Q40(model: nn.Module, **kwargs):\n    to_quantize_module_classes = kwargs.get(\n        \"to_quantize_module_classes\", (nn.Linear,)\n    )\n    assert isinstance(to_quantize_module_classes, tuple), (\n        to_quantize_module_classes\n    )\n    assert all(issubclass(_, nn.Module) for _ in to_quantize_module_classes), (\n        to_quantize_module_classes\n    )\n    with torch.no_grad():\n        ids_to_qtensor: T.Dict[int, T.Tuple[QTensor, OffloadedTensor]] = {}\n        \"\"\" try to avoid quant if used in other operators like mix of embedding/linear if linear only quant \"\"\"\n        mod_tensor_updater = ModTensorUpdater(model)\n\n        for name, mod in model.named_modules():\n            if isinstance(mod, to_quantize_module_classes):\n                LOGGER.info(f\"quantize layer: {name}\")\n                weight_id = id(getattr(mod, \"weight\"))\n                if weight_id in ids_to_qtensor:\n                    LOGGER.info(\n                        f\"detected shared weight between: '{ids_to_qtensor[weight_id].nnef_name}' and '{name}.weight'\"\n                    )\n                    continue\n                if not all(\n                    isinstance(m, to_quantize_module_classes)\n                    for m in mod_tensor_updater.id_to_modules[weight_id]\n                ):\n                    clss = [\n                        m.__class__\n                        for m in mod_tensor_updater.id_to_modules[weight_id]\n                    ]\n                    LOGGER.warning(\n                        f\"detected shared weight: '{name}' candidate has incompatible layer usage: {clss}, \"\n                        f\" but requested {to_quantize_module_classes}\"\n                    )\n                    continue\n                try:\n\n                    def q_fn(weight):\n                        q_weight = fp_to_tract_q4_0_with_grid_mse_calibration(\n                            weight,\n                            **{\n                                k: v\n                                for k, v in kwargs.items()\n                                if k in [\"grid_size\"]\n                            },\n                        )\n                        q_weight.nnef_name = f\"{name}.weight\"\n                        return q_weight\n\n                    q_weight = offloaded_tensor_qtensor(\n                        q_fn, mod.weight, \"q40_mse\"\n                    )\n                except T2NErrorImpossibleQuantization as exp:\n                    LOGGER.error(f\"quant layer: {name} error: {exp}\")\n                    continue\n                # =&gt; needs assignation next cause update_by_ref may create new Parameter object\n                q_weight = mod_tensor_updater.update_by_ref(\n                    getattr(mod, \"weight\"), q_weight\n                )\n                ids_to_qtensor[id(q_weight)] = q_weight\n    return model\n\nEXAMPLE_REGISTRY = {\n    \"grid_mse_q4_0_all\": partial(\n        quantize_weights_grid_mse_Q40,\n        grid_size=100,\n        to_quantize_module_classes=(\n            nn.Linear,\n            nn.Embedding,\n            nn.Conv1d,\n            nn.Conv2d,\n        ),\n    ),\n}\n</code></pre> <p>Note here the use of <code>ModTensorUpdater</code> this module updater allows to avoid breaking shared reference to a common tensor inside your network (by example embedding layer shared between input and output of a LLM) while updating the weights.</p> <p>We now just need to fill the <code>fp_to_tract_q4_0_with_grid_mse_calibration</code> function and we are done. Also note that I could have done a calibration stage with external data before end at the beginning (some quantization method need to minimize quantization error for activations). In this case we opt for simplicity:</p> <pre><code>from torch_to_nnef.tensor.quant import fp_to_tract_q4_0_with_min_max_calibration\n\ndef fp_to_tract_q4_0_with_grid_mse_calibration(\n    fp_weight, grid_size=50, maxshrink=0.8\n):\n    qtensor = fp_to_tract_q4_0_with_min_max_calibration(fp_weight)\n    qscheme_min_max = qtensor.qscheme\n    lower_bound_search_vals = qscheme_min_max.scale * maxshrink\n    step_size = (qscheme_min_max.scale - lower_bound_search_vals) / grid_size\n    current_vals = qscheme_min_max.scale.clone()\n    best_vals = current_vals\n\n    def get_current_error():\n        return (\n            ((fp_weight - qtensor.decompress()).abs() ** 2)\n            .view(-1, qscheme_min_max.group_size)\n            .mean(1)\n        )\n\n    best_val_error = get_current_error()\n    orignal_val_error = best_val_error.clone()\n    for _ in range(grid_size):\n        current_vals -= step_size\n        qtensor.qscheme.scale = current_vals.clone()\n        current_val_error = get_current_error()\n        better_error = current_val_error &lt; best_val_error\n        best_val_error = torch.where(\n            better_error, current_val_error, best_val_error\n        )\n        best_vals = torch.where(\n            better_error.unsqueeze(1), current_vals, best_vals\n        )\n    gain_over_min_max = (orignal_val_error - best_val_error).mean()\n    LOGGER.info(\n        f\"[{fp_weight.name}] quant grid search gained mse error from min/max: {gain_over_min_max}\"\n    )\n    qtensor.qscheme.scale = best_vals\n    return qtensor\n</code></pre> <p>Ok now we can simply run the register we just created by pointing it out with arguments we stated in upper sections and observe the magic. Of course gain using this mse technique are modest, but you now have the full knowledge to make your own super quant .</p>"},{"location":"tutos/6_quantization/#8bit-post-training-quantization-example","title":"8bit Post Training Quantization example","text":"<p>Quantization in 8bit including activation is something that is built-in PyTorch since a while. This is great because it means this is as well represented in the Intermediate representation after graph is traced, hence easily exportable with <code>torch_to_nnef</code>. Still, today tract only support 8bit asymmetric linear quantization per tensor (no per channel).</p> <p>We will demonstrate this ability on a simple usecase:</p> <p>Let's do a CNN + ReLU example and apply a classical PTQ from there:</p> simple PTQ export example<pre><code>from pathlib import Path\nimport torch\nfrom torch import nn\nfrom torch_to_nnef import TractNNEF, export_model_to_nnef\n\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.quant = torch.ao.quantization.QuantStub()\n        self.cnn1 = nn.Conv1d(10, 10, 3)\n        self.relu1 = nn.ReLU()\n        self.cnn2 = nn.Conv1d(10, 1, 3)\n        self.relu2 = nn.ReLU()\n        self.dequant = torch.ao.quantization.DeQuantStub()\n\n    def forward(self, x):\n        x = self.quant(x)\n        x = self.cnn1(x)\n        x = self.relu1(x)\n        x = self.cnn2(x)\n        x = self.relu2(x)\n        x = self.dequant(x)\n        return x\n\n\ntorch.backends.quantized.engine = \"qnnpack\"\nm = Model()\nm.eval()\nm.qconfig = torch.ao.quantization.get_default_qconfig(\"qnnpack\")\nmf = torch.ao.quantization.fuse_modules(\n    m, [[\"cnn1\", \"relu1\"], [\"cnn2\", \"relu2\"]]\n)\nmp = torch.ao.quantization.prepare(mf)\ninput_fp32 = torch.randn(1, 10, 15)\nmp(input_fp32)\nmodel_int8 = torch.ao.quantization.convert(mp)\nres = model_int8(input_fp32)\nfile_path_export = Path(\"model_q8_ptq.nnef.tgz\")\nexport_model_to_nnef(\n    model=model_int8,\n    args=input_fp32,\n    file_path_export=file_path_export,\n    inference_target=TractNNEF(\n        version=\"0.21.13\",\n        check_io=True,\n    ),\n    input_names=[\"input\"],\n    output_names=[\"output\"],\n    debug_bundle_path=Path(\"./debug.tgz\"),\n)\n</code></pre> <p>if you look at the model graph.nnef, there is no difference with a classical NNEF model but .dat exported are uint8 and a new textual file is set called  <code>graph.quant</code>.</p> <p>This new file contains quantization information for each tensors as follows:</p> graph.quant (with scale truncated for clarity)<pre><code>\"quant__input_quantize_per_tensor0\": zero_point_linear_quantize(zero_point = 119, scale = 0.021, bits = 8, signed = false, symmetric = false);\n\"cnn1__input_weight\": zero_point_linear_quantize(scale = 0.0014, zero_point = 0, bits = 8, signed = true, symmetric = false);\n\"cnn1__input_conv\": zero_point_linear_quantize(scale = 0.0046, zero_point = 0, bits = 8, signed = false, symmetric = false);\n\"cnn1__input\": zero_point_linear_quantize(scale = 0.0046, zero_point = 0, bits = 8, signed = false, symmetric = false);\n\"cnn2__Xq_weight\": zero_point_linear_quantize(scale = 0.0013, zero_point = 0, bits = 8, signed = true, symmetric = false);\n\"cnn2__Xq_conv\": zero_point_linear_quantize(scale = 0.0017, zero_point = 0, bits = 8, signed = false, symmetric = false);\n\"cnn2__Xq\": zero_point_linear_quantize(scale = 0.00172, zero_point = 0, bits = 8, signed = false, symmetric = false);\n</code></pre> <p>Finally running tract cli on this model:</p> <pre><code>tract ./model_q8_ptq.nnef.tgz --nnef-tract-core dump\n</code></pre> <p>You should observe that operators are correctly understood as Quantized with QU8 notation:</p> <pre><code> input\n\u2503   \u2501\u2501\u2501 1,10,15,F32\n\u2523 1 Cast quant__input_quantize_per_tensor0\n\u2503   \u2501\u2501\u2501 1,10,15,QU8(Z:119 S:0.021361168)\n\u2523\u253b\u253b\u253b\u253b\u253b\u253b\u253b\u253b 9 Conv cnn1__input_conv_4\n\u2503   \u2501\u2501\u2501 1,10,13,QU8(Z:0 S:0.004661602)\n\u2523\u253b 11 Max cnn1__input_relu_y_5\n\u2523\u253b\u253b\u253b\u253b\u253b\u253b\u253b\u253b 16 Conv cnn2__Xq_conv_1\n\u2503   \u2501\u2501\u2501 1,1,11,QU8(Z:0 S:0.0017290331)\n\u2523\u253b 18 Max cnn2__Xq_relu_y_4\n\u2523 19 Cast output\n    \u2501\u2501\u2501 1,1,11,F32\n</code></pre> <p>Congratulation</p> <p>You exported your first quantized network with <code>torch_to_nnef</code> in 8bit and learned how to create and manage your own quantization registry !</p>"},{"location":"tutos/7_offloaded_tensor/","title":"7. Offloaded Tensors","text":"<p>Goals</p> <p>At the end of this tutorial you will be able to:</p> <ol> <li> Use offloaded tensors when wished</li> </ol> <p>Prerequisite</p> <ul> <li> PyTorch and Python basics</li> <li> 5 min to read this page</li> </ul> <p>Offload tensors have been developed to allow to manipulate and export more easily large neural network models.</p> <p>Recall that if you only want to export a LLM model offloaded you can look at our related LLM tutorial and do not need to look at what happen behind.</p> <p>This class is defined as such:</p> <ul> <li> </li> </ul> <p>You can directly load any .safetensor or .pt into this object that will mimic classical <code>torch.Tensor</code> except that each access will load the Tensor from disk and remove it from RAM as soon as those are not needed, allowing to manipulate very large model bit by bit. It is composable with other <code>torch_to_nnef.tensor.opaque.OpaqueTensor</code> such as <code>QTensor</code>.</p> <p>To load from disk without overhead, you can call the <code>t2n_load_checkpoint_and_dispatch</code> with appropriate options like in the following example:</p> example of offload usage from disk (extracted from LLM exporter)<pre><code>import tempfile\nfrom pathlib import Path\nfrom torch_to_nnef.tensor.offload import (\n    ON_DISK_DEVICE_MAP_KEY,\n    t2n_load_checkpoint_and_dispatch,\n)\nfrom torch_to_nnef.utils import init_empty_weights\n\nfrom transformers import AutoModelForCausalLM\nimport huggingface_hub\n\nslug = \"meta-llama/Llama-3.2-1B-Instruct\"\nwith init_empty_weights():\n    # model instantiation with empty tensors\n    # this can be come from any library (here transformers)\n    model = AutoModelForCausalLM.from_pretrained(slug, **kwargs)\nhf_repo_files = huggingface_hub.list_repo_files(slug)\nweights_location = Path(\n    huggingface_hub.hf_hub_download(\n        slug, hf_repo_files[-1]\n    )  # assume at least 1 file is in targeted repo\n).parent\n\n# here model tensors are properly loaded into\nt2n_load_checkpoint_and_dispatch(\n    model,\n    weights_location,\n    device_map=ON_DISK_DEVICE_MAP_KEY,\n    offload_dir=Path(tempfile.mkdtemp(suffix=\"offload_t2n\")),\n)\n</code></pre> <p>These <code>OffloadedTensor</code> are also very useful to implement into quantization techniques to support very large model quantization with a calibration based on observed values like Hessian from activation. Indeed if we think of the Hessian example: these square matrices can be pretty large especially when multiplied by the number of activations on a big neural network.</p> <p>If you only wish to maintain QTensor into OffloadedTensor if original float tensor was offloaded you can just use the helper:</p> <ul> <li> </li> </ul> <p>If this is a new tensor just use the <code>OffloadedTensor.from_original_tensor</code> defined upper.</p>"},{"location":"tutos/7_offloaded_tensor/#torch_to_nnef.tensor.offload.OffloadedTensor","title":"torch_to_nnef.tensor.offload.OffloadedTensor","text":"<pre><code>OffloadedTensor(elem, device, offload_dir: Path, name: str, offloaded_tensor_type: T.Type[torch.Tensor], force_gc_collect: bool = False)\n</code></pre> <p>               Bases: <code>OpaqueTensor</code></p> <p>Tensor subclass that maintains data on disk.</p> <p>It hold an virtual internal memory storage (permanent) and a temporary instantiation at each operation accessing it on targeted device.</p> Warning <p>we recommend to version of PyTorch &gt; 1.12 for best compatibility.</p>"},{"location":"tutos/7_offloaded_tensor/#torch_to_nnef.tensor.offload.OffloadedTensor.is_meta","title":"is_meta  <code>property</code>","text":"<pre><code>is_meta: bool\n</code></pre> <p>Whether the tensor is on the meta device.</p> <p>Always False as the tensor is (off|re)loaded from disk.</p>"},{"location":"tutos/7_offloaded_tensor/#torch_to_nnef.tensor.offload.OffloadedTensor.from_original_tensor","title":"from_original_tensor  <code>classmethod</code>","text":"<pre><code>from_original_tensor(tensor: torch.Tensor, name: str, offload_dir: T.Optional[Path] = None, suffix_log_msg: str = '')\n</code></pre> <p>Take a torch.Tensor or OpaqueTensor and offload it to disk.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>the torch.Tensor or torch_to_nnef.tensor.OpaqueTensor to dump on disk</p> required <code>name</code> <code>str</code> <p>the name of the tensor that will be used to create the filename store on disk</p> required <code>offload_dir</code> <code>Optional[Path]</code> <p>The directory where this file will be stored (temporarly)</p> <code>None</code> <code>suffix_log_msg</code> <code>str</code> <p>Added message log suffix for context</p> <code>''</code>"},{"location":"tutos/7_offloaded_tensor/#torch_to_nnef.tensor.offload.OffloadedTensor.to","title":"to","text":"<pre><code>to(*args, **kwargs)\n</code></pre> <p>Change the target device when reloaded in memory.</p>"},{"location":"tutos/7_offloaded_tensor/#torch_to_nnef.tensor.offload.OffloadedTensor.update_values","title":"update_values","text":"<pre><code>update_values(values: torch.Tensor, strict_shape: bool = True, strict_dtype: bool = True)\n</code></pre> <p>Replace offloaded tensor by new 'values' tensor.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Tensor</code> <p>The tensor that will replace it on disk assertion are made to ensure same shape, dtype as prior</p> required <code>strict_shape</code> <code>bool</code> <p>if True (default) the shape of the new tensor must be the same as the prior one</p> <code>True</code> <code>strict_dtype</code> <code>bool</code> <p>if True (default) the dtype of the new tensor must be the same as the prior one</p> <code>True</code>"},{"location":"tutos/7_offloaded_tensor/#torch_to_nnef.compress.offloaded_tensor_qtensor","title":"torch_to_nnef.compress.offloaded_tensor_qtensor","text":"<pre><code>offloaded_tensor_qtensor(q_fn, tensor: torch.Tensor, suffix_name: str) -&gt; torch.Tensor\n</code></pre> <p>Maintains a QTensor offloaded if original tensor is offloaded.</p>"},{"location":"tutos/8_custom_operator/","title":"8. Custom operators","text":"<p>Goals</p> <p>At the end of this tutorial you will be able to:</p> <ol> <li> Control the transformation to NNEF of <code>nn.Module</code> as you wish.     This is often useful in case those modules are not representable in the     jit graph of PyTorch or because you wish to use custom NNEF operator for     your inference engine.</li> </ol> <p>Prerequisite</p> <ul> <li> PyTorch and Python basics</li> <li> 5 min to read this page</li> </ul> <p>Sometimes you want to control specific <code>torch.nn.Module</code> expansion to NNEF. It may happen because you want to use specific custom operator on inference target instead of basic primitives, or simply because you need to map to something that is not traceable, like for example (but not limited to) a physics engine.</p> <p>To this purpose with <code>torch_to_nnef</code>, you can create a subclass of <code>torch_to_nnef.op.custom_extractors.ModuleInfoExtractor</code> that is defined as such:</p> <ul> <li> </li> </ul> <p>To make it works you need to accomplish 4 steps:</p> <ol> <li>sub-classing it</li> <li>defining its <code>MODULE_CLASS</code> attribute.</li> <li>defining its <code>convert_to_nnef</code></li> <li>ensuring that the subclass you just defined is imported in your export script</li> </ol> <p>This would look something like that:</p> <pre><code>from torch_to_nnef.op.custom_extractors import ModuleInfoExtractor\n\nclass MyCustomHandler(ModuleInfoExtractor):\n    MODULE_CLASS = MyModuleToCustomConvert\n\n    def convert_to_nnef(\n        self,\n        g,\n        node,\n        name_to_tensor,\n        null_ref,\n        torch_graph,\n        **kwargs\n    ):\n        pass\n</code></pre> <p>You can take inspiration from our own management of RNN layers like:</p> <ul> <li> </li> </ul> <p>But ultimately this is just a chain of op's that needs to be written, inside the <code>g</code> graph, like we do when adding new aten operator</p>"},{"location":"tutos/8_custom_operator/#torch_to_nnef.op.custom_extractors.ModuleInfoExtractor","title":"torch_to_nnef.op.custom_extractors.ModuleInfoExtractor","text":"<pre><code>ModuleInfoExtractor()\n</code></pre> <p>Class to take manual control of NNEF expansion of a nn.Module.</p> <p>You need to subclass it, and set MODULE_CLASS according to your targeted module.</p> <p>Then write .convert_to_nnef according to your need.</p>"},{"location":"tutos/8_custom_operator/#torch_to_nnef.op.custom_extractors.ModuleInfoExtractor.convert_to_nnef","title":"convert_to_nnef","text":"<pre><code>convert_to_nnef(g, node, name_to_tensor, null_ref, torch_graph, inference_target, **kwargs)\n</code></pre> <p>Control NNEF content to be written for each MODULE_CLASS.</p> <p>This happen at macro level when converting from internal IR to NNEF IR stage.</p> <p>This is the Core method to overwrite in subclass.</p> <p>It is no different than any op implemented in <code>torch_to_nnef</code> in the module</p>"},{"location":"tutos/8_custom_operator/#torch_to_nnef.op.custom_extractors.ModuleInfoExtractor.generate_in_torch_graph","title":"generate_in_torch_graph","text":"<pre><code>generate_in_torch_graph(torch_graph, *args, **kwargs)\n</code></pre> <p>Internal method called by torch_to_nnef ir_graph.</p>"},{"location":"tutos/8_custom_operator/#torch_to_nnef.op.custom_extractors.ModuleInfoExtractor.get_by_kind","title":"get_by_kind  <code>classmethod</code>","text":"<pre><code>get_by_kind(kind: str)\n</code></pre> <p>Get ModuleInfoExtractor by kind in torch_to_nnef internal IR.</p>"},{"location":"tutos/8_custom_operator/#torch_to_nnef.op.custom_extractors.ModuleInfoExtractor.get_by_module","title":"get_by_module  <code>classmethod</code>","text":"<pre><code>get_by_module(module: nn.Module)\n</code></pre> <p>Search if the module is one of the MODULE_CLASS registered.</p> <p>return appropriate ModuleInfoExtractor subclass if found</p>"},{"location":"tutos/8_custom_operator/#torch_to_nnef.op.custom_extractors.ModuleInfoExtractor.ordered_args","title":"ordered_args","text":"<pre><code>ordered_args(torch_graph)\n</code></pre> <p>Odered args for the module call.</p> <p>Sometimes torch jit may reorder inputs. compared to targeted python ops in such case ordering need to be re-addressed</p>"},{"location":"tutos/8_custom_operator/#torch_to_nnef.op.custom_extractors.LSTMExtractor","title":"torch_to_nnef.op.custom_extractors.LSTMExtractor","text":"<pre><code>LSTMExtractor()\n</code></pre> <p>               Bases: <code>_RNNMixin</code>, <code>ModuleInfoExtractor</code></p>"},{"location":"tutos/9_peft/","title":"9. Export selected tensors &amp; PEFT","text":"<p>Goals</p> <p>At the end of this tutorial you will be able to:</p> <ol> <li> Export specific set of weight tensors without the graph.</li> </ol> <p>Prerequisite</p> <ul> <li> PyTorch and Python basics</li> <li> 5 min to read this page</li> </ul> <p>Sometime you wish to export only some weights, this happen for example if you fine-tuned a LLM with a PEFT technique like LoRA. In this case only a very limited set of weights are modified compared to the pre-trained neural network. So this can make sense to ship to the customer the base model only once (with the graph) at the beginning (for example app download), and then to send the LoRA weights separately as you iterate through your product (for example app update).</p> <p>This patching logic is also very interesting in case you wish to allow multiple PEFT to be shipped and switched/selected at model load time.</p> <p>To do so <code>torch_to_nnef</code> provide few convenient methods.</p>"},{"location":"tutos/9_peft/#command-line","title":"Command line","text":"<p>First if you happen to exactly want to export PEFT weights, we have a CLI for you:</p> <pre><code># filepath can be .pth, .bin, .safetensors ...\nt2n_export_peft_to_nnef \\\n    --read-filepath /my-little-model-file.pt \\\n    -o /tmp/my-outputdir\n</code></pre> <p>By default it exports LoRA weights, if you wish to apply it on different methods look at additional options (with <code>--help</code>), the core functionality behind this CLI is simple pattern matching so most of PEFT weight names matching with regex should work (DoRA, ...).</p>"},{"location":"tutos/9_peft/#basic-api","title":"Basic API","text":"<p>If you wish to have programmatic control you can also use for on disk tensors:</p> <p>Or from loaded tensors:</p>"},{"location":"tutos/9_peft/#torch_to_nnef.export_tensors_from_disk_to_nnef","title":"torch_to_nnef.export_tensors_from_disk_to_nnef","text":"<pre><code>export_tensors_from_disk_to_nnef(store_filepath: T.Union[Path, str], output_dir: T.Union[Path, str], filter_key: T.Optional[T.Callable[[str], bool]] = None, fn_check_found_tensors: T.Optional[T.Callable[[T.Dict[str, _Tensor]], bool]] = None) -&gt; T.Dict[str, _Tensor]\n</code></pre> <p>Export any statedict or safetensors file torch.Tensors to NNEF .dat file.</p> <p>Parameters:</p> Name Type Description Default <code>store_filepath</code> <code>Union[Path, str]</code> <p>the filepath that hold the .safetensors , .pt or .bin containing the state dict</p> required <code>output_dir</code> <code>Union[Path, str]</code> <p>directory to dump the NNEF tensor .dat files</p> required <code>filter_key</code> <code>Optional[Callable[[str], bool]]</code> <p>An optional function to filter specific keys to be exported</p> <code>None</code> <code>fn_check_found_tensors</code> <code>Optional[Callable[[Dict[str, _Tensor]], bool]]</code> <p>post checking function to ensure all requested tensors have effectively been dumped</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, _Tensor]</code> <p>a dict of tensor name as key and torch.Tensor values, identical to <code>torch_to_nnef.export.export_tensors_to_nnef</code></p> <p>Examples:</p> <p>Simple filtered example</p> <pre><code>&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; class Mod(nn.Module):\n...     def __init__(self):\n...         super().__init__()\n...         self.a = nn.Linear(1, 5)\n...         self.b = nn.Linear(5, 1)\n...\n...     def forward(self, x):\n...         return self.b(self.a(x))\n&gt;&gt;&gt; mod = Mod()\n&gt;&gt;&gt; pt_path = tempfile.mktemp(suffix=\".pt\")\n&gt;&gt;&gt; nnef_dir = tempfile.mkdtemp(suffix=\"_nnef\")\n&gt;&gt;&gt; torch.save(mod.state_dict(), pt_path)\n&gt;&gt;&gt; def check(ts):\n...     assert all(_.startswith(\"a.\") for _ in ts)\n&gt;&gt;&gt; exported_tensors = export_tensors_from_disk_to_nnef(\n...     pt_path,\n...     nnef_dir,\n...     lambda x: x.startswith(\"a.\"),\n...     check\n... )\n&gt;&gt;&gt; list(exported_tensors.keys())\n['a.weight', 'a.bias']\n</code></pre>"},{"location":"tutos/9_peft/#torch_to_nnef.export_tensors_to_nnef","title":"torch_to_nnef.export_tensors_to_nnef","text":"<pre><code>export_tensors_to_nnef(name_to_torch_tensors: T.Dict[str, _Tensor], output_dir: Path) -&gt; T.Dict[str, _Tensor]\n</code></pre> <p>Export any torch.Tensors list to NNEF .dat file.</p> <p>Parameters:</p> Name Type Description Default <code>name_to_torch_tensors</code> <code>Dict[str, _Tensor]</code> <p>dict A map of name (that will be used to define .dat filename) and tensor values (that can also be special torch_to_nnef tensors)</p> required <code>output_dir</code> <code>Path</code> <p>directory to dump the NNEF tensor .dat files</p> required <p>Returns:</p> Type Description <code>Dict[str, _Tensor]</code> <p>a dict of tensor name as key and torch.Tensor values, identical to <code>torch_to_nnef.export.export_tensors_to_nnef</code></p> <p>Examples:</p> <p>Simple example</p> <pre><code>&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; class Mod(nn.Module):\n...     def __init__(self):\n...         super().__init__()\n...         self.a = nn.Linear(1, 5)\n...         self.b = nn.Linear(5, 1)\n...\n...     def forward(self, x):\n...         return self.b(self.a(x))\n&gt;&gt;&gt; mod = Mod()\n&gt;&gt;&gt; nnef_dir = tempfile.mkdtemp(suffix=\"_nnef\")\n&gt;&gt;&gt; exported_tensors = export_tensors_to_nnef(\n...     {k: v for k, v in mod.named_parameters() if k.startswith(\"b.\")},\n...     nnef_dir,\n... )\n&gt;&gt;&gt; list(exported_tensors.keys())\n['b.weight', 'b.bias']\n</code></pre>"},{"location":"reference/torch_to_nnef/","title":"torch_to_nnef","text":""},{"location":"reference/torch_to_nnef/#torch_to_nnef","title":"torch_to_nnef","text":"<p>Top-level package for torch_to_nnef.</p>"},{"location":"reference/torch_to_nnef/#torch_to_nnef.KhronosNNEF","title":"KhronosNNEF","text":"<pre><code>KhronosNNEF(version: T.Union[SemanticVersion, str], check_io: bool = True)\n</code></pre> <p>               Bases: <code>InferenceTarget</code></p> <p>Khronos Specification compliant NNEF asset build.</p> <p>in case of check_io=True     we perform evaluation against nnef_tool nnef to pytorch converter.     And access original and reloaded pytorch model provide same outputs</p>"},{"location":"reference/torch_to_nnef/#torch_to_nnef.KhronosNNEF.post_export","title":"post_export","text":"<pre><code>post_export(model: nn.Module, nnef_graph: NGraph, args: T.List[T.Any], exported_filepath: Path, debug_bundle_path: T.Optional[Path] = None)\n</code></pre> <p>Check io via the Torch interpreter of NNEF-Tools.</p>"},{"location":"reference/torch_to_nnef/#torch_to_nnef.TractNNEF","title":"TractNNEF","text":"<pre><code>TractNNEF(version: T.Union[str, SemanticVersion], feature_flags: T.Optional[T.Set[TractFeatureFlag]] = None, check_io: bool = True, dynamic_axes: T.Optional[T.Dict[str, T.Dict[int, str]]] = None, specific_tract_binary_path: T.Optional[Path] = None, check_io_tolerance: TractCheckTolerance = TractCheckTolerance.APPROXIMATE, specific_properties: T.Optional[T.Dict[str, str]] = None, dump_identity_properties: bool = True, force_attention_inner_in_f32: bool = False, force_linear_accumulation_in_f32: bool = False, force_norm_in_f32: bool = False, reify_sdpa_operator: bool = False, upsample_with_debox: bool = False)\n</code></pre> <p>               Bases: <code>InferenceTarget</code></p> <p>Tract NNEF inference target.</p> <p>Init.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>Union[str, SemanticVersion]</code> <p>tract version targeted for export</p> required <code>feature_flags</code> <code>Optional[Set[TractFeatureFlag]]</code> <p>set of possibly added feature flags from tract (for example complex numbers)</p> <code>None</code> <code>check_io</code> <code>bool</code> <p>check between tract cli and Pytorch original model that given provided input, output is similar</p> <code>True</code> <code>dynamic_axes</code> <code>Optional[Dict[str, Dict[int, str]]]</code> <p>Optional specification of dynamic dimension By default the exported model will have the shapes of all input and output tensors set to exactly match those given in args. To specify axes of tensors as dynamic (i.e. known only at runtime) set dynamic_axes to a dict with schema:     KEY (str): an input or output name. Each name must also         be provided in input_names or output_names.     VALUE (dict or list): If a dict, keys are axis indices         and values are axis names. If a list, each element is         an axis index.</p> <code>None</code> <code>specific_tract_binary_path</code> <code>Optional[Path]</code> <p>filepath of tract cli in case of custom non released version of tract (for testing purpose)</p> <code>None</code> <code>check_io_tolerance</code> <code>TractCheckTolerance</code> <p>TractCheckTolerance level of difference tolerance between original output values and those generated by tract (those are defined tract levels)</p> <code>APPROXIMATE</code> <code>specific_properties</code> <code>Optional[Dict[str, str]]</code> <p>custom tract_properties you wish to add inside NNEF asset (will be parsed by tract as metadata)</p> <code>None</code> <code>dump_identity_properties</code> <code>bool</code> <p>add tract_properties relative to user identity (host, username, OS...), helpfull for debug</p> <code>True</code> <code>force_attention_inner_in_f32</code> <code>bool</code> <pre><code>control if attention should be forced as f32 inside\n(even if inputs are all f16), usefull for unstable networks\nlike qwen2.5\n</code></pre> <code>False</code> <code>force_linear_accumulation_in_f32</code> <code>bool</code> <p>usefull for f16 models to ensure that output of f16. f16 matmul become f32 accumulators.</p> <code>False</code> <code>force_norm_in_f32</code> <code>bool</code> <p>ensure that all normalization layers are in f32 whatever the original PyTorch modeling.</p> <code>False</code> <code>reify_sdpa_operator</code> <code>bool</code> <p>enable the conversion of scaled_dot_product_attention as a tract operator (intead of a NNEF fragment). Experimental feature.</p> <code>False</code> <code>upsample_with_debox</code> <code>bool</code> <p>use debox upsample operator instead of deconvolution. This should be faster. (if tract version support it). Experimental feature.</p> <code>False</code>"},{"location":"reference/torch_to_nnef/#torch_to_nnef.TractNNEF.post_export","title":"post_export","text":"<pre><code>post_export(model: nn.Module, nnef_graph: NGraph, args: T.List[T.Any], exported_filepath: Path, debug_bundle_path: T.Optional[Path] = None)\n</code></pre> <p>Perform check io and build debug bundle if fail.</p>"},{"location":"reference/torch_to_nnef/#torch_to_nnef.TractNNEF.post_trace","title":"post_trace","text":"<pre><code>post_trace(nnef_graph, active_custom_extensions)\n</code></pre> <p>Add dynamic axes in the NNEF graph.</p>"},{"location":"reference/torch_to_nnef/#torch_to_nnef.TractNNEF.pre_trace","title":"pre_trace","text":"<pre><code>pre_trace(model: nn.Module, input_names: T.Optional[T.List[str]], output_names: T.Optional[T.List[str]])\n</code></pre> <p>Check dynamic_axes are correctly formated.</p>"},{"location":"reference/torch_to_nnef/#torch_to_nnef.TractNNEF.specific_fragments","title":"specific_fragments","text":"<pre><code>specific_fragments(model: nn.Module) -&gt; T.Dict[str, str]\n</code></pre> <p>Optional custom fragments to pass.</p>"},{"location":"reference/torch_to_nnef/#torch_to_nnef.export_model_to_nnef","title":"export_model_to_nnef","text":"<pre><code>export_model_to_nnef(model: torch.nn.Module, args, file_path_export: T.Union[Path, str], inference_target: InferenceTarget, input_names: T.Optional[T.List[str]] = None, output_names: T.Optional[T.List[str]] = None, compression_level: int = 0, log_level: int = log.INFO, nnef_variable_naming_scheme: VariableNamingScheme = DEFAULT_VARNAME_SCHEME, check_io_names_qte_match: bool = True, debug_bundle_path: T.Optional[Path] = None, custom_extensions: T.Optional[T.List[str]] = None)\n</code></pre> <p>Main entrypoint of this library.</p> <p>Export any torch.nn.Module to NNEF file format archive</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>a nn.Module that have a <code>.forward</code> function with only tensor arguments and outputs (no tuple, list, dict or objects) Only this function will be serialized</p> required <code>args</code> <p>a flat ordered list of tensors for each forward inputs of <code>model</code> this list can not be of dynamic size (at serialization it will be fixed to quantity of tensor provided) WARNING! tensor size in args will increase export time so take that in consideration for dynamic axes</p> required <code>file_path_export</code> <code>Union[Path, str]</code> <p>a Path to the exported NNEF serialized model archive. It must by convention end with <code>.nnef.tgz</code> suffixes</p> required <code>inference_target</code> <code>InferenceTarget</code> <p>can be <code>torch_to_nnef.TractNNEF</code> or <code>torch_to_nnef.KhronosNNEF</code> for each you can specify version targeted: - KhronosNNEF is the least maintained so far,     and is checked against nnef-tools PyTorch interpreter - TractNNEF is our main focus at SONOS,   it is checked against tract inference engine   among key paramters there is     feature_flags: Optional[Set[str]],     that may contains tract specifics     dynamic_axes: Optional       By default the exported model will have       the shapes of all input and output tensors set       to exactly match those given in args.       To specify axes of tensors as dynamic       (i.e. known only at runtime)       set dynamic_axes to a dict with schema:           KEY (str):             an input or output name. Each name must also             be provided in input_names or output_names.           VALUE (dict or list): If a dict, keys are axis indices             and values are axis names. If a list, each element is             an axis index.</p> required <code>specific_tract_binary_path</code> <p>Optional[Path] ideal to check io against new tract versions</p> required <code>input_names</code> <code>Optional[List[str]]</code> <p>Optional list of names for args, it replaces variable inputs names traced from graph (if set it must have the same size as number of args)</p> <code>None</code> <code>output_names</code> <code>Optional[List[str]]</code> <p>Optional list of names for outputs of <code>model.forward</code>, it replaces variable output names traced from graph (if set it must have the same size as number of outputs)</p> <code>None</code> <code>compression_level</code> <code>int</code> <p>int (&gt;= 0) compression level of tar.gz (higher is more compressed)</p> <code>0</code> <code>log_level</code> <code>int</code> <p>int, logger level for <code>torch_to_nnef</code> following Python standard logging level can be set to: INFO, WARN, DEBUG ...</p> <code>INFO</code> <code>nnef_variable_naming_scheme</code> <code>VariableNamingScheme</code> <p>Possible choices NNEF variables naming schemes are: - \"raw\": Taking variable names from traced graph debugName directly - \"natural_verbose\": that try to provide nn.Module exported   variable naming consistency - \"natural_verbose_camel\": that try to provide nn.Module exported   variable naming consistency but with more consice camelCase   variable pattern - \"numeric\": that try to be as concise as possible</p> <code>DEFAULT_VARNAME_SCHEME</code> <code>check_io_names_qte_match</code> <code>bool</code> <p>(default: True) During the tracing process of the torch graph One or more input provided can be removed if not contributing to generate outputs while check_io_names_qte_match is True we ensure that this input and output quantity remain constant with numbers in <code>input_names</code> and <code>output_names</code>.</p> <code>True</code> <code>debug_bundle_path</code> <code>Optional[Path]</code> <p>Optional[Path] if specified it should create an archive bundle with all needed information to allow easier debug.</p> <code>None</code> <code>custom_extensions</code> <code>Optional[List[str]]</code> <p>Optional[List[str]] allow to add a set of extensions as defined in (https://registry.khronos.org/NNEF/specs/1.0/nnef-1.0.5.html) Useful to set specific extensions like for example: 'extension tract_assert S &gt;= 0' those assertion allows to add limitation on dynamic shapes that are not expressed in traced graph (like for example maximum number of tokens for an LLM)</p> <code>None</code> <p>Examples:</p> <p>For example this function can be used to export as simple perceptron model:</p> <pre><code>&gt;&gt;&gt; import os\n&gt;&gt;&gt; import tarfile\n&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; mod = nn.Sequential(nn.Linear(1, 5), nn.ReLU())\n&gt;&gt;&gt; export_path = tempfile.mktemp(suffix=\".nnef.tgz\")\n&gt;&gt;&gt; inference_target = TractNNEF.latest()\n&gt;&gt;&gt; export_model_to_nnef(\n...   mod,\n...   torch.rand(3, 1),\n...   export_path,\n...   inference_target,\n...   input_names=[\"inp\"],\n...   output_names=[\"out\"]\n... )\n&gt;&gt;&gt; os.chdir(export_path.rsplit(\"/\", maxsplit=1)[0])\n&gt;&gt;&gt; tarfile.open(export_path).extract(\"graph.nnef\")\n&gt;&gt;&gt; \"graph network(inp) -&gt; (out)\" in open(\"graph.nnef\").read()\nTrue\n</code></pre>"},{"location":"reference/torch_to_nnef/#torch_to_nnef.export_tensors_from_disk_to_nnef","title":"export_tensors_from_disk_to_nnef","text":"<pre><code>export_tensors_from_disk_to_nnef(store_filepath: T.Union[Path, str], output_dir: T.Union[Path, str], filter_key: T.Optional[T.Callable[[str], bool]] = None, fn_check_found_tensors: T.Optional[T.Callable[[T.Dict[str, _Tensor]], bool]] = None) -&gt; T.Dict[str, _Tensor]\n</code></pre> <p>Export any statedict or safetensors file torch.Tensors to NNEF .dat file.</p> <p>Parameters:</p> Name Type Description Default <code>store_filepath</code> <code>Union[Path, str]</code> <p>the filepath that hold the .safetensors , .pt or .bin containing the state dict</p> required <code>output_dir</code> <code>Union[Path, str]</code> <p>directory to dump the NNEF tensor .dat files</p> required <code>filter_key</code> <code>Optional[Callable[[str], bool]]</code> <p>An optional function to filter specific keys to be exported</p> <code>None</code> <code>fn_check_found_tensors</code> <code>Optional[Callable[[Dict[str, _Tensor]], bool]]</code> <p>post checking function to ensure all requested tensors have effectively been dumped</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, _Tensor]</code> <p>a dict of tensor name as key and torch.Tensor values, identical to <code>torch_to_nnef.export.export_tensors_to_nnef</code></p> <p>Examples:</p> <p>Simple filtered example</p> <pre><code>&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; class Mod(nn.Module):\n...     def __init__(self):\n...         super().__init__()\n...         self.a = nn.Linear(1, 5)\n...         self.b = nn.Linear(5, 1)\n...\n...     def forward(self, x):\n...         return self.b(self.a(x))\n&gt;&gt;&gt; mod = Mod()\n&gt;&gt;&gt; pt_path = tempfile.mktemp(suffix=\".pt\")\n&gt;&gt;&gt; nnef_dir = tempfile.mkdtemp(suffix=\"_nnef\")\n&gt;&gt;&gt; torch.save(mod.state_dict(), pt_path)\n&gt;&gt;&gt; def check(ts):\n...     assert all(_.startswith(\"a.\") for _ in ts)\n&gt;&gt;&gt; exported_tensors = export_tensors_from_disk_to_nnef(\n...     pt_path,\n...     nnef_dir,\n...     lambda x: x.startswith(\"a.\"),\n...     check\n... )\n&gt;&gt;&gt; list(exported_tensors.keys())\n['a.weight', 'a.bias']\n</code></pre>"},{"location":"reference/torch_to_nnef/#torch_to_nnef.export_tensors_to_nnef","title":"export_tensors_to_nnef","text":"<pre><code>export_tensors_to_nnef(name_to_torch_tensors: T.Dict[str, _Tensor], output_dir: Path) -&gt; T.Dict[str, _Tensor]\n</code></pre> <p>Export any torch.Tensors list to NNEF .dat file.</p> <p>Parameters:</p> Name Type Description Default <code>name_to_torch_tensors</code> <code>Dict[str, _Tensor]</code> <p>dict A map of name (that will be used to define .dat filename) and tensor values (that can also be special torch_to_nnef tensors)</p> required <code>output_dir</code> <code>Path</code> <p>directory to dump the NNEF tensor .dat files</p> required <p>Returns:</p> Type Description <code>Dict[str, _Tensor]</code> <p>a dict of tensor name as key and torch.Tensor values, identical to <code>torch_to_nnef.export.export_tensors_to_nnef</code></p> <p>Examples:</p> <p>Simple example</p> <pre><code>&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; class Mod(nn.Module):\n...     def __init__(self):\n...         super().__init__()\n...         self.a = nn.Linear(1, 5)\n...         self.b = nn.Linear(5, 1)\n...\n...     def forward(self, x):\n...         return self.b(self.a(x))\n&gt;&gt;&gt; mod = Mod()\n&gt;&gt;&gt; nnef_dir = tempfile.mkdtemp(suffix=\"_nnef\")\n&gt;&gt;&gt; exported_tensors = export_tensors_to_nnef(\n...     {k: v for k, v in mod.named_parameters() if k.startswith(\"b.\")},\n...     nnef_dir,\n... )\n&gt;&gt;&gt; list(exported_tensors.keys())\n['b.weight', 'b.bias']\n</code></pre>"},{"location":"reference/torch_to_nnef/collect_env/","title":"torch_to_nnef.collect_env","text":""},{"location":"reference/torch_to_nnef/collect_env/#torch_to_nnef.collect_env","title":"torch_to_nnef.collect_env","text":"<p>Used to collect environment status/versions for debuging purpose.</p>"},{"location":"reference/torch_to_nnef/collect_env/#torch_to_nnef.collect_env.check_release_file","title":"check_release_file","text":"<pre><code>check_release_file()\n</code></pre> <p>Read /etc/*-release file to get a pretty name for linux distros.</p>"},{"location":"reference/torch_to_nnef/collect_env/#torch_to_nnef.collect_env.dump_environment_versions","title":"dump_environment_versions","text":"<pre><code>dump_environment_versions(pathdir: Path, tract_path: Path)\n</code></pre> <p>Dumps software versions to a file 'versions' in the given folder.</p>"},{"location":"reference/torch_to_nnef/collect_env/#torch_to_nnef.collect_env.get_gcc_version","title":"get_gcc_version","text":"<pre><code>get_gcc_version()\n</code></pre> <p>Returns the GCC version string, or None if gcc is not found.</p>"},{"location":"reference/torch_to_nnef/collect_env/#torch_to_nnef.collect_env.get_hostname","title":"get_hostname","text":"<pre><code>get_hostname()\n</code></pre> <p>Returns the system hostname.</p>"},{"location":"reference/torch_to_nnef/collect_env/#torch_to_nnef.collect_env.get_lsb_version","title":"get_lsb_version","text":"<pre><code>get_lsb_version()\n</code></pre> <p>Returns lsb_release Description output like 'Ubuntu 20.04.6 LTS'.</p>"},{"location":"reference/torch_to_nnef/collect_env/#torch_to_nnef.collect_env.get_mac_version","title":"get_mac_version","text":"<pre><code>get_mac_version()\n</code></pre> <p>Returns macOS version like '10.14.6'.</p>"},{"location":"reference/torch_to_nnef/collect_env/#torch_to_nnef.collect_env.get_os","title":"get_os","text":"<pre><code>get_os() -&gt; str\n</code></pre> <p>Returns a pretty string describing the OS.</p>"},{"location":"reference/torch_to_nnef/collect_env/#torch_to_nnef.collect_env.get_pip_packages","title":"get_pip_packages","text":"<pre><code>get_pip_packages()\n</code></pre> <p>Returns <code>pip list</code> output.</p> <p>Note: will also find conda-installed pytorch and numpy packages.</p>"},{"location":"reference/torch_to_nnef/collect_env/#torch_to_nnef.collect_env.get_platform","title":"get_platform","text":"<pre><code>get_platform() -&gt; str\n</code></pre> <p>Returns a simplified platform name.</p>"},{"location":"reference/torch_to_nnef/collect_env/#torch_to_nnef.collect_env.get_uname","title":"get_uname","text":"<pre><code>get_uname() -&gt; str\n</code></pre> <p>Returns the output of <code>uname -a</code>.</p>"},{"location":"reference/torch_to_nnef/collect_env/#torch_to_nnef.collect_env.get_user","title":"get_user","text":"<pre><code>get_user()\n</code></pre> <p>OS current user name, or empty string if it cannot be determined.</p>"},{"location":"reference/torch_to_nnef/collect_env/#torch_to_nnef.collect_env.get_windows_version","title":"get_windows_version","text":"<pre><code>get_windows_version()\n</code></pre> <p>Returns Windows version like 'Microsoft Windows 10 Pro'.</p>"},{"location":"reference/torch_to_nnef/collect_env/#torch_to_nnef.collect_env.python_version","title":"python_version","text":"<pre><code>python_version() -&gt; str\n</code></pre> <p>Returns a one-liner with python version and bitness.</p>"},{"location":"reference/torch_to_nnef/collect_env/#torch_to_nnef.collect_env.run_and_parse_first_match","title":"run_and_parse_first_match","text":"<pre><code>run_and_parse_first_match(command, regex)\n</code></pre> <p>Runs command, returns the first regex match if it exists.</p>"},{"location":"reference/torch_to_nnef/collect_env/#torch_to_nnef.collect_env.run_and_read_all","title":"run_and_read_all","text":"<pre><code>run_and_read_all(command)\n</code></pre> <p>Runs command; reads and returns entire output if rc is 0.</p>"},{"location":"reference/torch_to_nnef/collect_env/#torch_to_nnef.collect_env.run_lambda","title":"run_lambda","text":"<pre><code>run_lambda(command)\n</code></pre> <p>Returns (return-code, stdout, stderr).</p> <p>And Strips trailing newlines.</p>"},{"location":"reference/torch_to_nnef/compress/","title":"torch_to_nnef.compress","text":""},{"location":"reference/torch_to_nnef/compress/#torch_to_nnef.compress","title":"torch_to_nnef.compress","text":"<p>Compression module mostly used as demonstration purpose.</p> <p>Examplify: how to implement quantization</p>"},{"location":"reference/torch_to_nnef/compress/#torch_to_nnef.compress.dynamic_load_registry","title":"dynamic_load_registry","text":"<pre><code>dynamic_load_registry(compression_registry_full_path: str)\n</code></pre> <p>Load a registry dynamically based on it's module path + dict name.</p>"},{"location":"reference/torch_to_nnef/compress/#torch_to_nnef.compress.offloaded_tensor_qtensor","title":"offloaded_tensor_qtensor","text":"<pre><code>offloaded_tensor_qtensor(q_fn, tensor: torch.Tensor, suffix_name: str) -&gt; torch.Tensor\n</code></pre> <p>Maintains a QTensor offloaded if original tensor is offloaded.</p>"},{"location":"reference/torch_to_nnef/compress/#torch_to_nnef.compress.quantize_weights_min_max_Q4_0","title":"quantize_weights_min_max_Q4_0","text":"<pre><code>quantize_weights_min_max_Q4_0(model: nn.Module, **kwargs)\n</code></pre> <p>Example of quantization function for a model to Q40.</p>"},{"location":"reference/torch_to_nnef/console/","title":"torch_to_nnef.console","text":""},{"location":"reference/torch_to_nnef/console/#torch_to_nnef.console","title":"torch_to_nnef.console","text":"<p>Helper to display in console nicely.</p>"},{"location":"reference/torch_to_nnef/console/#torch_to_nnef.console.Console","title":"Console","text":"<pre><code>Console(theme: T.Optional[T.Dict[str, str]])\n</code></pre> <p>Inspired by rich.console without the deps on rich.</p> <p>Only support linux/Mac TTY</p> <p>Initialize the Console with an optional theme.</p> <p>This constructor sets up the underlying printer. If the <code>rich</code> library is available, it will be used; otherwise a simple fallback printing routine is installed.</p> <p>Parameters:</p> Name Type Description Default <code>theme</code> <code>Optional[Dict[str, str]]</code> <p>Optional mapping of color names to ANSI codes for custom styling. If <code>None</code> the default <code>rich</code> theme is used.</p> required"},{"location":"reference/torch_to_nnef/dtypes/","title":"torch_to_nnef.dtypes","text":""},{"location":"reference/torch_to_nnef/dtypes/#torch_to_nnef.dtypes","title":"torch_to_nnef.dtypes","text":"<p>Module referencing all data types conversions between libraries.</p> <p>List of libraries being:PyTorch, Numpy, tract</p>"},{"location":"reference/torch_to_nnef/exceptions/","title":"torch_to_nnef.exceptions","text":""},{"location":"reference/torch_to_nnef/exceptions/#torch_to_nnef.exceptions","title":"torch_to_nnef.exceptions","text":"<p>Regroup all Exceptions that can happen in torch_to_nnef.</p> rational <p>Can catch all package related error with an except T2NError same apply for tract errors with except T2NErrorTract</p> <p>This library is embedded in others codebase: Errors should be specific to be catchable so no builtin Exception should be raised</p>"},{"location":"reference/torch_to_nnef/exceptions/#torch_to_nnef.exceptions.T2NError","title":"T2NError","text":"<p>               Bases: <code>Exception</code></p> <p>Generic error that all errors in this lib inherit.</p>"},{"location":"reference/torch_to_nnef/exceptions/#torch_to_nnef.exceptions.T2NErrorImport","title":"T2NErrorImport","text":"<p>               Bases: <code>T2NError</code></p> <p>Import error that all errors in this lib inherit.</p>"},{"location":"reference/torch_to_nnef/exceptions/#torch_to_nnef.exceptions.T2NErrorInvalidArgument","title":"T2NErrorInvalidArgument","text":"<p>               Bases: <code>ValueError</code>, <code>T2NError</code></p> <p>specification of torch_to_nnef export not respected.</p>"},{"location":"reference/torch_to_nnef/exceptions/#torch_to_nnef.exceptions.T2NErrorNotFoundFile","title":"T2NErrorNotFoundFile","text":"<p>               Bases: <code>ValueError</code>, <code>T2NError</code></p> <p>missing exit for file path.</p>"},{"location":"reference/torch_to_nnef/export/","title":"torch_to_nnef.export","text":""},{"location":"reference/torch_to_nnef/export/#torch_to_nnef.export","title":"torch_to_nnef.export","text":""},{"location":"reference/torch_to_nnef/export/#torch_to_nnef.export.export_model_to_nnef","title":"export_model_to_nnef","text":"<pre><code>export_model_to_nnef(model: torch.nn.Module, args, file_path_export: T.Union[Path, str], inference_target: InferenceTarget, input_names: T.Optional[T.List[str]] = None, output_names: T.Optional[T.List[str]] = None, compression_level: int = 0, log_level: int = log.INFO, nnef_variable_naming_scheme: VariableNamingScheme = DEFAULT_VARNAME_SCHEME, check_io_names_qte_match: bool = True, debug_bundle_path: T.Optional[Path] = None, custom_extensions: T.Optional[T.List[str]] = None)\n</code></pre> <p>Main entrypoint of this library.</p> <p>Export any torch.nn.Module to NNEF file format archive</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>a nn.Module that have a <code>.forward</code> function with only tensor arguments and outputs (no tuple, list, dict or objects) Only this function will be serialized</p> required <code>args</code> <p>a flat ordered list of tensors for each forward inputs of <code>model</code> this list can not be of dynamic size (at serialization it will be fixed to quantity of tensor provided) WARNING! tensor size in args will increase export time so take that in consideration for dynamic axes</p> required <code>file_path_export</code> <code>Union[Path, str]</code> <p>a Path to the exported NNEF serialized model archive. It must by convention end with <code>.nnef.tgz</code> suffixes</p> required <code>inference_target</code> <code>InferenceTarget</code> <p>can be <code>torch_to_nnef.TractNNEF</code> or <code>torch_to_nnef.KhronosNNEF</code> for each you can specify version targeted: - KhronosNNEF is the least maintained so far,     and is checked against nnef-tools PyTorch interpreter - TractNNEF is our main focus at SONOS,   it is checked against tract inference engine   among key paramters there is     feature_flags: Optional[Set[str]],     that may contains tract specifics     dynamic_axes: Optional       By default the exported model will have       the shapes of all input and output tensors set       to exactly match those given in args.       To specify axes of tensors as dynamic       (i.e. known only at runtime)       set dynamic_axes to a dict with schema:           KEY (str):             an input or output name. Each name must also             be provided in input_names or output_names.           VALUE (dict or list): If a dict, keys are axis indices             and values are axis names. If a list, each element is             an axis index.</p> required <code>specific_tract_binary_path</code> <p>Optional[Path] ideal to check io against new tract versions</p> required <code>input_names</code> <code>Optional[List[str]]</code> <p>Optional list of names for args, it replaces variable inputs names traced from graph (if set it must have the same size as number of args)</p> <code>None</code> <code>output_names</code> <code>Optional[List[str]]</code> <p>Optional list of names for outputs of <code>model.forward</code>, it replaces variable output names traced from graph (if set it must have the same size as number of outputs)</p> <code>None</code> <code>compression_level</code> <code>int</code> <p>int (&gt;= 0) compression level of tar.gz (higher is more compressed)</p> <code>0</code> <code>log_level</code> <code>int</code> <p>int, logger level for <code>torch_to_nnef</code> following Python standard logging level can be set to: INFO, WARN, DEBUG ...</p> <code>INFO</code> <code>nnef_variable_naming_scheme</code> <code>VariableNamingScheme</code> <p>Possible choices NNEF variables naming schemes are: - \"raw\": Taking variable names from traced graph debugName directly - \"natural_verbose\": that try to provide nn.Module exported   variable naming consistency - \"natural_verbose_camel\": that try to provide nn.Module exported   variable naming consistency but with more consice camelCase   variable pattern - \"numeric\": that try to be as concise as possible</p> <code>DEFAULT_VARNAME_SCHEME</code> <code>check_io_names_qte_match</code> <code>bool</code> <p>(default: True) During the tracing process of the torch graph One or more input provided can be removed if not contributing to generate outputs while check_io_names_qte_match is True we ensure that this input and output quantity remain constant with numbers in <code>input_names</code> and <code>output_names</code>.</p> <code>True</code> <code>debug_bundle_path</code> <code>Optional[Path]</code> <p>Optional[Path] if specified it should create an archive bundle with all needed information to allow easier debug.</p> <code>None</code> <code>custom_extensions</code> <code>Optional[List[str]]</code> <p>Optional[List[str]] allow to add a set of extensions as defined in (https://registry.khronos.org/NNEF/specs/1.0/nnef-1.0.5.html) Useful to set specific extensions like for example: 'extension tract_assert S &gt;= 0' those assertion allows to add limitation on dynamic shapes that are not expressed in traced graph (like for example maximum number of tokens for an LLM)</p> <code>None</code> <p>Examples:</p> <p>For example this function can be used to export as simple perceptron model:</p> <pre><code>&gt;&gt;&gt; import os\n&gt;&gt;&gt; import tarfile\n&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; mod = nn.Sequential(nn.Linear(1, 5), nn.ReLU())\n&gt;&gt;&gt; export_path = tempfile.mktemp(suffix=\".nnef.tgz\")\n&gt;&gt;&gt; inference_target = TractNNEF.latest()\n&gt;&gt;&gt; export_model_to_nnef(\n...   mod,\n...   torch.rand(3, 1),\n...   export_path,\n...   inference_target,\n...   input_names=[\"inp\"],\n...   output_names=[\"out\"]\n... )\n&gt;&gt;&gt; os.chdir(export_path.rsplit(\"/\", maxsplit=1)[0])\n&gt;&gt;&gt; tarfile.open(export_path).extract(\"graph.nnef\")\n&gt;&gt;&gt; \"graph network(inp) -&gt; (out)\" in open(\"graph.nnef\").read()\nTrue\n</code></pre>"},{"location":"reference/torch_to_nnef/export/#torch_to_nnef.export.export_tensors_from_disk_to_nnef","title":"export_tensors_from_disk_to_nnef","text":"<pre><code>export_tensors_from_disk_to_nnef(store_filepath: T.Union[Path, str], output_dir: T.Union[Path, str], filter_key: T.Optional[T.Callable[[str], bool]] = None, fn_check_found_tensors: T.Optional[T.Callable[[T.Dict[str, _Tensor]], bool]] = None) -&gt; T.Dict[str, _Tensor]\n</code></pre> <p>Export any statedict or safetensors file torch.Tensors to NNEF .dat file.</p> <p>Parameters:</p> Name Type Description Default <code>store_filepath</code> <code>Union[Path, str]</code> <p>the filepath that hold the .safetensors , .pt or .bin containing the state dict</p> required <code>output_dir</code> <code>Union[Path, str]</code> <p>directory to dump the NNEF tensor .dat files</p> required <code>filter_key</code> <code>Optional[Callable[[str], bool]]</code> <p>An optional function to filter specific keys to be exported</p> <code>None</code> <code>fn_check_found_tensors</code> <code>Optional[Callable[[Dict[str, _Tensor]], bool]]</code> <p>post checking function to ensure all requested tensors have effectively been dumped</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, _Tensor]</code> <p>a dict of tensor name as key and torch.Tensor values, identical to <code>torch_to_nnef.export.export_tensors_to_nnef</code></p> <p>Examples:</p> <p>Simple filtered example</p> <pre><code>&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; class Mod(nn.Module):\n...     def __init__(self):\n...         super().__init__()\n...         self.a = nn.Linear(1, 5)\n...         self.b = nn.Linear(5, 1)\n...\n...     def forward(self, x):\n...         return self.b(self.a(x))\n&gt;&gt;&gt; mod = Mod()\n&gt;&gt;&gt; pt_path = tempfile.mktemp(suffix=\".pt\")\n&gt;&gt;&gt; nnef_dir = tempfile.mkdtemp(suffix=\"_nnef\")\n&gt;&gt;&gt; torch.save(mod.state_dict(), pt_path)\n&gt;&gt;&gt; def check(ts):\n...     assert all(_.startswith(\"a.\") for _ in ts)\n&gt;&gt;&gt; exported_tensors = export_tensors_from_disk_to_nnef(\n...     pt_path,\n...     nnef_dir,\n...     lambda x: x.startswith(\"a.\"),\n...     check\n... )\n&gt;&gt;&gt; list(exported_tensors.keys())\n['a.weight', 'a.bias']\n</code></pre>"},{"location":"reference/torch_to_nnef/export/#torch_to_nnef.export.export_tensors_to_nnef","title":"export_tensors_to_nnef","text":"<pre><code>export_tensors_to_nnef(name_to_torch_tensors: T.Dict[str, _Tensor], output_dir: Path) -&gt; T.Dict[str, _Tensor]\n</code></pre> <p>Export any torch.Tensors list to NNEF .dat file.</p> <p>Parameters:</p> Name Type Description Default <code>name_to_torch_tensors</code> <code>Dict[str, _Tensor]</code> <p>dict A map of name (that will be used to define .dat filename) and tensor values (that can also be special torch_to_nnef tensors)</p> required <code>output_dir</code> <code>Path</code> <p>directory to dump the NNEF tensor .dat files</p> required <p>Returns:</p> Type Description <code>Dict[str, _Tensor]</code> <p>a dict of tensor name as key and torch.Tensor values, identical to <code>torch_to_nnef.export.export_tensors_to_nnef</code></p> <p>Examples:</p> <p>Simple example</p> <pre><code>&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; class Mod(nn.Module):\n...     def __init__(self):\n...         super().__init__()\n...         self.a = nn.Linear(1, 5)\n...         self.b = nn.Linear(5, 1)\n...\n...     def forward(self, x):\n...         return self.b(self.a(x))\n&gt;&gt;&gt; mod = Mod()\n&gt;&gt;&gt; nnef_dir = tempfile.mkdtemp(suffix=\"_nnef\")\n&gt;&gt;&gt; exported_tensors = export_tensors_to_nnef(\n...     {k: v for k, v in mod.named_parameters() if k.startswith(\"b.\")},\n...     nnef_dir,\n... )\n&gt;&gt;&gt; list(exported_tensors.keys())\n['b.weight', 'b.bias']\n</code></pre>"},{"location":"reference/torch_to_nnef/export/#torch_to_nnef.export.iter_torch_tensors_from_disk","title":"iter_torch_tensors_from_disk","text":"<pre><code>iter_torch_tensors_from_disk(store_filepath: Path, filter_key: T.Optional[T.Callable[[str], bool]] = None) -&gt; T.Iterator[T.Tuple[str, _Tensor]]\n</code></pre> <p>Iter on torch tensors from disk .safetensors, .pt, pth, .bin.</p> <p>Parameters:</p> Name Type Description Default <code>store_filepath</code> <code>Path</code> <p>path to the container file holding PyTorch tensors (.pt, .pth, .bin and .safetensors)</p> required <code>filter_key</code> <code>Optional[Callable[[str], bool]]</code> <p>if set, this function filter over tensor by name stored in those format</p> <code>None</code> <p>Yields:</p> Type Description <code>str</code> <p>provide each tensor that are validated by filter within store filepath</p> <code>_Tensor</code> <p>one at a time as tuple with name first then the torch.Tensor itself</p>"},{"location":"reference/torch_to_nnef/inference_target/","title":"torch_to_nnef.inference_target","text":""},{"location":"reference/torch_to_nnef/inference_target/#torch_to_nnef.inference_target","title":"torch_to_nnef.inference_target","text":"<p>Targeted inference engine.</p> <p>We mainly focus our effort to best support SONOS 'tract' inference engine.</p> <p>Stricter Khronos NNEF specification mode also exist but is less extensively tested.</p>"},{"location":"reference/torch_to_nnef/inference_target/#torch_to_nnef.inference_target.InferenceTarget","title":"InferenceTarget","text":"<pre><code>InferenceTarget(version: T.Union[SemanticVersion, str], check_io: bool = False)\n</code></pre> <p>Base abstract class to implement a new inference engine target.</p> <p>Init InferenceTarget.</p> <p>Each inference engine is supposed to have at least a version and a way to check output given an input.</p>"},{"location":"reference/torch_to_nnef/inference_target/#torch_to_nnef.inference_target.InferenceTarget.has_dynamic_axes","title":"has_dynamic_axes  <code>property</code>","text":"<pre><code>has_dynamic_axes: bool\n</code></pre> <p>Define if user request dynamic axes to be in the NNEF graph.</p> <p>Some inference engines may not support it hence False by default.</p>"},{"location":"reference/torch_to_nnef/inference_target/#torch_to_nnef.inference_target.InferenceTarget.post_export","title":"post_export","text":"<pre><code>post_export(model: nn.Module, nnef_graph: NGraph, args: T.List[T.Any], exported_filepath: Path, debug_bundle_path: T.Optional[Path] = None)\n</code></pre> <p>Get called after NNEF model asset is generated.</p> <p>This is typically where check_io is effectively applied.</p>"},{"location":"reference/torch_to_nnef/inference_target/#torch_to_nnef.inference_target.InferenceTarget.post_trace","title":"post_trace","text":"<pre><code>post_trace(nnef_graph: NGraph, active_custom_extensions: T.List[str])\n</code></pre> <p>Get called just after PyTorch graph is parsed.</p>"},{"location":"reference/torch_to_nnef/inference_target/#torch_to_nnef.inference_target.InferenceTarget.pre_trace","title":"pre_trace","text":"<pre><code>pre_trace(model: nn.Module, input_names: T.Optional[T.List[str]], output_names: T.Optional[T.List[str]])\n</code></pre> <p>Get called just before PyTorch graph is traced.</p> <p>(after auto wrapper)</p>"},{"location":"reference/torch_to_nnef/inference_target/#torch_to_nnef.inference_target.InferenceTarget.specific_fragments","title":"specific_fragments","text":"<pre><code>specific_fragments(model: nn.Module) -&gt; T.Dict[str, str]\n</code></pre> <p>Optional custom fragments to pass.</p>"},{"location":"reference/torch_to_nnef/inference_target/#torch_to_nnef.inference_target.KhronosNNEF","title":"KhronosNNEF","text":"<pre><code>KhronosNNEF(version: T.Union[SemanticVersion, str], check_io: bool = True)\n</code></pre> <p>               Bases: <code>InferenceTarget</code></p> <p>Khronos Specification compliant NNEF asset build.</p> <p>in case of check_io=True     we perform evaluation against nnef_tool nnef to pytorch converter.     And access original and reloaded pytorch model provide same outputs</p>"},{"location":"reference/torch_to_nnef/inference_target/#torch_to_nnef.inference_target.KhronosNNEF.post_export","title":"post_export","text":"<pre><code>post_export(model: nn.Module, nnef_graph: NGraph, args: T.List[T.Any], exported_filepath: Path, debug_bundle_path: T.Optional[Path] = None)\n</code></pre> <p>Check io via the Torch interpreter of NNEF-Tools.</p>"},{"location":"reference/torch_to_nnef/inference_target/#torch_to_nnef.inference_target.TractNNEF","title":"TractNNEF","text":"<pre><code>TractNNEF(version: T.Union[str, SemanticVersion], feature_flags: T.Optional[T.Set[TractFeatureFlag]] = None, check_io: bool = True, dynamic_axes: T.Optional[T.Dict[str, T.Dict[int, str]]] = None, specific_tract_binary_path: T.Optional[Path] = None, check_io_tolerance: TractCheckTolerance = TractCheckTolerance.APPROXIMATE, specific_properties: T.Optional[T.Dict[str, str]] = None, dump_identity_properties: bool = True, force_attention_inner_in_f32: bool = False, force_linear_accumulation_in_f32: bool = False, force_norm_in_f32: bool = False, reify_sdpa_operator: bool = False, upsample_with_debox: bool = False)\n</code></pre> <p>               Bases: <code>InferenceTarget</code></p> <p>Tract NNEF inference target.</p> <p>Init.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>Union[str, SemanticVersion]</code> <p>tract version targeted for export</p> required <code>feature_flags</code> <code>Optional[Set[TractFeatureFlag]]</code> <p>set of possibly added feature flags from tract (for example complex numbers)</p> <code>None</code> <code>check_io</code> <code>bool</code> <p>check between tract cli and Pytorch original model that given provided input, output is similar</p> <code>True</code> <code>dynamic_axes</code> <code>Optional[Dict[str, Dict[int, str]]]</code> <p>Optional specification of dynamic dimension By default the exported model will have the shapes of all input and output tensors set to exactly match those given in args. To specify axes of tensors as dynamic (i.e. known only at runtime) set dynamic_axes to a dict with schema:     KEY (str): an input or output name. Each name must also         be provided in input_names or output_names.     VALUE (dict or list): If a dict, keys are axis indices         and values are axis names. If a list, each element is         an axis index.</p> <code>None</code> <code>specific_tract_binary_path</code> <code>Optional[Path]</code> <p>filepath of tract cli in case of custom non released version of tract (for testing purpose)</p> <code>None</code> <code>check_io_tolerance</code> <code>TractCheckTolerance</code> <p>TractCheckTolerance level of difference tolerance between original output values and those generated by tract (those are defined tract levels)</p> <code>APPROXIMATE</code> <code>specific_properties</code> <code>Optional[Dict[str, str]]</code> <p>custom tract_properties you wish to add inside NNEF asset (will be parsed by tract as metadata)</p> <code>None</code> <code>dump_identity_properties</code> <code>bool</code> <p>add tract_properties relative to user identity (host, username, OS...), helpfull for debug</p> <code>True</code> <code>force_attention_inner_in_f32</code> <code>bool</code> <pre><code>control if attention should be forced as f32 inside\n(even if inputs are all f16), usefull for unstable networks\nlike qwen2.5\n</code></pre> <code>False</code> <code>force_linear_accumulation_in_f32</code> <code>bool</code> <p>usefull for f16 models to ensure that output of f16. f16 matmul become f32 accumulators.</p> <code>False</code> <code>force_norm_in_f32</code> <code>bool</code> <p>ensure that all normalization layers are in f32 whatever the original PyTorch modeling.</p> <code>False</code> <code>reify_sdpa_operator</code> <code>bool</code> <p>enable the conversion of scaled_dot_product_attention as a tract operator (intead of a NNEF fragment). Experimental feature.</p> <code>False</code> <code>upsample_with_debox</code> <code>bool</code> <p>use debox upsample operator instead of deconvolution. This should be faster. (if tract version support it). Experimental feature.</p> <code>False</code>"},{"location":"reference/torch_to_nnef/inference_target/#torch_to_nnef.inference_target.TractNNEF.post_export","title":"post_export","text":"<pre><code>post_export(model: nn.Module, nnef_graph: NGraph, args: T.List[T.Any], exported_filepath: Path, debug_bundle_path: T.Optional[Path] = None)\n</code></pre> <p>Perform check io and build debug bundle if fail.</p>"},{"location":"reference/torch_to_nnef/inference_target/#torch_to_nnef.inference_target.TractNNEF.post_trace","title":"post_trace","text":"<pre><code>post_trace(nnef_graph, active_custom_extensions)\n</code></pre> <p>Add dynamic axes in the NNEF graph.</p>"},{"location":"reference/torch_to_nnef/inference_target/#torch_to_nnef.inference_target.TractNNEF.pre_trace","title":"pre_trace","text":"<pre><code>pre_trace(model: nn.Module, input_names: T.Optional[T.List[str]], output_names: T.Optional[T.List[str]])\n</code></pre> <p>Check dynamic_axes are correctly formated.</p>"},{"location":"reference/torch_to_nnef/inference_target/#torch_to_nnef.inference_target.TractNNEF.specific_fragments","title":"specific_fragments","text":"<pre><code>specific_fragments(model: nn.Module) -&gt; T.Dict[str, str]\n</code></pre> <p>Optional custom fragments to pass.</p>"},{"location":"reference/torch_to_nnef/inference_target/base/","title":"torch_to_nnef.inference_target.base","text":""},{"location":"reference/torch_to_nnef/inference_target/base/#torch_to_nnef.inference_target.base","title":"torch_to_nnef.inference_target.base","text":""},{"location":"reference/torch_to_nnef/inference_target/base/#torch_to_nnef.inference_target.base.InferenceTarget","title":"InferenceTarget","text":"<pre><code>InferenceTarget(version: T.Union[SemanticVersion, str], check_io: bool = False)\n</code></pre> <p>Base abstract class to implement a new inference engine target.</p> <p>Init InferenceTarget.</p> <p>Each inference engine is supposed to have at least a version and a way to check output given an input.</p>"},{"location":"reference/torch_to_nnef/inference_target/base/#torch_to_nnef.inference_target.base.InferenceTarget.has_dynamic_axes","title":"has_dynamic_axes  <code>property</code>","text":"<pre><code>has_dynamic_axes: bool\n</code></pre> <p>Define if user request dynamic axes to be in the NNEF graph.</p> <p>Some inference engines may not support it hence False by default.</p>"},{"location":"reference/torch_to_nnef/inference_target/base/#torch_to_nnef.inference_target.base.InferenceTarget.post_export","title":"post_export","text":"<pre><code>post_export(model: nn.Module, nnef_graph: NGraph, args: T.List[T.Any], exported_filepath: Path, debug_bundle_path: T.Optional[Path] = None)\n</code></pre> <p>Get called after NNEF model asset is generated.</p> <p>This is typically where check_io is effectively applied.</p>"},{"location":"reference/torch_to_nnef/inference_target/base/#torch_to_nnef.inference_target.base.InferenceTarget.post_trace","title":"post_trace","text":"<pre><code>post_trace(nnef_graph: NGraph, active_custom_extensions: T.List[str])\n</code></pre> <p>Get called just after PyTorch graph is parsed.</p>"},{"location":"reference/torch_to_nnef/inference_target/base/#torch_to_nnef.inference_target.base.InferenceTarget.pre_trace","title":"pre_trace","text":"<pre><code>pre_trace(model: nn.Module, input_names: T.Optional[T.List[str]], output_names: T.Optional[T.List[str]])\n</code></pre> <p>Get called just before PyTorch graph is traced.</p> <p>(after auto wrapper)</p>"},{"location":"reference/torch_to_nnef/inference_target/base/#torch_to_nnef.inference_target.base.InferenceTarget.specific_fragments","title":"specific_fragments","text":"<pre><code>specific_fragments(model: nn.Module) -&gt; T.Dict[str, str]\n</code></pre> <p>Optional custom fragments to pass.</p>"},{"location":"reference/torch_to_nnef/inference_target/khronos/","title":"torch_to_nnef.inference_target.khronos","text":""},{"location":"reference/torch_to_nnef/inference_target/khronos/#torch_to_nnef.inference_target.khronos","title":"torch_to_nnef.inference_target.khronos","text":""},{"location":"reference/torch_to_nnef/inference_target/khronos/#torch_to_nnef.inference_target.khronos.KhronosNNEF","title":"KhronosNNEF","text":"<pre><code>KhronosNNEF(version: T.Union[SemanticVersion, str], check_io: bool = True)\n</code></pre> <p>               Bases: <code>InferenceTarget</code></p> <p>Khronos Specification compliant NNEF asset build.</p> <p>in case of check_io=True     we perform evaluation against nnef_tool nnef to pytorch converter.     And access original and reloaded pytorch model provide same outputs</p>"},{"location":"reference/torch_to_nnef/inference_target/khronos/#torch_to_nnef.inference_target.khronos.KhronosNNEF.post_export","title":"post_export","text":"<pre><code>post_export(model: nn.Module, nnef_graph: NGraph, args: T.List[T.Any], exported_filepath: Path, debug_bundle_path: T.Optional[Path] = None)\n</code></pre> <p>Check io via the Torch interpreter of NNEF-Tools.</p>"},{"location":"reference/torch_to_nnef/inference_target/tract/","title":"torch_to_nnef.inference_target.tract","text":""},{"location":"reference/torch_to_nnef/inference_target/tract/#torch_to_nnef.inference_target.tract","title":"torch_to_nnef.inference_target.tract","text":"<p>Tools to manipulate tract programatically.</p> <p>NOTE: interaction are done with *Nix tty system in mind, no support for Windows</p>"},{"location":"reference/torch_to_nnef/inference_target/tract/#torch_to_nnef.inference_target.tract.TractBinaryDownloader","title":"TractBinaryDownloader","text":"<pre><code>TractBinaryDownloader(version: SemanticVersion, auto_download: bool = True)\n</code></pre> <p>Tract Downloader.</p> <p>NOTE: Current version assume you are using hardware officialy supported by tract with pre-built binaries.</p>"},{"location":"reference/torch_to_nnef/inference_target/tract/#torch_to_nnef.inference_target.tract.TractBinaryDownloader.arch","title":"arch  <code>property</code>","text":"<pre><code>arch\n</code></pre> <p>Current OS architecture name needed to download tract cli asset.</p>"},{"location":"reference/torch_to_nnef/inference_target/tract/#torch_to_nnef.inference_target.tract.TractBinaryDownloader.dl_tract","title":"dl_tract","text":"<pre><code>dl_tract()\n</code></pre> <p>Download tract requested version in cache directory.</p>"},{"location":"reference/torch_to_nnef/inference_target/tract/#torch_to_nnef.inference_target.tract.TractCheckTolerance","title":"TractCheckTolerance","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Level of tolerated difference between output values of PyTorch and tract.</p> <p>(those are defined in tract)</p>"},{"location":"reference/torch_to_nnef/inference_target/tract/#torch_to_nnef.inference_target.tract.TractCli","title":"TractCli","text":"<pre><code>TractCli(tract_path: Path)\n</code></pre> <p>tract calls from CLI.</p> <p>Why not use python package provided since few release of tract ?</p> <ul> <li>we do not want to be coupled with a python lib as we declare   version requested in API   because this would lead to the need for an auto package   download/import then rollback   (since original environement may use another version)</li> </ul>"},{"location":"reference/torch_to_nnef/inference_target/tract/#torch_to_nnef.inference_target.tract.TractCli.assert_io_cmd_str","title":"assert_io_cmd_str","text":"<pre><code>assert_io_cmd_str(nnef_path: Path, io_npz_path: Path, check_tolerance: TractCheckTolerance = TractCheckTolerance.EXACT)\n</code></pre> <p>Assert a NNEF asset has outputs within tolerance bound with tract.</p>"},{"location":"reference/torch_to_nnef/inference_target/tract/#torch_to_nnef.inference_target.tract.TractCli.download","title":"download  <code>classmethod</code>","text":"<pre><code>download(version: SemanticVersion) -&gt; TractCli\n</code></pre> <p>Download tract requested version in cache directory.</p>"},{"location":"reference/torch_to_nnef/inference_target/tract/#torch_to_nnef.inference_target.tract.TractNNEF","title":"TractNNEF","text":"<pre><code>TractNNEF(version: T.Union[str, SemanticVersion], feature_flags: T.Optional[T.Set[TractFeatureFlag]] = None, check_io: bool = True, dynamic_axes: T.Optional[T.Dict[str, T.Dict[int, str]]] = None, specific_tract_binary_path: T.Optional[Path] = None, check_io_tolerance: TractCheckTolerance = TractCheckTolerance.APPROXIMATE, specific_properties: T.Optional[T.Dict[str, str]] = None, dump_identity_properties: bool = True, force_attention_inner_in_f32: bool = False, force_linear_accumulation_in_f32: bool = False, force_norm_in_f32: bool = False, reify_sdpa_operator: bool = False, upsample_with_debox: bool = False)\n</code></pre> <p>               Bases: <code>InferenceTarget</code></p> <p>Tract NNEF inference target.</p> <p>Init.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>Union[str, SemanticVersion]</code> <p>tract version targeted for export</p> required <code>feature_flags</code> <code>Optional[Set[TractFeatureFlag]]</code> <p>set of possibly added feature flags from tract (for example complex numbers)</p> <code>None</code> <code>check_io</code> <code>bool</code> <p>check between tract cli and Pytorch original model that given provided input, output is similar</p> <code>True</code> <code>dynamic_axes</code> <code>Optional[Dict[str, Dict[int, str]]]</code> <p>Optional specification of dynamic dimension By default the exported model will have the shapes of all input and output tensors set to exactly match those given in args. To specify axes of tensors as dynamic (i.e. known only at runtime) set dynamic_axes to a dict with schema:     KEY (str): an input or output name. Each name must also         be provided in input_names or output_names.     VALUE (dict or list): If a dict, keys are axis indices         and values are axis names. If a list, each element is         an axis index.</p> <code>None</code> <code>specific_tract_binary_path</code> <code>Optional[Path]</code> <p>filepath of tract cli in case of custom non released version of tract (for testing purpose)</p> <code>None</code> <code>check_io_tolerance</code> <code>TractCheckTolerance</code> <p>TractCheckTolerance level of difference tolerance between original output values and those generated by tract (those are defined tract levels)</p> <code>APPROXIMATE</code> <code>specific_properties</code> <code>Optional[Dict[str, str]]</code> <p>custom tract_properties you wish to add inside NNEF asset (will be parsed by tract as metadata)</p> <code>None</code> <code>dump_identity_properties</code> <code>bool</code> <p>add tract_properties relative to user identity (host, username, OS...), helpfull for debug</p> <code>True</code> <code>force_attention_inner_in_f32</code> <code>bool</code> <pre><code>control if attention should be forced as f32 inside\n(even if inputs are all f16), usefull for unstable networks\nlike qwen2.5\n</code></pre> <code>False</code> <code>force_linear_accumulation_in_f32</code> <code>bool</code> <p>usefull for f16 models to ensure that output of f16. f16 matmul become f32 accumulators.</p> <code>False</code> <code>force_norm_in_f32</code> <code>bool</code> <p>ensure that all normalization layers are in f32 whatever the original PyTorch modeling.</p> <code>False</code> <code>reify_sdpa_operator</code> <code>bool</code> <p>enable the conversion of scaled_dot_product_attention as a tract operator (intead of a NNEF fragment). Experimental feature.</p> <code>False</code> <code>upsample_with_debox</code> <code>bool</code> <p>use debox upsample operator instead of deconvolution. This should be faster. (if tract version support it). Experimental feature.</p> <code>False</code>"},{"location":"reference/torch_to_nnef/inference_target/tract/#torch_to_nnef.inference_target.tract.TractNNEF.post_export","title":"post_export","text":"<pre><code>post_export(model: nn.Module, nnef_graph: NGraph, args: T.List[T.Any], exported_filepath: Path, debug_bundle_path: T.Optional[Path] = None)\n</code></pre> <p>Perform check io and build debug bundle if fail.</p>"},{"location":"reference/torch_to_nnef/inference_target/tract/#torch_to_nnef.inference_target.tract.TractNNEF.post_trace","title":"post_trace","text":"<pre><code>post_trace(nnef_graph, active_custom_extensions)\n</code></pre> <p>Add dynamic axes in the NNEF graph.</p>"},{"location":"reference/torch_to_nnef/inference_target/tract/#torch_to_nnef.inference_target.tract.TractNNEF.pre_trace","title":"pre_trace","text":"<pre><code>pre_trace(model: nn.Module, input_names: T.Optional[T.List[str]], output_names: T.Optional[T.List[str]])\n</code></pre> <p>Check dynamic_axes are correctly formated.</p>"},{"location":"reference/torch_to_nnef/inference_target/tract/#torch_to_nnef.inference_target.tract.TractNNEF.specific_fragments","title":"specific_fragments","text":"<pre><code>specific_fragments(model: nn.Module) -&gt; T.Dict[str, str]\n</code></pre> <p>Optional custom fragments to pass.</p>"},{"location":"reference/torch_to_nnef/inference_target/tract/#torch_to_nnef.inference_target.tract.assert_io","title":"assert_io","text":"<pre><code>assert_io(model: nn.Module, test_input, nnef_file_path: Path, tract_cli: TractCli, io_npz_path: T.Optional[Path] = None, input_names: T.Optional[T.List[str]] = None, output_names: T.Optional[T.List[str]] = None, check_tolerance: TractCheckTolerance = TractCheckTolerance.EXACT)\n</code></pre> <p>Simple assertion without debug bundle.</p> <p>With addition of gc of model once output is generated.</p>"},{"location":"reference/torch_to_nnef/inference_target/tract/#torch_to_nnef.inference_target.tract.assert_io_and_debug_bundle","title":"assert_io_and_debug_bundle","text":"<pre><code>assert_io_and_debug_bundle(model: nn.Module, test_input, nnef_file_path: Path, tract_cli: TractCli, io_npz_path: T.Optional[Path] = None, debug_bundle_path: T.Optional[Path] = None, input_names: T.Optional[T.List[str]] = None, output_names: T.Optional[T.List[str]] = None, check_tolerance: TractCheckTolerance = TractCheckTolerance.EXACT)\n</code></pre> <p>Core check to ensure tract give same output as PyTorch within bounds.</p>"},{"location":"reference/torch_to_nnef/inference_target/tract/#torch_to_nnef.inference_target.tract.debug_dumper_pytorch_to_onnx_to_nnef","title":"debug_dumper_pytorch_to_onnx_to_nnef","text":"<pre><code>debug_dumper_pytorch_to_onnx_to_nnef(model: nn.Module, test_input, target_folder: Path, tract_cli: TractCli, raise_export_error: bool = True) -&gt; bool\n</code></pre> <p>Try to export the model with ONNX and convert the ONNX to NNEF via tract.</p> <p>Used in debug bundle build (if it works, that's give a valuable reference, to debug T2N)</p>"},{"location":"reference/torch_to_nnef/llm_tract/","title":"torch_to_nnef.llm_tract","text":""},{"location":"reference/torch_to_nnef/llm_tract/#torch_to_nnef.llm_tract","title":"torch_to_nnef.llm_tract","text":""},{"location":"reference/torch_to_nnef/llm_tract/cli/","title":"torch_to_nnef.llm_tract.cli","text":""},{"location":"reference/torch_to_nnef/llm_tract/cli/#torch_to_nnef.llm_tract.cli","title":"torch_to_nnef.llm_tract.cli","text":"<p>Export any huggingface transformers LLM to tract NNEF.</p> <p>With options to compress it to Q4_0 and use float16</p>"},{"location":"reference/torch_to_nnef/llm_tract/config/","title":"torch_to_nnef.llm_tract.config","text":""},{"location":"reference/torch_to_nnef/llm_tract/config/#torch_to_nnef.llm_tract.config","title":"torch_to_nnef.llm_tract.config","text":""},{"location":"reference/torch_to_nnef/llm_tract/config/#torch_to_nnef.llm_tract.config.HFConfigHelper","title":"HFConfigHelper","text":"<pre><code>HFConfigHelper(conf)\n</code></pre> <p>HuggingFace config helper.</p> <p>Allow to extract usefull informations from config to set export parameters</p>"},{"location":"reference/torch_to_nnef/llm_tract/exporter/","title":"torch_to_nnef.llm_tract.exporter","text":""},{"location":"reference/torch_to_nnef/llm_tract/exporter/#torch_to_nnef.llm_tract.exporter","title":"torch_to_nnef.llm_tract.exporter","text":""},{"location":"reference/torch_to_nnef/llm_tract/exporter/#torch_to_nnef.llm_tract.exporter.LLMExporter","title":"LLMExporter","text":"<pre><code>LLMExporter(hf_model_causal: nn.Module, tokenizer: AutoTokenizer, local_dir: T.Optional[Path] = None, force_module_dtype: T.Optional[DtypeStr] = None, force_inputs_dtype: T.Optional[DtypeStr] = None, num_logits_to_keep: int = 1)\n</code></pre> <p>Init LLMExporter.</p> <p>Parameters:</p> Name Type Description Default <code>hf_model_causal</code> <code>Module</code> <p>Any Causal model from <code>transformers</code> library</p> required <code>tokenizer</code> <code>AutoTokenizer</code> <p>Any tokenizer from <code>transformers</code> library</p> required <code>local_dir</code> <code>Optional[Path]</code> <p>If set this is the local directory from where model was loaded.</p> <code>None</code> <code>force_module_dtype</code> <code>Optional[DtypeStr]</code> <p>Force PyTorch dtype in parameters.</p> <code>None</code> <code>force_inputs_dtype</code> <code>Optional[DtypeStr]</code> <p>Force PyTorch dtype in inputs of the models.</p> <code>None</code> <code>num_logits_to_keep</code> <code>int</code> <p>int number of token to keep (if 0 all are kept) by default for classical inference setting it to 1 is fine, in case of speculative decoding it may be more (typically 2 or 3)</p> <code>1</code>"},{"location":"reference/torch_to_nnef/llm_tract/exporter/#torch_to_nnef.llm_tract.exporter.LLMExporter.apply_half_precision_fixes","title":"apply_half_precision_fixes","text":"<pre><code>apply_half_precision_fixes()\n</code></pre> <p>Align float dtype arguments in few graph ops.</p> <p>Indeed all LLM are trained using GPU/TPU/CPU kernels related PyTorch backend support f16 dtype in some operators contrary to PyTorch CPU inference (@ 2024-09-09).</p> <p>To solve this issue we monkey patch in this cli few functional API.</p>"},{"location":"reference/torch_to_nnef/llm_tract/exporter/#torch_to_nnef.llm_tract.exporter.LLMExporter.check_wrapper_io","title":"check_wrapper_io","text":"<pre><code>check_wrapper_io()\n</code></pre> <p>Check the wrapper gives same outputs compared to vanilla model.</p>"},{"location":"reference/torch_to_nnef/llm_tract/exporter/#torch_to_nnef.llm_tract.exporter.LLMExporter.dump","title":"dump","text":"<pre><code>dump(**kwargs)\n</code></pre> <p>Prepare and export model to NNEF.</p>"},{"location":"reference/torch_to_nnef/llm_tract/exporter/#torch_to_nnef.llm_tract.exporter.LLMExporter.dump_all_io_npz_kind","title":"dump_all_io_npz_kind","text":"<pre><code>dump_all_io_npz_kind(io_npz_dirpath: Path, size: int = 6) -&gt; T.List[Path]\n</code></pre> <p>Realistic dump of IO's.</p>"},{"location":"reference/torch_to_nnef/llm_tract/exporter/#torch_to_nnef.llm_tract.exporter.LLMExporter.export_model","title":"export_model","text":"<pre><code>export_model(export_dirpath: Path, inference_target: TractNNEF, naming_scheme: VariableNamingScheme = LM_VAR_SCHEME, log_level=logging.INFO, dump_with_tokenizer_and_conf: bool = False, check_inference_modes: bool = True, sample_generation_total_size: int = 0, ignore_already_exist_dir: bool = False, export_dir_struct: ExportDirStruct = ExportDirStruct.DEEP, debug_bundle_path: T.Optional[Path] = None)\n</code></pre> <p>Export model has is currently in self.hf_model_causal.</p> <p>and dump some npz tests to check io latter-on</p>"},{"location":"reference/torch_to_nnef/llm_tract/exporter/#torch_to_nnef.llm_tract.exporter.LLMExporter.load","title":"load  <code>staticmethod</code>","text":"<pre><code>load(model_slug: T.Optional[str] = None, local_dir: T.Optional[Path] = None, **kwargs)\n</code></pre> <p>Load from either huggingface model slug hub or local_dir.</p>"},{"location":"reference/torch_to_nnef/llm_tract/exporter/#torch_to_nnef.llm_tract.exporter.LLMExporter.prepare","title":"prepare","text":"<pre><code>prepare(compression_method: T.Optional[str] = None, compression_registry: str = DEFAULT_COMPRESSION_REGISTRY, test_display_token_gens: bool = False, wrapper_io_check: bool = True, export_dirpath: T.Optional[Path] = None, log_level: int = logging.INFO)\n</code></pre> <p>Prepare model to export (f16/compression/checks...).</p>"},{"location":"reference/torch_to_nnef/llm_tract/exporter/#torch_to_nnef.llm_tract.exporter.LLMExporter.reset_torch_fns","title":"reset_torch_fns","text":"<pre><code>reset_torch_fns()\n</code></pre> <p>Cleanup any torch behavior alterations.</p>"},{"location":"reference/torch_to_nnef/llm_tract/exporter/#torch_to_nnef.llm_tract.exporter.StateLessF32LayerNorm","title":"StateLessF32LayerNorm","text":"<p>               Bases: <code>Module</code></p>"},{"location":"reference/torch_to_nnef/llm_tract/exporter/#torch_to_nnef.llm_tract.exporter.StateLessF32LayerNorm.forward","title":"forward","text":"<pre><code>forward(input: torch.Tensor, normalized_shape: T.List[int], weight: T.Optional[torch.Tensor] = None, bias: T.Optional[torch.Tensor] = None, eps: float = 1e-05)\n</code></pre> <p>Upcast and apply layer norm in f32.</p> <p>This is because f16 is not implemented on CPU in PyTorch (only GPU) as of torch 2.2.2 (2024-09-10): <pre><code>RuntimeError: \"LayerNormKernelImpl\" not implemented for 'Half'\n</code></pre></p>"},{"location":"reference/torch_to_nnef/llm_tract/exporter/#torch_to_nnef.llm_tract.exporter.dump_llm","title":"dump_llm","text":"<pre><code>dump_llm(model_slug: T.Optional[str] = None, local_dir: T.Optional[Path] = None, force_module_dtype: T.Optional[DtypeStr] = None, force_inputs_dtype: T.Optional[DtypeStr] = None, merge_peft: T.Optional[bool] = None, num_logits_to_keep: int = 1, device_map: TYPE_OPTIONAL_DEVICE_MAP = None, **kwargs) -&gt; T.Tuple[T.Union[Path, None], LLMExporter]\n</code></pre> <p>Util to export LLM model.</p>"},{"location":"reference/torch_to_nnef/llm_tract/exporter/#torch_to_nnef.llm_tract.exporter.find_subdir_with_filename_in","title":"find_subdir_with_filename_in","text":"<pre><code>find_subdir_with_filename_in(dirpath: Path, filename: str) -&gt; Path\n</code></pre> <p>Find a subdir with filename in it.</p>"},{"location":"reference/torch_to_nnef/llm_tract/exporter/#torch_to_nnef.llm_tract.exporter.load_peft_model","title":"load_peft_model","text":"<pre><code>load_peft_model(local_dir, kwargs)\n</code></pre> <p>Load PEFT adapted models.</p> <p>Try to avoid direct reference to tokenizer object/config to limit dependencies of the function</p> <p>While also trying to be robust to 'wrong' key/values</p>"},{"location":"reference/torch_to_nnef/llm_tract/models/","title":"torch_to_nnef.llm_tract.models","text":""},{"location":"reference/torch_to_nnef/llm_tract/models/#torch_to_nnef.llm_tract.models","title":"torch_to_nnef.llm_tract.models","text":""},{"location":"reference/torch_to_nnef/llm_tract/models/base/","title":"torch_to_nnef.llm_tract.models.base","text":""},{"location":"reference/torch_to_nnef/llm_tract/models/base/#torch_to_nnef.llm_tract.models.base","title":"torch_to_nnef.llm_tract.models.base","text":""},{"location":"reference/torch_to_nnef/llm_tract/models/base/#torch_to_nnef.llm_tract.models.base.BaseCausalWithDynCacheAndTriu","title":"BaseCausalWithDynCacheAndTriu","text":"<pre><code>BaseCausalWithDynCacheAndTriu(model: AutoModelForCausalLM, num_logits_to_keep: int = 1)\n</code></pre> <p>               Bases: <code>TorchToNNEFWrappedLLM</code></p> <p>Assume common AutoModelForCausalLM arch.</p> <p>with : - .model - .lm_head</p>"},{"location":"reference/torch_to_nnef/llm_tract/models/base/#torch_to_nnef.llm_tract.models.base.BaseCausalWithDynCacheAndTriu.forward","title":"forward","text":"<pre><code>forward(input_ids: torch.Tensor, *args)\n</code></pre> <p>Forward of BaseCausalWithDynCacheAndTriu.</p> <p>Same as calling without any smart caching mechanism self.model.model+lm_head and softmax.</p> <p>This export module is extremly ineficient because no caching can be provided ...</p>"},{"location":"reference/torch_to_nnef/llm_tract/models/base/#torch_to_nnef.llm_tract.models.base.TorchToNNEFWrappedLLM","title":"TorchToNNEFWrappedLLM","text":"<pre><code>TorchToNNEFWrappedLLM()\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Base module class for all LLM wrapping.</p> <p>These wrapper are needed to ensure deterministic inputs/outputs graph signature and allow some modeling optimization of few architecture.</p>"},{"location":"reference/torch_to_nnef/llm_tract/models/base/#torch_to_nnef.llm_tract.models.base.ctx_dtype_dyn_cache","title":"ctx_dtype_dyn_cache","text":"<pre><code>ctx_dtype_dyn_cache()\n</code></pre> <p>Context Manager to handle inconsistent device type in KV-cache update.</p> <p>This may be due for example to the use of accelerate 'meta' tensors device.</p> <p>This manager is stackable in such case only largest context will be applied.</p>"},{"location":"reference/torch_to_nnef/llm_tract/models/base/#torch_to_nnef.llm_tract.models.base.update_forward_signature","title":"update_forward_signature","text":"<pre><code>update_forward_signature(self)\n</code></pre> <p>Trickery to help torch &gt; 2.0 new export API tracing.</p>"},{"location":"reference/torch_to_nnef/llm_tract/models/base/#torch_to_nnef.llm_tract.models.base.use_dtype_dyn_cache","title":"use_dtype_dyn_cache","text":"<pre><code>use_dtype_dyn_cache(f)\n</code></pre> <p>Annotator for forward function applying <code>ctx_dtype_dyn_cache</code>.</p>"},{"location":"reference/torch_to_nnef/log/","title":"torch_to_nnef.log","text":""},{"location":"reference/torch_to_nnef/log/#torch_to_nnef.log","title":"torch_to_nnef.log","text":"<p>Simple helper to manage torch_to_nnef logging.</p>"},{"location":"reference/torch_to_nnef/log/#torch_to_nnef.log.init_log","title":"init_log","text":"<pre><code>init_log()\n</code></pre> <p>Default init log handlers for torch_to_nnef clis.</p>"},{"location":"reference/torch_to_nnef/log/#torch_to_nnef.log.set_lib_log_level","title":"set_lib_log_level","text":"<pre><code>set_lib_log_level(log_level: int)\n</code></pre> <p>Set torch_to_nnef log_level.</p>"},{"location":"reference/torch_to_nnef/model_wrapper/","title":"torch_to_nnef.model_wrapper","text":""},{"location":"reference/torch_to_nnef/model_wrapper/#torch_to_nnef.model_wrapper","title":"torch_to_nnef.model_wrapper","text":"<p>Wrap model to bypass limitation of torch_to_nnef internals.</p> <p>ie: Cases where inputs or outputs of a model contains tuples</p>"},{"location":"reference/torch_to_nnef/model_wrapper/#torch_to_nnef.model_wrapper.WrapStructIO","title":"WrapStructIO","text":"<pre><code>WrapStructIO(model: nn.Module, input_infos, output_infos)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Once traced it should be nop in final graph.</p>"},{"location":"reference/torch_to_nnef/nnef_graph/","title":"torch_to_nnef.nnef_graph","text":""},{"location":"reference/torch_to_nnef/nnef_graph/#torch_to_nnef.nnef_graph","title":"torch_to_nnef.nnef_graph","text":"<p>Core parsing and NNEF transformation module.</p>"},{"location":"reference/torch_to_nnef/nnef_graph/#torch_to_nnef.nnef_graph.TorchToNGraphExtractor","title":"TorchToNGraphExtractor","text":"<pre><code>TorchToNGraphExtractor(model: torch.nn.Module, args: T.Tuple[torch.Tensor, ...], inference_target: InferenceTarget, nnef_variable_naming_scheme: VariableNamingScheme = DEFAULT_VARNAME_SCHEME, forced_inputs_names: T.Optional[T.List[str]] = None, forced_outputs_names: T.Optional[T.List[str]] = None, check_io_names_qte_match: bool = True)\n</code></pre> <p>Extract PyTorch Graph and build associated nnef_tools.model.Graph.</p>"},{"location":"reference/torch_to_nnef/op/","title":"torch_to_nnef.op","text":""},{"location":"reference/torch_to_nnef/op/#torch_to_nnef.op","title":"torch_to_nnef.op","text":"<p>Module that translate all internal torch_to_nnef IR operators into NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/","title":"torch_to_nnef.op.aten","text":""},{"location":"reference/torch_to_nnef/op/aten/#torch_to_nnef.op.aten","title":"torch_to_nnef.op.aten","text":"<p>PyTorch Aten::* operators translation.</p>"},{"location":"reference/torch_to_nnef/op/aten/#torch_to_nnef.op.aten.aten_to_nnef_tensor_and_ops","title":"aten_to_nnef_tensor_and_ops","text":"<pre><code>aten_to_nnef_tensor_and_ops(g, node, name_to_tensor, null_ref, torch_graph, inference_target: InferenceTarget) -&gt; T.Optional[T.List[str]]\n</code></pre> <p>Main primitive dispatcher.</p> <p>Allow to write in graph any not Quantized Operation from pytorch defined in node attribute.</p>"},{"location":"reference/torch_to_nnef/op/aten/activation/","title":"torch_to_nnef.op.aten.activation","text":""},{"location":"reference/torch_to_nnef/op/aten/activation/#torch_to_nnef.op.aten.activation","title":"torch_to_nnef.op.aten.activation","text":""},{"location":"reference/torch_to_nnef/op/aten/activation/#torch_to_nnef.op.aten.activation.clamp","title":"clamp","text":"<pre><code>clamp(g, node, name_to_tensor, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:clamp' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/activation/#torch_to_nnef.op.aten.activation.clamp_max","title":"clamp_max","text":"<pre><code>clamp_max(g, node, name_to_tensor, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:clamp_max' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/activation/#torch_to_nnef.op.aten.activation.clamp_min","title":"clamp_min","text":"<pre><code>clamp_min(g, node, name_to_tensor, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:clamp_min' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/activation/#torch_to_nnef.op.aten.activation.elu","title":"elu","text":"<pre><code>elu(**kwargs)\n</code></pre> <p>Map PyTorch: 'aten:elu' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/activation/#torch_to_nnef.op.aten.activation.erf","title":"erf","text":"<pre><code>erf(g, node, name_to_tensor, null_ref, inference_target, **kwargs)\n</code></pre> <p>Op should be added to tract-nnef eventualy.</p>"},{"location":"reference/torch_to_nnef/op/aten/activation/#torch_to_nnef.op.aten.activation.gelu","title":"gelu","text":"<pre><code>gelu(g, node, name_to_tensor, null_ref, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:gelu' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/activation/#torch_to_nnef.op.aten.activation.glu","title":"glu","text":"<pre><code>glu(g, node, name_to_tensor, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:glu' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/activation/#torch_to_nnef.op.aten.activation.hardswish","title":"hardswish","text":"<pre><code>hardswish(inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:hardswish' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/activation/#torch_to_nnef.op.aten.activation.hardtanh","title":"hardtanh","text":"<pre><code>hardtanh(**kwargs)\n</code></pre> <p>Map PyTorch: 'aten:hardtanh' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/activation/#torch_to_nnef.op.aten.activation.leaky_relu","title":"leaky_relu","text":"<pre><code>leaky_relu(**kwargs)\n</code></pre> <p>Map PyTorch: 'aten:leaky_relu' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/activation/#torch_to_nnef.op.aten.activation.log_softmax","title":"log_softmax","text":"<pre><code>log_softmax(inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:log_softmax' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/activation/#torch_to_nnef.op.aten.activation.prelu","title":"prelu","text":"<pre><code>prelu(**kwargs)\n</code></pre> <p>Map PyTorch: 'aten:prelu' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/activation/#torch_to_nnef.op.aten.activation.relu6","title":"relu6","text":"<pre><code>relu6(**kwargs)\n</code></pre> <p>Map PyTorch: 'aten:relu6' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/activation/#torch_to_nnef.op.aten.activation.selu","title":"selu","text":"<pre><code>selu(**kwargs)\n</code></pre> <p>Map PyTorch: 'aten:selu' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/activation/#torch_to_nnef.op.aten.activation.silu","title":"silu","text":"<pre><code>silu(**kwargs)\n</code></pre> <p>Map PyTorch: 'aten:silu' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/activation/#torch_to_nnef.op.aten.activation.softmax","title":"softmax","text":"<pre><code>softmax(**kwargs)\n</code></pre> <p>Map PyTorch: 'aten:softmax', 'aten:_softmax' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/activation/#torch_to_nnef.op.aten.activation.softplus","title":"softplus","text":"<pre><code>softplus(**kwargs)\n</code></pre> <p>Map PyTorch: 'aten:softplus' to NNEF.</p> <p>Note: numerical stability applied in PyTorch is not done in NNEF vanilla implementation, nor case beta != 1.</p> PyTorch ref <p>y = (1/beta) * log(exp(beta * x) + 1)  if ((beta * x) &lt; thresh) else x</p> NNEF ref <p>y = log(exp(x) + 1.0)</p>"},{"location":"reference/torch_to_nnef/op/aten/attn/","title":"torch_to_nnef.op.aten.attn","text":""},{"location":"reference/torch_to_nnef/op/aten/attn/#torch_to_nnef.op.aten.attn","title":"torch_to_nnef.op.aten.attn","text":"<p>Attention mechanisms.</p>"},{"location":"reference/torch_to_nnef/op/aten/attn/#torch_to_nnef.op.aten.attn.scaled_dot_product_attention","title":"scaled_dot_product_attention","text":"<pre><code>scaled_dot_product_attention(g, node, name_to_tensor, inference_target, **kwargs)\n</code></pre> <p>Translate operator: <code>aten::scaled_dot_product_attention</code> to NNEF.</p> reference <p>https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html</p>"},{"location":"reference/torch_to_nnef/op/aten/axes_change/","title":"torch_to_nnef.op.aten.axes_change","text":""},{"location":"reference/torch_to_nnef/op/aten/axes_change/#torch_to_nnef.op.aten.axes_change","title":"torch_to_nnef.op.aten.axes_change","text":""},{"location":"reference/torch_to_nnef/op/aten/axes_change/#torch_to_nnef.op.aten.axes_change.flatten","title":"flatten","text":"<pre><code>flatten(g, node, name_to_tensor, inference_target, **kwargs)\n</code></pre> <p>Translate operator: <code>aten::flatten</code> to NNEF.</p> <p>Using NNEF:. fragment reshape&lt;?&gt;(     input: tensor&lt;?&gt;,     shape: integer[],     axis_start: integer = 0,     axis_count: integer = -1 ) -&gt; ( output: tensor&lt;?&gt; );</p>"},{"location":"reference/torch_to_nnef/op/aten/axes_change/#torch_to_nnef.op.aten.axes_change.permute","title":"permute","text":"<pre><code>permute(g, node, name_to_tensor, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:permute' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/axes_change/#torch_to_nnef.op.aten.axes_change.reshape","title":"reshape","text":"<pre><code>reshape(g, node, name_to_tensor, torch_graph, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:reshape' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/axes_change/#torch_to_nnef.op.aten.axes_change.squeeze","title":"squeeze","text":"<pre><code>squeeze(g, node, name_to_tensor, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:squeeze' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/axes_change/#torch_to_nnef.op.aten.axes_change.transpose","title":"transpose","text":"<pre><code>transpose(g, node, name_to_tensor, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:transpose' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/axes_change/#torch_to_nnef.op.aten.axes_change.unflatten","title":"unflatten","text":"<pre><code>unflatten(g, node, name_to_tensor, torch_graph, op_helper, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:unflatten' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/axes_change/#torch_to_nnef.op.aten.axes_change.unsqueeze","title":"unsqueeze","text":"<pre><code>unsqueeze(g, node, name_to_tensor, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:unsqueeze' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/axes_change/#torch_to_nnef.op.aten.axes_change.view","title":"view","text":"<pre><code>view(g, node, name_to_tensor, torch_graph, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:view' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/complex/","title":"torch_to_nnef.op.aten.complex","text":""},{"location":"reference/torch_to_nnef/op/aten/complex/#torch_to_nnef.op.aten.complex","title":"torch_to_nnef.op.aten.complex","text":""},{"location":"reference/torch_to_nnef/op/aten/complex/#torch_to_nnef.op.aten.complex.view_as_complex","title":"view_as_complex","text":"<pre><code>view_as_complex(node, inference_target, torch_graph, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:view_as_complex' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/complex/#torch_to_nnef.op.aten.complex.view_as_real","title":"view_as_real","text":"<pre><code>view_as_real(node, torch_graph, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:view_as_real' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/concat/","title":"torch_to_nnef.op.aten.concat","text":""},{"location":"reference/torch_to_nnef/op/aten/concat/#torch_to_nnef.op.aten.concat","title":"torch_to_nnef.op.aten.concat","text":""},{"location":"reference/torch_to_nnef/op/aten/concat/#torch_to_nnef.op.aten.concat.cat","title":"cat","text":"<pre><code>cat(g, node, name_to_tensor, torch_graph, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:cat' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/concat/#torch_to_nnef.op.aten.concat.hstack","title":"hstack","text":"<pre><code>hstack(g, node, name_to_tensor, torch_graph, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:hstack' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/concat/#torch_to_nnef.op.aten.concat.roll","title":"roll","text":"<pre><code>roll(g, node, name_to_tensor, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:roll' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/concat/#torch_to_nnef.op.aten.concat.stack","title":"stack","text":"<pre><code>stack(g, node, name_to_tensor, torch_graph, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:stack' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/concat/#torch_to_nnef.op.aten.concat.vstack","title":"vstack","text":"<pre><code>vstack(g, node, name_to_tensor, torch_graph, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:vstack' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/expand/","title":"torch_to_nnef.op.aten.expand","text":""},{"location":"reference/torch_to_nnef/op/aten/expand/#torch_to_nnef.op.aten.expand","title":"torch_to_nnef.op.aten.expand","text":""},{"location":"reference/torch_to_nnef/op/aten/expand/#torch_to_nnef.op.aten.expand.expand","title":"expand","text":"<pre><code>expand(node, inference_target, op_helper, **kwargs)\n</code></pre> <p>Translate operator <code>aten::expand</code> to NNEF.</p> <p>Illustration of expand:.     torch.arange(9).reshape(3, 3).expand(2, 3, 3)</p> <pre><code>Out[4]:\ntensor([[[0, 1, 2],\n         [3, 4, 5],\n         [6, 7, 8]],\n\n        [[0, 1, 2],\n         [3, 4, 5],\n         [6, 7, 8]]])\n</code></pre> which can be re-expressed as <p>torch.arange(9).reshape(3, 3).repeat(2).reshape(2, 3, 3)</p> <p>this allows us to express it as a NNEF tile followed by a reshape.</p>"},{"location":"reference/torch_to_nnef/op/aten/expand/#torch_to_nnef.op.aten.expand.repeat","title":"repeat","text":"<pre><code>repeat(g, node, name_to_tensor, op_helper, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:repeat' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/expand/#torch_to_nnef.op.aten.expand.repeat_interleave","title":"repeat_interleave","text":"<pre><code>repeat_interleave(g, node, name_to_tensor, inference_target, **kwargs)\n</code></pre> <p>This is same as np.repeat.</p> Equivalent with repeat <p>te = y new = te.unsqueeze(dim+1) new_repeats = [1] * (len(te.shape) + 1) new_repeats[ dim + 1 ] = n_repeat shapes = list(te.shape) shapes[dim] *= n_repeat new.repeat(new_repeats).reshape(shapes)</p>"},{"location":"reference/torch_to_nnef/op/aten/fft/","title":"torch_to_nnef.op.aten.fft","text":""},{"location":"reference/torch_to_nnef/op/aten/fft/#torch_to_nnef.op.aten.fft","title":"torch_to_nnef.op.aten.fft","text":""},{"location":"reference/torch_to_nnef/op/aten/fft/#torch_to_nnef.op.aten.fft.fft_fft","title":"fft_fft","text":"<pre><code>fft_fft(g, node, name_to_tensor, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:fft_fft' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/fft/#torch_to_nnef.op.aten.fft.fft_ifft","title":"fft_ifft","text":"<pre><code>fft_ifft(g, node, name_to_tensor, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:fft_ifft' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/fft/#torch_to_nnef.op.aten.fft.stft","title":"stft","text":"<pre><code>stft(g, node, name_to_tensor, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:stft' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/math/","title":"torch_to_nnef.op.aten.math","text":""},{"location":"reference/torch_to_nnef/op/aten/math/#torch_to_nnef.op.aten.math","title":"torch_to_nnef.op.aten.math","text":""},{"location":"reference/torch_to_nnef/op/aten/math/#torch_to_nnef.op.aten.math.atan2","title":"atan2","text":"<pre><code>atan2(node, op_helper, **kwargs)\n</code></pre> <p>aten::atan2.</p>"},{"location":"reference/torch_to_nnef/op/aten/math/#torch_to_nnef.op.aten.math.bitwise_and","title":"bitwise_and","text":"<pre><code>bitwise_and(node, op_helper, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:bitwise_and', 'aten:bitwise_cpu' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/math/#torch_to_nnef.op.aten.math.bitwise_not","title":"bitwise_not","text":"<pre><code>bitwise_not(node, op_helper, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:bitwise_not', 'aten:bitwise_not_cpu' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/math/#torch_to_nnef.op.aten.math.bitwise_or","title":"bitwise_or","text":"<pre><code>bitwise_or(node, op_helper, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:bitwise_or' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/math/#torch_to_nnef.op.aten.math.bitwise_xor","title":"bitwise_xor","text":"<pre><code>bitwise_xor(node, op_helper, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:bitwise_xor' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/math/#torch_to_nnef.op.aten.math.div","title":"div","text":"<pre><code>div(node, op_helper, inference_target, torch_graph, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:div' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/math/#torch_to_nnef.op.aten.math.expm1","title":"expm1","text":"<pre><code>expm1(node, op_helper, **kwargs)\n</code></pre> <p>aten::exp1m.</p>"},{"location":"reference/torch_to_nnef/op/aten/math/#torch_to_nnef.op.aten.math.floor_divide","title":"floor_divide","text":"<pre><code>floor_divide(node, op_helper, inference_target, torch_graph, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:floor_divide' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/math/#torch_to_nnef.op.aten.math.fmod","title":"fmod","text":"<pre><code>fmod(node, op_helper, **kwargs)\n</code></pre> <p>aten::fmod.</p> equivalent <p>a - a.div(b, rounding_mode=\"trunc\") * b</p>"},{"location":"reference/torch_to_nnef/op/aten/math/#torch_to_nnef.op.aten.math.log10","title":"log10","text":"<pre><code>log10(node, op_helper, **kwargs)\n</code></pre> <p>Mul val may not be good enough.</p>"},{"location":"reference/torch_to_nnef/op/aten/math/#torch_to_nnef.op.aten.math.log1p","title":"log1p","text":"<pre><code>log1p(node, op_helper, **kwargs)\n</code></pre> <p>aten::log1p.</p>"},{"location":"reference/torch_to_nnef/op/aten/math/#torch_to_nnef.op.aten.math.logical_xor","title":"logical_xor","text":"<pre><code>logical_xor(node, op_helper, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:logical_xor' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/math/#torch_to_nnef.op.aten.math.mul","title":"mul","text":"<pre><code>mul(node, op_helper, torch_graph, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:mul' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/math/#torch_to_nnef.op.aten.math.pow_","title":"pow_","text":"<pre><code>pow_(node, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:pow' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/math/#torch_to_nnef.op.aten.math.remainder","title":"remainder","text":"<pre><code>remainder(node, op_helper, torch_graph, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:remainder' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/math/#torch_to_nnef.op.aten.math.round_","title":"round_","text":"<pre><code>round_(inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:round' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/math/#torch_to_nnef.op.aten.math.rsub","title":"rsub","text":"<pre><code>rsub(node, op_helper, torch_graph, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:rsub' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/math/#torch_to_nnef.op.aten.math.trunc","title":"trunc","text":"<pre><code>trunc(node, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:trunc' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/math/#torch_to_nnef.op.aten.math.var","title":"var","text":"<pre><code>var(node, op_helper, **kwargs)\n</code></pre> <p>aten::var.</p>"},{"location":"reference/torch_to_nnef/op/aten/matmul/","title":"torch_to_nnef.op.aten.matmul","text":""},{"location":"reference/torch_to_nnef/op/aten/matmul/#torch_to_nnef.op.aten.matmul","title":"torch_to_nnef.op.aten.matmul","text":""},{"location":"reference/torch_to_nnef/op/aten/matmul/#torch_to_nnef.op.aten.matmul.baddbmm","title":"baddbmm","text":"<pre><code>baddbmm(g, node, name_to_tensor, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:baddbmm', 'aten:addmm' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/matmul/#torch_to_nnef.op.aten.matmul.einsum","title":"einsum","text":"<pre><code>einsum(g, node, name_to_tensor, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:einsum' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/matmul/#torch_to_nnef.op.aten.matmul.linear","title":"linear","text":"<pre><code>linear(g, node, name_to_tensor, null_ref, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:linear' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/matmul/#torch_to_nnef.op.aten.matmul.matmul","title":"matmul","text":"<pre><code>matmul(g, node, name_to_tensor, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:matmul', 'aten:bmm', 'aten:mm' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/norm/","title":"torch_to_nnef.op.aten.norm","text":""},{"location":"reference/torch_to_nnef/op/aten/norm/#torch_to_nnef.op.aten.norm","title":"torch_to_nnef.op.aten.norm","text":""},{"location":"reference/torch_to_nnef/op/aten/norm/#torch_to_nnef.op.aten.norm.batch_norm","title":"batch_norm","text":"<pre><code>batch_norm(g, node, name_to_tensor, null_ref, inference_target, **kwargs)\n</code></pre> <p>Translate operator <code>aten::batch_norm</code> to NNEF.</p> <p>Nnef inputs:.     input: tensor     mean: tensor     variance: tensor     offset: tensor     scale: tensor     epsilon: scalar nnef op <p>output = offset + scale * (input - mean) / sqrt(variance + epsilon);</p>"},{"location":"reference/torch_to_nnef/op/aten/norm/#torch_to_nnef.op.aten.norm.group_norm","title":"group_norm","text":"<pre><code>group_norm(g, node, name_to_tensor, inference_target, **kwargs)\n</code></pre> <p>Translate operators <code>aten::group_norm</code> to NNEF.</p> <p>It is a special case of NNEF batch_normalization. with variance and mean being tensor</p>"},{"location":"reference/torch_to_nnef/op/aten/norm/#torch_to_nnef.op.aten.norm.layer_norm","title":"layer_norm","text":"<pre><code>layer_norm(g, node, name_to_tensor, null_ref, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:layer_norm', 'aten:native_layer_norm' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/norm/#torch_to_nnef.op.aten.norm.norm","title":"norm","text":"<pre><code>norm(g, node, name_to_tensor, inference_target, **kwargs)\n</code></pre> <p>NOTE this is only the normed vector.</p>"},{"location":"reference/torch_to_nnef/op/aten/other/","title":"torch_to_nnef.op.aten.other","text":""},{"location":"reference/torch_to_nnef/op/aten/other/#torch_to_nnef.op.aten.other","title":"torch_to_nnef.op.aten.other","text":""},{"location":"reference/torch_to_nnef/op/aten/other/#torch_to_nnef.op.aten.other.contiguous","title":"contiguous","text":"<pre><code>contiguous(node, torch_graph, **kwargs)\n</code></pre> <p>This does not translate to any operation.</p>"},{"location":"reference/torch_to_nnef/op/aten/other/#torch_to_nnef.op.aten.other.detach","title":"detach","text":"<pre><code>detach(node, torch_graph, **kwargs)\n</code></pre> <p>This does not translate to any operation.</p>"},{"location":"reference/torch_to_nnef/op/aten/other/#torch_to_nnef.op.aten.other.dropout","title":"dropout","text":"<pre><code>dropout(node, torch_graph, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:dropout' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/other/#torch_to_nnef.op.aten.other.external","title":"external","text":"<pre><code>external(g: NGraph, node: TensorVariable, name_to_tensor: T.Dict[str, NTensor], inference_target: InferenceTarget)\n</code></pre> <p>Add External NNEF Operation in graph.</p>"},{"location":"reference/torch_to_nnef/op/aten/other/#torch_to_nnef.op.aten.other.numel","title":"numel","text":"<pre><code>numel(node, inference_target, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:numel' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/other/#torch_to_nnef.op.aten.other.scalar_tensor","title":"scalar_tensor","text":"<pre><code>scalar_tensor(node, inference_target, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:scalar_tensor' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/other/#torch_to_nnef.op.aten.other.size","title":"size","text":"<pre><code>size(g, node, name_to_tensor, inference_target, torch_graph, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch <code>aten::size</code> as NNEF.</p> <p>We can not use NNEF shape_of that have been deprecated since 1.0.1 version:.</p> <pre><code>The shape_of function is deprecated and is discouraged from use.\nThe reason is that it provides syntactic means to access a\nproperty of tensors that is not defined via the syntax itself.\n\nFurthermore, its definition is problematic in cases where the shape\nof a tensor is not known in graph compilation time.\n\nThese result in problems with custom operations and operations with results\nof dynamic shape for a consumer of an NNEF document.\n\nBy removing support for the shape_of function from NNEF syntax,\nit becomes possible to de-couple parsing\nfrom shape propagation in a consumer of an NNEF document.\n</code></pre> <p>Since it is a core component to express some dynamic network that may use tract symbolic dimensions: for example using stream size to apply an averaging: We map it to <code>tract_core_shape_of</code></p>"},{"location":"reference/torch_to_nnef/op/aten/other/#torch_to_nnef.op.aten.other.to","title":"to","text":"<pre><code>to(g, node, name_to_tensor, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:to' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/other/#torch_to_nnef.op.aten.other.type_as","title":"type_as","text":"<pre><code>type_as(g, node, name_to_tensor, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:type_as' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/pad/","title":"torch_to_nnef.op.aten.pad","text":""},{"location":"reference/torch_to_nnef/op/aten/pad/#torch_to_nnef.op.aten.pad","title":"torch_to_nnef.op.aten.pad","text":""},{"location":"reference/torch_to_nnef/op/aten/pad/#torch_to_nnef.op.aten.pad.constant_pad_nd","title":"constant_pad_nd","text":"<pre><code>constant_pad_nd(g, node, name_to_tensor, torch_graph, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:constant_pad_{1,n}d' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/pad/#torch_to_nnef.op.aten.pad.pad","title":"pad","text":"<pre><code>pad(node, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:pad' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/pad/#torch_to_nnef.op.aten.pad.reflection_padnd","title":"reflection_padnd","text":"<pre><code>reflection_padnd(g, node, name_to_tensor, torch_graph, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:reflection_pad{1,2,3,n}d' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/pad/#torch_to_nnef.op.aten.pad.replication_padnd","title":"replication_padnd","text":"<pre><code>replication_padnd(g, node, name_to_tensor, torch_graph, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:replication_pad{1,2,3,n}d' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/pool/","title":"torch_to_nnef.op.aten.pool","text":""},{"location":"reference/torch_to_nnef/op/aten/pool/#torch_to_nnef.op.aten.pool","title":"torch_to_nnef.op.aten.pool","text":""},{"location":"reference/torch_to_nnef/op/aten/pool/#torch_to_nnef.op.aten.pool.adaptive_avg_poolnd","title":"adaptive_avg_poolnd","text":"<pre><code>adaptive_avg_poolnd(g, node, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:adaptive_avg_pool{1,2,3}d' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/pool/#torch_to_nnef.op.aten.pool.adaptive_max_poolnd","title":"adaptive_max_poolnd","text":"<pre><code>adaptive_max_poolnd(node, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch: adaptive_max_pool{1,2,3}d to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/pool/#torch_to_nnef.op.aten.pool.avg_pool1d","title":"avg_pool1d","text":"<pre><code>avg_pool1d(node, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:avg_pool1d' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/pool/#torch_to_nnef.op.aten.pool.avg_pool_nd","title":"avg_pool_nd","text":"<pre><code>avg_pool_nd(node, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:avg_pool(2|3)d', 'aten:max_pool3d' to NNEF.</p> <p>Cpp func parameters:. (const Tensor&amp; input, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional divisor_override <p>_pooling_op expect:</p> <p>(input_node, kernel_size_node, stride_node, padding_node, dilation_node, ceil_mode_node)</p>"},{"location":"reference/torch_to_nnef/op/aten/pool/#torch_to_nnef.op.aten.pool.max_pool1d","title":"max_pool1d","text":"<pre><code>max_pool1d(g, node, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:max_pool1d' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/pool/#torch_to_nnef.op.aten.pool.max_pool_nd","title":"max_pool_nd","text":"<pre><code>max_pool_nd(node, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:max_pool2d', 'aten:max_pool3d' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/pool/#torch_to_nnef.op.aten.pool.upsample_nearest2d","title":"upsample_nearest2d","text":"<pre><code>upsample_nearest2d(node, op_helper, **kwargs)\n</code></pre> <p>Operator mapping PyTorch: 'aten:upsample_nearest2d' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/qops/","title":"torch_to_nnef.op.aten.qops","text":""},{"location":"reference/torch_to_nnef/op/aten/qops/#torch_to_nnef.op.aten.qops","title":"torch_to_nnef.op.aten.qops","text":""},{"location":"reference/torch_to_nnef/op/aten/qops/#torch_to_nnef.op.aten.qops.dequantize","title":"dequantize","text":"<pre><code>dequantize(g, node, name_to_tensor, inference_target, **kwargs)\n</code></pre> <p>Translate <code>aten::dequantize</code> to NNEF.</p> <p>We will only handle the case of zero_point affine quantization for now.. which in reverse of quantization is:</p> <p>(x - zero_point) * scale</p>"},{"location":"reference/torch_to_nnef/op/aten/qops/#torch_to_nnef.op.aten.qops.quantize_per_tensor","title":"quantize_per_tensor","text":"<pre><code>quantize_per_tensor(g, node, name_to_tensor, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:quantize_per_tensor' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/reducer/","title":"torch_to_nnef.op.aten.reducer","text":""},{"location":"reference/torch_to_nnef/op/aten/reducer/#torch_to_nnef.op.aten.reducer","title":"torch_to_nnef.op.aten.reducer","text":""},{"location":"reference/torch_to_nnef/op/aten/reducer/#torch_to_nnef.op.aten.reducer.argmax","title":"argmax","text":"<pre><code>argmax(node, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:argmax' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/reducer/#torch_to_nnef.op.aten.reducer.argmin","title":"argmin","text":"<pre><code>argmin(node, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:argmin' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/reducer/#torch_to_nnef.op.aten.reducer.max_","title":"max_","text":"<pre><code>max_(node, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:max' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/reducer/#torch_to_nnef.op.aten.reducer.mean","title":"mean","text":"<pre><code>mean(node, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:mean' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/reducer/#torch_to_nnef.op.aten.reducer.min_","title":"min_","text":"<pre><code>min_(node, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:min' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/reducer/#torch_to_nnef.op.aten.reducer.prod","title":"prod","text":"<pre><code>prod(node, op_helper, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:prod' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/reducer/#torch_to_nnef.op.aten.reducer.reduce_all","title":"reduce_all","text":"<pre><code>reduce_all(node, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:reduce_all', 'aten:all' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/reducer/#torch_to_nnef.op.aten.reducer.reduce_any","title":"reduce_any","text":"<pre><code>reduce_any(node, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:reduce_any', 'aten:any' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/reducer/#torch_to_nnef.op.aten.reducer.reduce_max","title":"reduce_max","text":"<pre><code>reduce_max(node, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:reduce_max', 'aten:amax' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/reducer/#torch_to_nnef.op.aten.reducer.reduce_min","title":"reduce_min","text":"<pre><code>reduce_min(node, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:reduce_min', 'aten:amin' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/reducer/#torch_to_nnef.op.aten.reducer.reduce_sum","title":"reduce_sum","text":"<pre><code>reduce_sum(node, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:reduce_sum', 'aten:sum' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/selector/","title":"torch_to_nnef.op.aten.selector","text":""},{"location":"reference/torch_to_nnef/op/aten/selector/#torch_to_nnef.op.aten.selector","title":"torch_to_nnef.op.aten.selector","text":""},{"location":"reference/torch_to_nnef/op/aten/selector/#torch_to_nnef.op.aten.selector.argsort","title":"argsort","text":"<pre><code>argsort(node, op_helper, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:argsort' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/selector/#torch_to_nnef.op.aten.selector.embedding","title":"embedding","text":"<pre><code>embedding(node, op_helper, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:embedding' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/selector/#torch_to_nnef.op.aten.selector.gather","title":"gather","text":"<pre><code>gather(node, op_helper, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:gather' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/selector/#torch_to_nnef.op.aten.selector.index_","title":"index_","text":"<pre><code>index_(node, op_helper, inference_target, **kwargs)\n</code></pre> <p>Translate <code>aten::index</code> to NNEF.</p> <p>Fragment gather&lt;?&gt;(.     input: tensor&lt;?&gt;,                 # the tensor to gather from     indices: tensor,         # the indices to gather at     axis: integer = 0 )               # the axis to gather at -&gt; ( output: tensor&lt;?&gt; ) <p>torch ir, in this case structure <code>indexes_node</code> with: a list of n values where n &lt;= input_node rank each value is either a constant or a tensor. if the constant is None this means the full dimension</p>"},{"location":"reference/torch_to_nnef/op/aten/selector/#torch_to_nnef.op.aten.selector.index_select","title":"index_select","text":"<pre><code>index_select(node, op_helper, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:index_select' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/selector/#torch_to_nnef.op.aten.selector.masked_fill","title":"masked_fill","text":"<pre><code>masked_fill(node, op_helper, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:masked_fill' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/selector/#torch_to_nnef.op.aten.selector.narrow","title":"narrow","text":"<pre><code>narrow(node, op_helper, **kwargs)\n</code></pre> <p>Fancy slice made in PyTorch.</p> <p>torch.narrow(input, dim, start, length)</p> <p>Example:</p> <p>import torch x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) torch.narrow(x, 0, 0, 2) tensor([[1, 2, 3],         [4, 5, 6]])</p>"},{"location":"reference/torch_to_nnef/op/aten/selector/#torch_to_nnef.op.aten.selector.scatter","title":"scatter","text":"<pre><code>scatter(node, op_helper, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:scatter' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/selector/#torch_to_nnef.op.aten.selector.select","title":"select","text":"<pre><code>select(node, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:select' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/selector/#torch_to_nnef.op.aten.selector.slice_","title":"slice_","text":"<pre><code>slice_(node, torch_graph, inference_target, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:slice' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/selector/#torch_to_nnef.op.aten.selector.sort","title":"sort","text":"<pre><code>sort(node, op_helper, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:sort' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/selector/#torch_to_nnef.op.aten.selector.topk","title":"topk","text":"<pre><code>topk(node, op_helper, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:topk' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/selector/#torch_to_nnef.op.aten.selector.tract_pre_0_21_7_slice","title":"tract_pre_0_21_7_slice","text":"<pre><code>tract_pre_0_21_7_slice(node, torch_graph, nnef_spec_strict, has_dynamic_axes, op_helper, **kwargs)\n</code></pre> <p>Old version of slice for tract version prior to 0.21.7.</p>"},{"location":"reference/torch_to_nnef/op/aten/selector/#torch_to_nnef.op.aten.selector.where","title":"where","text":"<pre><code>where(node, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:where' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/split/","title":"torch_to_nnef.op.aten.split","text":""},{"location":"reference/torch_to_nnef/op/aten/split/#torch_to_nnef.op.aten.split","title":"torch_to_nnef.op.aten.split","text":""},{"location":"reference/torch_to_nnef/op/aten/split/#torch_to_nnef.op.aten.split.chunk","title":"chunk","text":"<pre><code>chunk(g, node, name_to_tensor, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:chunk' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/split/#torch_to_nnef.op.aten.split.split_with_sizes","title":"split_with_sizes","text":"<pre><code>split_with_sizes(g, node, name_to_tensor, **kwargs)\n</code></pre> <p>Translate <code>aten::split_with_sizes</code> to NNEF.</p> <p>We are aware that. split&lt;?&gt;(     value: tensor&lt;?&gt;,     axis: integer,     ratios: integer[] ) -&gt; ( values: tensor&lt;?&gt;[] )</p> <p>exists but since tract does not support it, we reexpress it with slice</p>"},{"location":"reference/torch_to_nnef/op/aten/split/#torch_to_nnef.op.aten.split.unbind","title":"unbind","text":"<pre><code>unbind(g, node, name_to_tensor, **kwargs)\n</code></pre> <p>Unbind is <code>unstack</code> in NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/tensor_build/","title":"torch_to_nnef.op.aten.tensor_build","text":""},{"location":"reference/torch_to_nnef/op/aten/tensor_build/#torch_to_nnef.op.aten.tensor_build","title":"torch_to_nnef.op.aten.tensor_build","text":""},{"location":"reference/torch_to_nnef/op/aten/tensor_build/#torch_to_nnef.op.aten.tensor_build.arange","title":"arange","text":"<pre><code>arange(g, node, name_to_tensor, inference_target, **kwargs)\n</code></pre> <p>This operator can not be exactly exported to NNEF.</p> <p>In general NNEF spec is against dynamism it could provide so</p> <p>we implement it as a simple constant variable.</p>"},{"location":"reference/torch_to_nnef/op/aten/tensor_build/#torch_to_nnef.op.aten.tensor_build.copy","title":"copy","text":"<pre><code>copy(g, node, name_to_tensor, inference_target, torch_graph, null_ref, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:copy', 'aten:clone' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/tensor_build/#torch_to_nnef.op.aten.tensor_build.empty_like","title":"empty_like","text":"<pre><code>empty_like(**kwargs)\n</code></pre> <p>Operator can not be exactly exported to NNEF if dynamic.</p> <p>With tract we use use expansion</p>"},{"location":"reference/torch_to_nnef/op/aten/tensor_build/#torch_to_nnef.op.aten.tensor_build.fill","title":"fill","text":"<pre><code>fill(g, node, name_to_tensor, torch_graph, inference_target, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:fill', 'aten:fill_' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/tensor_build/#torch_to_nnef.op.aten.tensor_build.full","title":"full","text":"<pre><code>full(g, node, name_to_tensor, torch_graph, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:full' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/tensor_build/#torch_to_nnef.op.aten.tensor_build.full_like","title":"full_like","text":"<pre><code>full_like(**kwargs)\n</code></pre> <p>Operator can not be exactly exported to NNEF if dynamic.</p> <p>With tract we use use expansion</p>"},{"location":"reference/torch_to_nnef/op/aten/tensor_build/#torch_to_nnef.op.aten.tensor_build.new_zeros","title":"new_zeros","text":"<pre><code>new_zeros(g, node, name_to_tensor, torch_graph, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:new_zeros' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/tensor_build/#torch_to_nnef.op.aten.tensor_build.ones","title":"ones","text":"<pre><code>ones(g, node, name_to_tensor, torch_graph, inference_target, **kwargs)\n</code></pre> <p>This operator can not be exactly exported to NNEF.</p> <p>In general NNEF spec is against dynamism it could provide so</p> <p>we implement it as a simple constant variable.</p>"},{"location":"reference/torch_to_nnef/op/aten/tensor_build/#torch_to_nnef.op.aten.tensor_build.ones_like","title":"ones_like","text":"<pre><code>ones_like(**kwargs)\n</code></pre> <p>Operator can not be exactly exported to NNEF if dynamic.</p> <p>With tract we use use expansion</p>"},{"location":"reference/torch_to_nnef/op/aten/tensor_build/#torch_to_nnef.op.aten.tensor_build.tril","title":"tril","text":"<pre><code>tril(g, node, name_to_tensor, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:tril' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/tensor_build/#torch_to_nnef.op.aten.tensor_build.triu","title":"triu","text":"<pre><code>triu(g, node, name_to_tensor, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:triu' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/tensor_build/#torch_to_nnef.op.aten.tensor_build.zeros","title":"zeros","text":"<pre><code>zeros(g, node, name_to_tensor, torch_graph, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'aten:zeros' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/aten/tensor_build/#torch_to_nnef.op.aten.tensor_build.zeros_like","title":"zeros_like","text":"<pre><code>zeros_like(**kwargs)\n</code></pre> <p>Operator can not be exactly exported to NNEF if dynamic.</p> <p>With tract we use use exapnsion</p>"},{"location":"reference/torch_to_nnef/op/aten/unary/","title":"torch_to_nnef.op.aten.unary","text":""},{"location":"reference/torch_to_nnef/op/aten/unary/#torch_to_nnef.op.aten.unary","title":"torch_to_nnef.op.aten.unary","text":""},{"location":"reference/torch_to_nnef/op/aten/unary/#torch_to_nnef.op.aten.unary.generic_unary","title":"generic_unary","text":"<pre><code>generic_unary(aten_op_id, node, op_helper, **kwargs)\n</code></pre> <p>Map PyTorch generic operators to NNEF (direct map).</p> List is <p>'aten:relu', 'aten:sigmoid', 'aten:log', 'aten:exp', 'aten:sin', 'aten:cos', 'aten:tan', 'aten:asin', 'aten:acos', 'aten:atan', 'aten:sinh', 'aten:cosh', 'aten:tanh', 'aten:asinh', 'aten:acosh', 'aten:atanh', 'aten:sign', 'aten:neg', 'aten:floor', 'aten:ceil', 'aten:sqrt', 'aten:rsqrt', 'aten:log2', 'aten:rcp', 'aten:not', 'aten:eq', 'aten:ne', 'aten:add', 'aten:sub', 'aten:lt', 'aten:gt', 'aten:le', 'aten:ge', 'aten:and', 'aten:or', 'aten:and', 'aten:or', 'aten:_relu', 'aten:greater', 'aten:greater_equal', 'aten:less', 'aten:less_equal', 'aten:logical_not', 'aten:logical_and', 'aten:logical_or', 'aten:reciprocal', 'aten:minimum', 'aten:maximum'</p>"},{"location":"reference/torch_to_nnef/op/custom_extractors/","title":"torch_to_nnef.op.custom_extractors","text":""},{"location":"reference/torch_to_nnef/op/custom_extractors/#torch_to_nnef.op.custom_extractors","title":"torch_to_nnef.op.custom_extractors","text":"<p><code>op.custom_extractors</code> provides mechanism to control extraction to NNEF.</p> <p>while bypassing PyTorch full expansion of <code>torch.Module</code> within <code>torch_graph</code> which by default use torch.jit.trace .</p> This may be for two main reasons <ul> <li>Some layer such as LSTM/GRU have complex expension which are better   handled by encapsulation instead of spreading high number of variable</li> <li>Some layer might not be serializable to .jit</li> <li>There might be some edge case where you prefer to keep full control on   exported NNEF subgraph.</li> </ul>"},{"location":"reference/torch_to_nnef/op/custom_extractors/#torch_to_nnef.op.custom_extractors.LSTMExtractor","title":"LSTMExtractor","text":"<pre><code>LSTMExtractor()\n</code></pre> <p>               Bases: <code>_RNNMixin</code>, <code>ModuleInfoExtractor</code></p>"},{"location":"reference/torch_to_nnef/op/custom_extractors/#torch_to_nnef.op.custom_extractors.ModuleInfoExtractor","title":"ModuleInfoExtractor","text":"<pre><code>ModuleInfoExtractor()\n</code></pre> <p>Class to take manual control of NNEF expansion of a nn.Module.</p> <p>You need to subclass it, and set MODULE_CLASS according to your targeted module.</p> <p>Then write .convert_to_nnef according to your need.</p>"},{"location":"reference/torch_to_nnef/op/custom_extractors/#torch_to_nnef.op.custom_extractors.ModuleInfoExtractor.convert_to_nnef","title":"convert_to_nnef","text":"<pre><code>convert_to_nnef(g, node, name_to_tensor, null_ref, torch_graph, inference_target, **kwargs)\n</code></pre> <p>Control NNEF content to be written for each MODULE_CLASS.</p> <p>This happen at macro level when converting from internal IR to NNEF IR stage.</p> <p>This is the Core method to overwrite in subclass.</p> <p>It is no different than any op implemented in <code>torch_to_nnef</code> in the module</p>"},{"location":"reference/torch_to_nnef/op/custom_extractors/#torch_to_nnef.op.custom_extractors.ModuleInfoExtractor.generate_in_torch_graph","title":"generate_in_torch_graph","text":"<pre><code>generate_in_torch_graph(torch_graph, *args, **kwargs)\n</code></pre> <p>Internal method called by torch_to_nnef ir_graph.</p>"},{"location":"reference/torch_to_nnef/op/custom_extractors/#torch_to_nnef.op.custom_extractors.ModuleInfoExtractor.get_by_kind","title":"get_by_kind  <code>classmethod</code>","text":"<pre><code>get_by_kind(kind: str)\n</code></pre> <p>Get ModuleInfoExtractor by kind in torch_to_nnef internal IR.</p>"},{"location":"reference/torch_to_nnef/op/custom_extractors/#torch_to_nnef.op.custom_extractors.ModuleInfoExtractor.get_by_module","title":"get_by_module  <code>classmethod</code>","text":"<pre><code>get_by_module(module: nn.Module)\n</code></pre> <p>Search if the module is one of the MODULE_CLASS registered.</p> <p>return appropriate ModuleInfoExtractor subclass if found</p>"},{"location":"reference/torch_to_nnef/op/custom_extractors/#torch_to_nnef.op.custom_extractors.ModuleInfoExtractor.ordered_args","title":"ordered_args","text":"<pre><code>ordered_args(torch_graph)\n</code></pre> <p>Odered args for the module call.</p> <p>Sometimes torch jit may reorder inputs. compared to targeted python ops in such case ordering need to be re-addressed</p>"},{"location":"reference/torch_to_nnef/op/custom_extractors/base/","title":"torch_to_nnef.op.custom_extractors.base","text":""},{"location":"reference/torch_to_nnef/op/custom_extractors/base/#torch_to_nnef.op.custom_extractors.base","title":"torch_to_nnef.op.custom_extractors.base","text":""},{"location":"reference/torch_to_nnef/op/custom_extractors/base/#torch_to_nnef.op.custom_extractors.base.ModuleInfoExtractor","title":"ModuleInfoExtractor","text":"<pre><code>ModuleInfoExtractor()\n</code></pre> <p>Class to take manual control of NNEF expansion of a nn.Module.</p> <p>You need to subclass it, and set MODULE_CLASS according to your targeted module.</p> <p>Then write .convert_to_nnef according to your need.</p>"},{"location":"reference/torch_to_nnef/op/custom_extractors/base/#torch_to_nnef.op.custom_extractors.base.ModuleInfoExtractor.convert_to_nnef","title":"convert_to_nnef","text":"<pre><code>convert_to_nnef(g, node, name_to_tensor, null_ref, torch_graph, inference_target, **kwargs)\n</code></pre> <p>Control NNEF content to be written for each MODULE_CLASS.</p> <p>This happen at macro level when converting from internal IR to NNEF IR stage.</p> <p>This is the Core method to overwrite in subclass.</p> <p>It is no different than any op implemented in <code>torch_to_nnef</code> in the module</p>"},{"location":"reference/torch_to_nnef/op/custom_extractors/base/#torch_to_nnef.op.custom_extractors.base.ModuleInfoExtractor.generate_in_torch_graph","title":"generate_in_torch_graph","text":"<pre><code>generate_in_torch_graph(torch_graph, *args, **kwargs)\n</code></pre> <p>Internal method called by torch_to_nnef ir_graph.</p>"},{"location":"reference/torch_to_nnef/op/custom_extractors/base/#torch_to_nnef.op.custom_extractors.base.ModuleInfoExtractor.get_by_kind","title":"get_by_kind  <code>classmethod</code>","text":"<pre><code>get_by_kind(kind: str)\n</code></pre> <p>Get ModuleInfoExtractor by kind in torch_to_nnef internal IR.</p>"},{"location":"reference/torch_to_nnef/op/custom_extractors/base/#torch_to_nnef.op.custom_extractors.base.ModuleInfoExtractor.get_by_module","title":"get_by_module  <code>classmethod</code>","text":"<pre><code>get_by_module(module: nn.Module)\n</code></pre> <p>Search if the module is one of the MODULE_CLASS registered.</p> <p>return appropriate ModuleInfoExtractor subclass if found</p>"},{"location":"reference/torch_to_nnef/op/custom_extractors/base/#torch_to_nnef.op.custom_extractors.base.ModuleInfoExtractor.ordered_args","title":"ordered_args","text":"<pre><code>ordered_args(torch_graph)\n</code></pre> <p>Odered args for the module call.</p> <p>Sometimes torch jit may reorder inputs. compared to targeted python ops in such case ordering need to be re-addressed</p>"},{"location":"reference/torch_to_nnef/op/custom_extractors/rnn/","title":"torch_to_nnef.op.custom_extractors.rnn","text":""},{"location":"reference/torch_to_nnef/op/custom_extractors/rnn/#torch_to_nnef.op.custom_extractors.rnn","title":"torch_to_nnef.op.custom_extractors.rnn","text":""},{"location":"reference/torch_to_nnef/op/custom_extractors/rnn/#torch_to_nnef.op.custom_extractors.rnn.LSTMExtractor","title":"LSTMExtractor","text":"<pre><code>LSTMExtractor()\n</code></pre> <p>               Bases: <code>_RNNMixin</code>, <code>ModuleInfoExtractor</code></p>"},{"location":"reference/torch_to_nnef/op/fragment/","title":"torch_to_nnef.op.fragment","text":""},{"location":"reference/torch_to_nnef/op/fragment/#torch_to_nnef.op.fragment","title":"torch_to_nnef.op.fragment","text":"<p>List of fragment directly written in NNEF to allow composition.</p> <p>Most reimplement aten:: operators.</p>"},{"location":"reference/torch_to_nnef/op/fragment/#torch_to_nnef.op.fragment.Fragment","title":"Fragment  <code>dataclass</code>","text":"<pre><code>Fragment(name: str, extensions: T.Tuple[str, ...], definition: str)\n</code></pre> <p>Extract definitions and extensions from our custom NNEF files.</p>"},{"location":"reference/torch_to_nnef/op/helper/","title":"torch_to_nnef.op.helper","text":""},{"location":"reference/torch_to_nnef/op/helper/#torch_to_nnef.op.helper","title":"torch_to_nnef.op.helper","text":""},{"location":"reference/torch_to_nnef/op/helper/#torch_to_nnef.op.helper.OpHelper","title":"OpHelper","text":"<pre><code>OpHelper(g, node, name_to_tensor, null_ref, inference_target)\n</code></pre>"},{"location":"reference/torch_to_nnef/op/helper/#torch_to_nnef.op.helper.OpHelper.add_single_output_op_from_ir_datas","title":"add_single_output_op_from_ir_datas","text":"<pre><code>add_single_output_op_from_ir_datas(nnef_op_type: str, input_nodes: T.List[Data], output_tensor_name_suffix: str = '', force_full_output_tensor_name: str = '', reuse_if_name_exists: bool = False, **kwargs) -&gt; TorchOp\n</code></pre> <p>Use input_nodes Data instead of nnef.Tensor.</p> <p>Also nnefe</p>"},{"location":"reference/torch_to_nnef/op/helper/#torch_to_nnef.op.helper.OpRegistry","title":"OpRegistry","text":"<pre><code>OpRegistry(torch_mod_id: str)\n</code></pre>"},{"location":"reference/torch_to_nnef/op/helper/#torch_to_nnef.op.helper.OpRegistry.register","title":"register","text":"<pre><code>register(torch_op_ids: T.Optional[T.List[str]] = None)\n</code></pre> <p>By default we take the name of the function if not specified.</p>"},{"location":"reference/torch_to_nnef/op/helper/#torch_to_nnef.op.helper.add_tensor_variable_node_as_nnef_tensor","title":"add_tensor_variable_node_as_nnef_tensor","text":"<pre><code>add_tensor_variable_node_as_nnef_tensor(g: NGraph, node: TensorVariable, name_to_tensor: T.Dict[str, NTensor], name_suffix: str = '', prevent_variable: bool = False, force_full_output_tensor_name: T.Optional[str] = None) -&gt; NTensor\n</code></pre> <p>Create NNEF tensor and register in graph from torch_graph.Data node.</p> <p>It automatically adds variable if node is a torch tensor is associated (it avoids bloating nnef graph file with matrix values)</p>"},{"location":"reference/torch_to_nnef/op/helper/#torch_to_nnef.op.helper.cast_and_add_nnef_operation","title":"cast_and_add_nnef_operation","text":"<pre><code>cast_and_add_nnef_operation(name_to_tensor: str, **kwargs)\n</code></pre> <p>Ensure to cast parameters before adding operation to NNEF graph.</p>"},{"location":"reference/torch_to_nnef/op/helper/#torch_to_nnef.op.helper.cast_inputs_and_attrs","title":"cast_inputs_and_attrs","text":"<pre><code>cast_inputs_and_attrs(inputs, attrs, g, name_to_tensor)\n</code></pre> <p>Catch input or attr that would still be torch_graph values into NNEF.</p>"},{"location":"reference/torch_to_nnef/op/helper/#torch_to_nnef.op.helper.cast_to_if_not_dtype_and_variable","title":"cast_to_if_not_dtype_and_variable","text":"<pre><code>cast_to_if_not_dtype_and_variable(g, name_to_tensor, node, nnef_tensor: NTensor, cast_to: np.dtype, suffix: str = '')\n</code></pre> <p>Force casting not expressed in IR graph in case of div for example.</p> <p>This is neccessary since tract and maybe other inference engine may not cast implicitly to float during div operation for example leading to rounding issues.</p>"},{"location":"reference/torch_to_nnef/op/helper/#torch_to_nnef.op.helper.maybe_align_inputs_ranks","title":"maybe_align_inputs_ranks","text":"<pre><code>maybe_align_inputs_ranks(g: NGraph, inputs: T.Sequence[NTensor], outputs: T.Sequence[NTensor], op_type: str) -&gt; T.Sequence[NTensor]\n</code></pre> <p>Ensure consistent rank between inputs and outputs with regard to spec.</p> <ul> <li>May unsqueeze at 0 rank n time to align inputs</li> </ul> <p>This is done at export time and not inference time because: - inference implementation may use 1 dim expansion from left to right like Tract or Tensorflow instead of PyTorch expansion which happen in opposite direction.</p>"},{"location":"reference/torch_to_nnef/op/helper/#torch_to_nnef.op.helper.pick_axis","title":"pick_axis","text":"<pre><code>pick_axis(input_node, rank: int) -&gt; int\n</code></pre> <p>Enforce that axis, axes ect does contains only positive values.</p>"},{"location":"reference/torch_to_nnef/op/helper/#torch_to_nnef.op.helper.pick_index_in_axis","title":"pick_index_in_axis","text":"<pre><code>pick_index_in_axis(input_node, rank: int, index: int, check_is_positive: bool = True) -&gt; int\n</code></pre> <p>Enforce that index in axis does contains only values within bounds.</p> <p>Because in case of tract out of bound is not supported !</p>"},{"location":"reference/torch_to_nnef/op/quantized/","title":"torch_to_nnef.op.quantized","text":""},{"location":"reference/torch_to_nnef/op/quantized/#torch_to_nnef.op.quantized","title":"torch_to_nnef.op.quantized","text":"<p>PyTorch quantized::* operators translation.</p> <p>Quantized layers and primitives</p> Maybe usefull when looking at X <p>packed_params._method_names()</p>"},{"location":"reference/torch_to_nnef/op/quantized/#torch_to_nnef.op.quantized.add","title":"add","text":"<pre><code>add(**kwargs)\n</code></pre> <p>Map PyTorch: 'quantized:add' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/quantized/#torch_to_nnef.op.quantized.add_relu","title":"add_relu","text":"<pre><code>add_relu(g, node, name_to_tensor, null_ref, **kwargs)\n</code></pre> <p>Map PyTorch: 'quantized:add_relu' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/quantized/#torch_to_nnef.op.quantized.conv1d","title":"conv1d","text":"<pre><code>conv1d(g, node, name_to_tensor, null_ref, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'quantized:conv1d' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/quantized/#torch_to_nnef.op.quantized.conv1d_relu","title":"conv1d_relu","text":"<pre><code>conv1d_relu(g, node, name_to_tensor, inference_target, null_ref, **kwargs)\n</code></pre> <p>Map PyTorch: 'quantized:conv1d_relu' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/quantized/#torch_to_nnef.op.quantized.conv2d","title":"conv2d","text":"<pre><code>conv2d(g, node, name_to_tensor, null_ref, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'quantized:conv2d' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/quantized/#torch_to_nnef.op.quantized.conv2d_relu","title":"conv2d_relu","text":"<pre><code>conv2d_relu(g, node, name_to_tensor, null_ref, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'quantized:conv2d_relu' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/quantized/#torch_to_nnef.op.quantized.div","title":"div","text":"<pre><code>div(**kwargs)\n</code></pre> <p>Map PyTorch: 'quantized:div' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/quantized/#torch_to_nnef.op.quantized.linear","title":"linear","text":"<pre><code>linear(g, node, name_to_tensor, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'quantized:linear' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/quantized/#torch_to_nnef.op.quantized.linear_relu","title":"linear_relu","text":"<pre><code>linear_relu(g, node, name_to_tensor, inference_target, **kwargs)\n</code></pre> <p>Map PyTorch: 'quantized:linear_relu' to NNEF.</p>"},{"location":"reference/torch_to_nnef/op/quantized/#torch_to_nnef.op.quantized.mul","title":"mul","text":"<pre><code>mul(**kwargs)\n</code></pre> <p>Map PyTorch: 'quantized:mul' to NNEF.</p>"},{"location":"reference/torch_to_nnef/tensor/","title":"torch_to_nnef.tensor","text":""},{"location":"reference/torch_to_nnef/tensor/#torch_to_nnef.tensor","title":"torch_to_nnef.tensor","text":""},{"location":"reference/torch_to_nnef/tensor/#torch_to_nnef.tensor.NamedTensor","title":"NamedTensor","text":"<pre><code>NamedTensor(fp_tensor: torch.Tensor, nnef_name: str)\n</code></pre> <p>               Bases: <code>Tensor</code></p> <p>Tensor enriched with name attribute.</p>"},{"location":"reference/torch_to_nnef/tensor/#torch_to_nnef.tensor.NamedTensor.data","title":"data  <code>property</code> <code>writable</code>","text":"<pre><code>data\n</code></pre> <p>Very important to keep access to all special attr of NamedTensor.</p>"},{"location":"reference/torch_to_nnef/tensor/#torch_to_nnef.tensor.OffloadedTensor","title":"OffloadedTensor","text":"<pre><code>OffloadedTensor(elem, device, offload_dir: Path, name: str, offloaded_tensor_type: T.Type[torch.Tensor], force_gc_collect: bool = False)\n</code></pre> <p>               Bases: <code>OpaqueTensor</code></p> <p>Tensor subclass that maintains data on disk.</p> <p>It hold an virtual internal memory storage (permanent) and a temporary instantiation at each operation accessing it on targeted device.</p> Warning <p>we recommend to version of PyTorch &gt; 1.12 for best compatibility.</p>"},{"location":"reference/torch_to_nnef/tensor/#torch_to_nnef.tensor.OffloadedTensor.is_meta","title":"is_meta  <code>property</code>","text":"<pre><code>is_meta: bool\n</code></pre> <p>Whether the tensor is on the meta device.</p> <p>Always False as the tensor is (off|re)loaded from disk.</p>"},{"location":"reference/torch_to_nnef/tensor/#torch_to_nnef.tensor.OffloadedTensor.from_original_tensor","title":"from_original_tensor  <code>classmethod</code>","text":"<pre><code>from_original_tensor(tensor: torch.Tensor, name: str, offload_dir: T.Optional[Path] = None, suffix_log_msg: str = '')\n</code></pre> <p>Take a torch.Tensor or OpaqueTensor and offload it to disk.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>the torch.Tensor or torch_to_nnef.tensor.OpaqueTensor to dump on disk</p> required <code>name</code> <code>str</code> <p>the name of the tensor that will be used to create the filename store on disk</p> required <code>offload_dir</code> <code>Optional[Path]</code> <p>The directory where this file will be stored (temporarly)</p> <code>None</code> <code>suffix_log_msg</code> <code>str</code> <p>Added message log suffix for context</p> <code>''</code>"},{"location":"reference/torch_to_nnef/tensor/#torch_to_nnef.tensor.OffloadedTensor.to","title":"to","text":"<pre><code>to(*args, **kwargs)\n</code></pre> <p>Change the target device when reloaded in memory.</p>"},{"location":"reference/torch_to_nnef/tensor/#torch_to_nnef.tensor.OffloadedTensor.update_values","title":"update_values","text":"<pre><code>update_values(values: torch.Tensor, strict_shape: bool = True, strict_dtype: bool = True)\n</code></pre> <p>Replace offloaded tensor by new 'values' tensor.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Tensor</code> <p>The tensor that will replace it on disk assertion are made to ensure same shape, dtype as prior</p> required <code>strict_shape</code> <code>bool</code> <p>if True (default) the shape of the new tensor must be the same as the prior one</p> <code>True</code> <code>strict_dtype</code> <code>bool</code> <p>if True (default) the dtype of the new tensor must be the same as the prior one</p> <code>True</code>"},{"location":"reference/torch_to_nnef/tensor/#torch_to_nnef.tensor.OpaqueTensorRef","title":"OpaqueTensorRef","text":"<pre><code>OpaqueTensorRef(meta_tensor: torch.Tensor, opaque_tensor: OpaqueTensor)\n</code></pre> <p>               Bases: <code>Tensor</code></p> <p>Allow to pass through 'tracing'.</p>"},{"location":"reference/torch_to_nnef/tensor/#torch_to_nnef.tensor.QScalePerGroupF16","title":"QScalePerGroupF16","text":"<pre><code>QScalePerGroupF16(group_size: int, scale: torch.Tensor, n_bits: int)\n</code></pre> <p>               Bases: <code>QScheme</code></p> <p>f16 scale only per group.</p> <p>Tract aligned using negative scales.</p>"},{"location":"reference/torch_to_nnef/tensor/#torch_to_nnef.tensor.QTensor","title":"QTensor","text":"<pre><code>QTensor(fp_tensor: torch.Tensor, qscheme: QScheme, dequant_to_dtype=torch.float32, u8_compressors: T.Optional[T.List[U8Compressor]] = None)\n</code></pre> <p>               Bases: <code>OpaqueTensor</code></p> <p>Common interface for all Compressed storage.</p>"},{"location":"reference/torch_to_nnef/tensor/#torch_to_nnef.tensor.QTensor.to_device","title":"to_device","text":"<pre><code>to_device(new_device)\n</code></pre> <p>Specific device handling.</p>"},{"location":"reference/torch_to_nnef/tensor/#torch_to_nnef.tensor.QTensor.write_in_file","title":"write_in_file","text":"<pre><code>write_in_file(dirpath: T.Union[str, Path], label: str)\n</code></pre> <p>Called at NNEF write time.</p> <p>Each specific inference engine format should implement the file dump prefered.</p>"},{"location":"reference/torch_to_nnef/tensor/#torch_to_nnef.tensor.QTensorTractScaleOnly","title":"QTensorTractScaleOnly","text":"<pre><code>QTensorTractScaleOnly(*args, specific_machine: T.Optional[str] = None, **kwargs)\n</code></pre> <p>               Bases: <code>QTensorTract</code></p> <p>Tract data format it serializes to: Q4_0.</p>"},{"location":"reference/torch_to_nnef/tensor/#torch_to_nnef.tensor.QTensorTractScaleOnly.decompress","title":"decompress","text":"<pre><code>decompress()\n</code></pre> <p>Tract dequantization depends on hardware.</p> <p>Typically dequantization happen with ops in f16 on ARM and f32 (scale directly casted) on others so we overwrite the function to be consistant with tract.</p>"},{"location":"reference/torch_to_nnef/tensor/#torch_to_nnef.tensor.apply_name_to_tensor_in_module","title":"apply_name_to_tensor_in_module","text":"<pre><code>apply_name_to_tensor_in_module(model: torch.nn.Module)\n</code></pre> <p>Transform torch.Tensor or Parameters into NamedTensor.</p> <p>This is applied at export time of <code>torch_to_nnef</code> Just before doing any tracing and allow to keep variable naming identical to PyTorch one</p> <p>This consistent naming unlock subsequent manipulations such as LORA applications @ inference or such.</p>"},{"location":"reference/torch_to_nnef/tensor/#torch_to_nnef.tensor.set_opaque_tensor_in_params_as_ref","title":"set_opaque_tensor_in_params_as_ref","text":"<pre><code>set_opaque_tensor_in_params_as_ref(model: torch.nn.Module)\n</code></pre> <p>Transform OpaqueTensor Parameters into OpaqueTensorRef.</p> <p>This is applied at export time of <code>torch_to_nnef</code> Just before doing any tracing</p>"},{"location":"reference/torch_to_nnef/tensor/named/","title":"torch_to_nnef.tensor.named","text":""},{"location":"reference/torch_to_nnef/tensor/named/#torch_to_nnef.tensor.named","title":"torch_to_nnef.tensor.named","text":""},{"location":"reference/torch_to_nnef/tensor/named/#torch_to_nnef.tensor.named.NamedTensor","title":"NamedTensor","text":"<pre><code>NamedTensor(fp_tensor: torch.Tensor, nnef_name: str)\n</code></pre> <p>               Bases: <code>Tensor</code></p> <p>Tensor enriched with name attribute.</p>"},{"location":"reference/torch_to_nnef/tensor/named/#torch_to_nnef.tensor.named.NamedTensor.data","title":"data  <code>property</code> <code>writable</code>","text":"<pre><code>data\n</code></pre> <p>Very important to keep access to all special attr of NamedTensor.</p>"},{"location":"reference/torch_to_nnef/tensor/named/#torch_to_nnef.tensor.named.apply_name_to_tensor_in_module","title":"apply_name_to_tensor_in_module","text":"<pre><code>apply_name_to_tensor_in_module(model: torch.nn.Module)\n</code></pre> <p>Transform torch.Tensor or Parameters into NamedTensor.</p> <p>This is applied at export time of <code>torch_to_nnef</code> Just before doing any tracing and allow to keep variable naming identical to PyTorch one</p> <p>This consistent naming unlock subsequent manipulations such as LORA applications @ inference or such.</p>"},{"location":"reference/torch_to_nnef/tensor/offload/","title":"torch_to_nnef.tensor.offload","text":""},{"location":"reference/torch_to_nnef/tensor/offload/#torch_to_nnef.tensor.offload","title":"torch_to_nnef.tensor.offload","text":"<p>OffLoad Tensor.</p> <p>Tensor subclass to work around memories limit on various devices by offloading on disk or on a different 'memory' than final one.</p> <p>It holds an internal memory storage (permanent) and a temporary     instantiation at each operation accessing it on targeted device.</p>"},{"location":"reference/torch_to_nnef/tensor/offload/#torch_to_nnef.tensor.offload--huggingface-accelerate-difference","title":"HuggingFace 'accelerate' difference","text":"<p>This is different than HuggingFace 'accelerate' that would spread once the layout of your network accross the different devices available, but preventing to move data to other device afterward.</p> <p>Indeed we use the torch \"Tensor\" API instead of the torch.device(\"meta\") allowing to hold more informations such as the final targeted device (and other stuff).</p> <p>This avoid us to have any need for the Hooking system done in accelerate, and skip need to align data flow graph by pre&amp;post casting.</p> <p>In short it is transparent for end-user that can use those like read-only device movable tensors (mutation support could be envisioned if needed).</p>"},{"location":"reference/torch_to_nnef/tensor/offload/#torch_to_nnef.tensor.offload.OffloadedTensor","title":"OffloadedTensor","text":"<pre><code>OffloadedTensor(elem, device, offload_dir: Path, name: str, offloaded_tensor_type: T.Type[torch.Tensor], force_gc_collect: bool = False)\n</code></pre> <p>               Bases: <code>OpaqueTensor</code></p> <p>Tensor subclass that maintains data on disk.</p> <p>It hold an virtual internal memory storage (permanent) and a temporary instantiation at each operation accessing it on targeted device.</p> Warning <p>we recommend to version of PyTorch &gt; 1.12 for best compatibility.</p>"},{"location":"reference/torch_to_nnef/tensor/offload/#torch_to_nnef.tensor.offload.OffloadedTensor.is_meta","title":"is_meta  <code>property</code>","text":"<pre><code>is_meta: bool\n</code></pre> <p>Whether the tensor is on the meta device.</p> <p>Always False as the tensor is (off|re)loaded from disk.</p>"},{"location":"reference/torch_to_nnef/tensor/offload/#torch_to_nnef.tensor.offload.OffloadedTensor.from_original_tensor","title":"from_original_tensor  <code>classmethod</code>","text":"<pre><code>from_original_tensor(tensor: torch.Tensor, name: str, offload_dir: T.Optional[Path] = None, suffix_log_msg: str = '')\n</code></pre> <p>Take a torch.Tensor or OpaqueTensor and offload it to disk.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>the torch.Tensor or torch_to_nnef.tensor.OpaqueTensor to dump on disk</p> required <code>name</code> <code>str</code> <p>the name of the tensor that will be used to create the filename store on disk</p> required <code>offload_dir</code> <code>Optional[Path]</code> <p>The directory where this file will be stored (temporarly)</p> <code>None</code> <code>suffix_log_msg</code> <code>str</code> <p>Added message log suffix for context</p> <code>''</code>"},{"location":"reference/torch_to_nnef/tensor/offload/#torch_to_nnef.tensor.offload.OffloadedTensor.to","title":"to","text":"<pre><code>to(*args, **kwargs)\n</code></pre> <p>Change the target device when reloaded in memory.</p>"},{"location":"reference/torch_to_nnef/tensor/offload/#torch_to_nnef.tensor.offload.OffloadedTensor.update_values","title":"update_values","text":"<pre><code>update_values(values: torch.Tensor, strict_shape: bool = True, strict_dtype: bool = True)\n</code></pre> <p>Replace offloaded tensor by new 'values' tensor.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Tensor</code> <p>The tensor that will replace it on disk assertion are made to ensure same shape, dtype as prior</p> required <code>strict_shape</code> <code>bool</code> <p>if True (default) the shape of the new tensor must be the same as the prior one</p> <code>True</code> <code>strict_dtype</code> <code>bool</code> <p>if True (default) the dtype of the new tensor must be the same as the prior one</p> <code>True</code>"},{"location":"reference/torch_to_nnef/tensor/offload/#torch_to_nnef.tensor.offload.ctx_maybe_load_from_disk_as_offloaded","title":"ctx_maybe_load_from_disk_as_offloaded","text":"<pre><code>ctx_maybe_load_from_disk_as_offloaded(offload_dir: T.Optional[T.Union[str, Path]] = None)\n</code></pre> <p>Context manager to force safetensors/torch_load to offload to disk.</p> <p>Example: <pre><code>with ctx_load_from_disk_as_offloaded():\n    if filename.endswith(\".safetensors\"):\n        adapters_weights = safe_load_file(filename, device=\"cpu\")\n    else:\n        adapters_weights = torch_load(\n            filename,\n            map_location=torch.device(device)\n        )\n</code></pre> will offload every tensor to disk as soon as possible.</p>"},{"location":"reference/torch_to_nnef/tensor/offload/#torch_to_nnef.tensor.offload.load_state_dict","title":"load_state_dict","text":"<pre><code>load_state_dict(checkpoint_file, device_map=None, offload_dir: T.Optional[Path] = None, apply_offload: bool = False)\n</code></pre> <p>Load a checkpoint from a given file.</p> <p>If the checkpoint is in the safetensors format and a device map is passed, the weights can be fast-loaded directly on the GPU.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_file</code> <code>`str`</code> <p>The path to the checkpoint to load.</p> required <code>device_map</code> <code>`Dict[str, Union[int, str, torch.device]]`, *optional*</code> <p>A map that specifies where each submodule should go. It doesn't need to be refined to each parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the same device.</p> <code>None</code> <code>offload_dir</code> <code>Optional[Path]</code> <p>Path optional Offload directory to store tensors</p> <code>None</code> <code>apply_offload</code> <code>bool</code> <p>bool if activated it will offload each loaded tensor as soon as possible (we disable it in most case to allow set_module_tensor_to_device dtype casting in memory directly)</p> <code>False</code>"},{"location":"reference/torch_to_nnef/tensor/offload/#torch_to_nnef.tensor.offload.safe_load_file","title":"safe_load_file","text":"<pre><code>safe_load_file(filename: T.Union[str, os.PathLike], device: TDEVICE = 'cpu', offload_dir: T.Optional[Path] = None, apply_offload: bool = False) -&gt; T.Dict[str, torch.Tensor]\n</code></pre> <p>Loads a safetensors file into torch format.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>`str`, or `os.PathLike`</code> <p>The name of the file which contains the tensors</p> required <code>device</code> <code>`Union[str, int]`, *optional*, defaults to `cpu`</code> <p>The device where the tensors need to be located after load. available options are all regular torch device locations.</p> <code>'cpu'</code> <code>offload_dir</code> <code>Optional[Path]</code> <p>Path location where tensor with device disk will be offloaded</p> <code>None</code> <code>apply_offload</code> <code>bool</code> <p>if offload is applyied or left to cpu</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p><code>Dict[str, torch.Tensor]</code>: dictionary that contains name as key,</p> <code>Dict[str, Tensor]</code> <p>value as <code>torch.Tensor</code></p> <p>Example: <pre><code>from safetensors.torch import load_file\n\nfile_path = \"./my_folder/bert.safetensors\"\nloaded = load_file(file_path)\n</code></pre></p>"},{"location":"reference/torch_to_nnef/tensor/offload/#torch_to_nnef.tensor.offload.set_module_tensor_to_device","title":"set_module_tensor_to_device","text":"<pre><code>set_module_tensor_to_device(mod_updater: ModTensorUpdater, tensor_name: str, device: TDEVICE, value: T.Optional[torch.Tensor] = None, dtype: T.Optional[T.Union[str, torch.dtype]] = None, offload_dir: T.Optional[Path] = None)\n</code></pre> <p>A helper function to set a given tensor (parameter of buffer) to device.</p> <p>(     note that doing <code>param.to(device)</code> creates a new tensor not linked     to the parameter, which is why we need this function ).</p> <p>Parameters:</p> Name Type Description Default <code>mod_updater</code> <code>`ModTensorUpdater`</code> <p>The module updater instance that contains the module</p> required <code>tensor_name</code> <code>`str`</code> <p>The full name of the parameter/buffer.</p> required <code>device</code> <code>`int`, `str` or `torch.device`</code> <p>The device on which to set the tensor.</p> required <code>value</code> <code>`torch.Tensor`, *optional*</code> <p>The value of the tensor (useful when going from the meta device to any other device).</p> <code>None</code> <code>dtype</code> <code>`torch.dtype`, *optional*</code> <p>If set, the value of the parameter will be cast to this <code>dtype</code>. Otherwise, <code>value</code> will be cast to the dtype of the existing parameter in the model.</p> <code>None</code> <code>offload_dir</code> <code>Optional[Path]</code> <p>The directory where tensor offloaded on disk will be stored.</p> <code>None</code>"},{"location":"reference/torch_to_nnef/tensor/offload/#torch_to_nnef.tensor.offload.t2n_load_checkpoint_and_dispatch","title":"t2n_load_checkpoint_and_dispatch","text":"<pre><code>t2n_load_checkpoint_and_dispatch(model: nn.Module, checkpoint: Path, device_map: T.Optional[T.Union[str, T.Dict[str, T.Union[str, int, torch.device]]]], offload_dir: Path, strict: bool = False, offload_at_load_state_dict: bool = False)\n</code></pre> <p>Allow to offload as soon as possible.</p> <p>This may be benefical in some rare case where partitioned safetensors file are too big for RAM else it's better to offload after dtype cast in set_module_tensor_to_device.</p>"},{"location":"reference/torch_to_nnef/tensor/opaque/","title":"torch_to_nnef.tensor.opaque","text":""},{"location":"reference/torch_to_nnef/tensor/opaque/#torch_to_nnef.tensor.opaque","title":"torch_to_nnef.tensor.opaque","text":""},{"location":"reference/torch_to_nnef/tensor/opaque/#torch_to_nnef.tensor.opaque.OpaqueTensor","title":"OpaqueTensor","text":"<p>               Bases: <code>Tensor</code></p>"},{"location":"reference/torch_to_nnef/tensor/opaque/#torch_to_nnef.tensor.opaque.OpaqueTensor.data","title":"data  <code>property</code> <code>writable</code>","text":"<pre><code>data\n</code></pre> <p>Very important to keep access to all special attr of OpaqueTensor.</p>"},{"location":"reference/torch_to_nnef/tensor/opaque/#torch_to_nnef.tensor.opaque.OpaqueTensor.to_base_tensor","title":"to_base_tensor","text":"<pre><code>to_base_tensor()\n</code></pre> <p>Wrap _to_base_tensor with jit export infos.</p>"},{"location":"reference/torch_to_nnef/tensor/opaque/#torch_to_nnef.tensor.opaque.OpaqueTensorRef","title":"OpaqueTensorRef","text":"<pre><code>OpaqueTensorRef(meta_tensor: torch.Tensor, opaque_tensor: OpaqueTensor)\n</code></pre> <p>               Bases: <code>Tensor</code></p> <p>Allow to pass through 'tracing'.</p>"},{"location":"reference/torch_to_nnef/tensor/opaque/#torch_to_nnef.tensor.opaque.find_opaque_ref_by_py_id","title":"find_opaque_ref_by_py_id","text":"<pre><code>find_opaque_ref_by_py_id(module: torch.nn.Module, py_id: int)\n</code></pre> <p>Allow to fetch back the opaque parameter once passed the jit 'wall'.</p>"},{"location":"reference/torch_to_nnef/tensor/opaque/#torch_to_nnef.tensor.opaque.opaque_to_final_tensor","title":"opaque_to_final_tensor","text":"<pre><code>opaque_to_final_tensor(rtensor: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p>Even if OpaqueTensor are composed it exposes fully expanded tensor.</p> <p>So for example: an OffloadedTensor that contains a QTensor will 'load' then 'decompress' to show final fp tensor.</p>"},{"location":"reference/torch_to_nnef/tensor/opaque/#torch_to_nnef.tensor.opaque.set_opaque_tensor_in_params_as_ref","title":"set_opaque_tensor_in_params_as_ref","text":"<pre><code>set_opaque_tensor_in_params_as_ref(model: torch.nn.Module)\n</code></pre> <p>Transform OpaqueTensor Parameters into OpaqueTensorRef.</p> <p>This is applied at export time of <code>torch_to_nnef</code> Just before doing any tracing</p>"},{"location":"reference/torch_to_nnef/tensor/quant/","title":"torch_to_nnef.tensor.quant","text":""},{"location":"reference/torch_to_nnef/tensor/quant/#torch_to_nnef.tensor.quant","title":"torch_to_nnef.tensor.quant","text":"<p>Advanced QTensor (&lt;= 8bits) with complex quant scheme non torch native.</p>"},{"location":"reference/torch_to_nnef/tensor/quant/#torch_to_nnef.tensor.quant.QScalePerGroupF16","title":"QScalePerGroupF16","text":"<pre><code>QScalePerGroupF16(group_size: int, scale: torch.Tensor, n_bits: int)\n</code></pre> <p>               Bases: <code>QScheme</code></p> <p>f16 scale only per group.</p> <p>Tract aligned using negative scales.</p>"},{"location":"reference/torch_to_nnef/tensor/quant/#torch_to_nnef.tensor.quant.QScheme","title":"QScheme","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"reference/torch_to_nnef/tensor/quant/#torch_to_nnef.tensor.quant.QScheme.to_device","title":"to_device","text":"<pre><code>to_device(new_device)\n</code></pre> <p>Specific device handling.</p> <p>Each QScheme may implement support for specific device switching for internal quant/dequant (like GPU, ...) allowing faster computation</p>"},{"location":"reference/torch_to_nnef/tensor/quant/#torch_to_nnef.tensor.quant.QTensor","title":"QTensor","text":"<pre><code>QTensor(fp_tensor: torch.Tensor, qscheme: QScheme, dequant_to_dtype=torch.float32, u8_compressors: T.Optional[T.List[U8Compressor]] = None)\n</code></pre> <p>               Bases: <code>OpaqueTensor</code></p> <p>Common interface for all Compressed storage.</p>"},{"location":"reference/torch_to_nnef/tensor/quant/#torch_to_nnef.tensor.quant.QTensor.to_device","title":"to_device","text":"<pre><code>to_device(new_device)\n</code></pre> <p>Specific device handling.</p>"},{"location":"reference/torch_to_nnef/tensor/quant/#torch_to_nnef.tensor.quant.QTensor.write_in_file","title":"write_in_file","text":"<pre><code>write_in_file(dirpath: T.Union[str, Path], label: str)\n</code></pre> <p>Called at NNEF write time.</p> <p>Each specific inference engine format should implement the file dump prefered.</p>"},{"location":"reference/torch_to_nnef/tensor/quant/#torch_to_nnef.tensor.quant.QTensorTract","title":"QTensorTract","text":"<pre><code>QTensorTract(fp_tensor: torch.Tensor, qscheme: QScheme, dequant_to_dtype=torch.float32, u8_compressors: T.Optional[T.List[U8Compressor]] = None)\n</code></pre> <p>               Bases: <code>QTensor</code></p> <p>All QTensorTract implementations.</p>"},{"location":"reference/torch_to_nnef/tensor/quant/#torch_to_nnef.tensor.quant.QTensorTractScaleOnly","title":"QTensorTractScaleOnly","text":"<pre><code>QTensorTractScaleOnly(*args, specific_machine: T.Optional[str] = None, **kwargs)\n</code></pre> <p>               Bases: <code>QTensorTract</code></p> <p>Tract data format it serializes to: Q4_0.</p>"},{"location":"reference/torch_to_nnef/tensor/quant/#torch_to_nnef.tensor.quant.QTensorTractScaleOnly.decompress","title":"decompress","text":"<pre><code>decompress()\n</code></pre> <p>Tract dequantization depends on hardware.</p> <p>Typically dequantization happen with ops in f16 on ARM and f32 (scale directly casted) on others so we overwrite the function to be consistant with tract.</p>"},{"location":"reference/torch_to_nnef/tensor/quant/#torch_to_nnef.tensor.quant.U8Compressor","title":"U8Compressor","text":"<p>Abstract class to add u8 compression methods.</p> <p>This can be used to</p> <p>Apply bitpack elements bellow 8bit Apply classic compression algorithm</p> <p>Warning !! .shape of u8_tensor compressed             must be same as            .shape once decompressed</p>"},{"location":"reference/torch_to_nnef/tensor/quant/#torch_to_nnef.tensor.quant.U8Compressor.compress","title":"compress  <code>abstractmethod</code>","text":"<pre><code>compress(u8_tensor) -&gt; torch.Tensor\n</code></pre> <p>Compress a u8 tensor (into u8).</p> <p>Parameters:</p> Name Type Description Default <code>u8_tensor</code> <p>tensor to be compressed with dtype torch.uint8</p> required <p>Return:     compressed tensor with dtype torch.uint8</p>"},{"location":"reference/torch_to_nnef/tensor/quant/#torch_to_nnef.tensor.quant.U8Compressor.decompress","title":"decompress  <code>abstractmethod</code>","text":"<pre><code>decompress(u8_tensor) -&gt; torch.Tensor\n</code></pre> <p>Decompress an u8 torch tensor (into u8).</p> <p>Parameters:</p> Name Type Description Default <code>u8_tensor</code> <p>compressed tensor with dtype torch.uint8</p> required <p>Return:     tensor decompressed with dtype torch.uint8</p>"},{"location":"reference/torch_to_nnef/tensor/quant/#torch_to_nnef.tensor.quant.U8Compressor.to_device","title":"to_device","text":"<pre><code>to_device(new_device)\n</code></pre> <p>Specific device handling.</p> <p>Each compressor may implement support for specific device (like GPU, ...)</p> <p>Allowing faster computation</p>"},{"location":"reference/torch_to_nnef/tensor/quant/#torch_to_nnef.tensor.quant.fp_to_tract_q4_0_with_min_max_calibration","title":"fp_to_tract_q4_0_with_min_max_calibration","text":"<pre><code>fp_to_tract_q4_0_with_min_max_calibration(fp_tensor, percentile: float = 1.0) -&gt; QTensorTractScaleOnly\n</code></pre> <p>Min-Max method to quantize float tensor to tract supported Q4_0.</p>"},{"location":"reference/torch_to_nnef/tensor/quant/#torch_to_nnef.tensor.quant.qscale_per_group_f16_min_max_calibration","title":"qscale_per_group_f16_min_max_calibration","text":"<pre><code>qscale_per_group_f16_min_max_calibration(fp_tensor, n_bits: int, group_size: int, percentile: float = 1.0) -&gt; QScalePerGroupF16\n</code></pre> <p>Build QScalePerGroupF16 and calibrate requested float tensor.</p> Return <p>Tuple(     QScalePerGroupF16 qscheme,     torch.Tensor[uint8] )</p>"},{"location":"reference/torch_to_nnef/tensor/quant/base/","title":"torch_to_nnef.tensor.quant.base","text":""},{"location":"reference/torch_to_nnef/tensor/quant/base/#torch_to_nnef.tensor.quant.base","title":"torch_to_nnef.tensor.quant.base","text":""},{"location":"reference/torch_to_nnef/tensor/quant/base/#torch_to_nnef.tensor.quant.base.QScalePerGroupF16","title":"QScalePerGroupF16","text":"<pre><code>QScalePerGroupF16(group_size: int, scale: torch.Tensor, n_bits: int)\n</code></pre> <p>               Bases: <code>QScheme</code></p> <p>f16 scale only per group.</p> <p>Tract aligned using negative scales.</p>"},{"location":"reference/torch_to_nnef/tensor/quant/base/#torch_to_nnef.tensor.quant.base.QScheme","title":"QScheme","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"reference/torch_to_nnef/tensor/quant/base/#torch_to_nnef.tensor.quant.base.QScheme.to_device","title":"to_device","text":"<pre><code>to_device(new_device)\n</code></pre> <p>Specific device handling.</p> <p>Each QScheme may implement support for specific device switching for internal quant/dequant (like GPU, ...) allowing faster computation</p>"},{"location":"reference/torch_to_nnef/tensor/quant/base/#torch_to_nnef.tensor.quant.base.QTensor","title":"QTensor","text":"<pre><code>QTensor(fp_tensor: torch.Tensor, qscheme: QScheme, dequant_to_dtype=torch.float32, u8_compressors: T.Optional[T.List[U8Compressor]] = None)\n</code></pre> <p>               Bases: <code>OpaqueTensor</code></p> <p>Common interface for all Compressed storage.</p>"},{"location":"reference/torch_to_nnef/tensor/quant/base/#torch_to_nnef.tensor.quant.base.QTensor.to_device","title":"to_device","text":"<pre><code>to_device(new_device)\n</code></pre> <p>Specific device handling.</p>"},{"location":"reference/torch_to_nnef/tensor/quant/base/#torch_to_nnef.tensor.quant.base.QTensor.write_in_file","title":"write_in_file","text":"<pre><code>write_in_file(dirpath: T.Union[str, Path], label: str)\n</code></pre> <p>Called at NNEF write time.</p> <p>Each specific inference engine format should implement the file dump prefered.</p>"},{"location":"reference/torch_to_nnef/tensor/quant/base/#torch_to_nnef.tensor.quant.base.U8Compressor","title":"U8Compressor","text":"<p>Abstract class to add u8 compression methods.</p> <p>This can be used to</p> <p>Apply bitpack elements bellow 8bit Apply classic compression algorithm</p> <p>Warning !! .shape of u8_tensor compressed             must be same as            .shape once decompressed</p>"},{"location":"reference/torch_to_nnef/tensor/quant/base/#torch_to_nnef.tensor.quant.base.U8Compressor.compress","title":"compress  <code>abstractmethod</code>","text":"<pre><code>compress(u8_tensor) -&gt; torch.Tensor\n</code></pre> <p>Compress a u8 tensor (into u8).</p> <p>Parameters:</p> Name Type Description Default <code>u8_tensor</code> <p>tensor to be compressed with dtype torch.uint8</p> required <p>Return:     compressed tensor with dtype torch.uint8</p>"},{"location":"reference/torch_to_nnef/tensor/quant/base/#torch_to_nnef.tensor.quant.base.U8Compressor.decompress","title":"decompress  <code>abstractmethod</code>","text":"<pre><code>decompress(u8_tensor) -&gt; torch.Tensor\n</code></pre> <p>Decompress an u8 torch tensor (into u8).</p> <p>Parameters:</p> Name Type Description Default <code>u8_tensor</code> <p>compressed tensor with dtype torch.uint8</p> required <p>Return:     tensor decompressed with dtype torch.uint8</p>"},{"location":"reference/torch_to_nnef/tensor/quant/base/#torch_to_nnef.tensor.quant.base.U8Compressor.to_device","title":"to_device","text":"<pre><code>to_device(new_device)\n</code></pre> <p>Specific device handling.</p> <p>Each compressor may implement support for specific device (like GPU, ...)</p> <p>Allowing faster computation</p>"},{"location":"reference/torch_to_nnef/tensor/quant/base/#torch_to_nnef.tensor.quant.base.qscale_per_group_f16_min_max_calibration","title":"qscale_per_group_f16_min_max_calibration","text":"<pre><code>qscale_per_group_f16_min_max_calibration(fp_tensor, n_bits: int, group_size: int, percentile: float = 1.0) -&gt; QScalePerGroupF16\n</code></pre> <p>Build QScalePerGroupF16 and calibrate requested float tensor.</p> Return <p>Tuple(     QScalePerGroupF16 qscheme,     torch.Tensor[uint8] )</p>"},{"location":"reference/torch_to_nnef/tensor/quant/qtract/","title":"torch_to_nnef.tensor.quant.qtract","text":""},{"location":"reference/torch_to_nnef/tensor/quant/qtract/#torch_to_nnef.tensor.quant.qtract","title":"torch_to_nnef.tensor.quant.qtract","text":""},{"location":"reference/torch_to_nnef/tensor/quant/qtract/#torch_to_nnef.tensor.quant.qtract.QTensorTract","title":"QTensorTract","text":"<pre><code>QTensorTract(fp_tensor: torch.Tensor, qscheme: QScheme, dequant_to_dtype=torch.float32, u8_compressors: T.Optional[T.List[U8Compressor]] = None)\n</code></pre> <p>               Bases: <code>QTensor</code></p> <p>All QTensorTract implementations.</p>"},{"location":"reference/torch_to_nnef/tensor/quant/qtract/#torch_to_nnef.tensor.quant.qtract.QTensorTractScaleOnly","title":"QTensorTractScaleOnly","text":"<pre><code>QTensorTractScaleOnly(*args, specific_machine: T.Optional[str] = None, **kwargs)\n</code></pre> <p>               Bases: <code>QTensorTract</code></p> <p>Tract data format it serializes to: Q4_0.</p>"},{"location":"reference/torch_to_nnef/tensor/quant/qtract/#torch_to_nnef.tensor.quant.qtract.QTensorTractScaleOnly.decompress","title":"decompress","text":"<pre><code>decompress()\n</code></pre> <p>Tract dequantization depends on hardware.</p> <p>Typically dequantization happen with ops in f16 on ARM and f32 (scale directly casted) on others so we overwrite the function to be consistant with tract.</p>"},{"location":"reference/torch_to_nnef/tensor/quant/qtract/#torch_to_nnef.tensor.quant.qtract.fp_to_tract_q4_0_with_min_max_calibration","title":"fp_to_tract_q4_0_with_min_max_calibration","text":"<pre><code>fp_to_tract_q4_0_with_min_max_calibration(fp_tensor, percentile: float = 1.0) -&gt; QTensorTractScaleOnly\n</code></pre> <p>Min-Max method to quantize float tensor to tract supported Q4_0.</p>"},{"location":"reference/torch_to_nnef/tensor/updater/","title":"torch_to_nnef.tensor.updater","text":""},{"location":"reference/torch_to_nnef/tensor/updater/#torch_to_nnef.tensor.updater","title":"torch_to_nnef.tensor.updater","text":""},{"location":"reference/torch_to_nnef/tensor/updater/#torch_to_nnef.tensor.updater.ModTensorUpdater","title":"ModTensorUpdater","text":"<pre><code>ModTensorUpdater(model: torch.nn.Module, add_parameter_if_unset: bool = True, add_buffers: bool = False, add_unregistred_tensor: bool = False, disable_requires_grad: bool = False)\n</code></pre> <p>Helper to update parameter/buffer/unregistred tensor of a model cleanly.</p> <p>Cleanly means without breaking shared reference between Tensors.</p> <p>An example is the shared reference on transformers between first input_ids embedding and last linear layer projection weights.</p> <p>Init ModTensorUpdater.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>nn.Module model that will have tensors updated with this class</p> required <code>add_parameter_if_unset</code> <code>bool</code> <p>if you add a tensor where there is not yet a torch.nn.Parameters in the model it will add it</p> <code>True</code> <code>add_buffers</code> <code>bool</code> <p>Scope all nn.Buffer PyTorch object of the model to be 'updatable'</p> <code>False</code> <code>add_unregistred_tensor</code> <code>bool</code> <p>Scope all tensor PyTorch object of the model not referenced in nn.Parameters &amp; nn.Buffer</p> <code>False</code> <code>disable_requires_grad</code> <code>bool</code> <p>If set it force tensors replaced to be with no 'requires_grad' at update time</p> <code>False</code>"},{"location":"reference/torch_to_nnef/tensor/updater/#torch_to_nnef.tensor.updater.ModTensorUpdater.get_by_name","title":"get_by_name","text":"<pre><code>get_by_name(name: str) -&gt; torch.Tensor\n</code></pre> <p>Get tensor based on it's  reference name.</p>"},{"location":"reference/torch_to_nnef/tensor/updater/#torch_to_nnef.tensor.updater.ModTensorUpdater.update_by_name","title":"update_by_name","text":"<pre><code>update_by_name(name: str, new_tensor: torch.Tensor, tie_replacements: bool = True, enforce_tensor_consistency: bool = True) -&gt; torch.Tensor\n</code></pre> <p>Update tensor based on it's  reference name.</p>"},{"location":"reference/torch_to_nnef/tensor/updater/#torch_to_nnef.tensor.updater.ModTensorUpdater.update_by_ref","title":"update_by_ref","text":"<pre><code>update_by_ref(ref: torch.nn.Parameter, new_tensor: torch.Tensor, enforce_tensor_consistency: bool = True) -&gt; torch.Tensor\n</code></pre> <p>Update tensor based on it's  reference object.</p>"},{"location":"reference/torch_to_nnef/tensor/utils/","title":"torch_to_nnef.tensor.utils","text":""},{"location":"reference/torch_to_nnef/tensor/utils/#torch_to_nnef.tensor.utils","title":"torch_to_nnef.tensor.utils","text":""},{"location":"reference/torch_to_nnef/torch_graph/","title":"torch_to_nnef.torch_graph","text":""},{"location":"reference/torch_to_nnef/torch_graph/#torch_to_nnef.torch_graph","title":"torch_to_nnef.torch_graph","text":"<p>torch_graph is intended to extract full representation of PyTorch Graph.</p> <p>From PyTorch into a stable intermediate representation suitable to then apply translation operation to NNEF. This means that not all PyTorch orginal graph is translated. For example, we ignore part linked to device location informations, memory specific operation or parameters linked to gradients.</p> <p>This choice which is different compared to torch.onnx module due to the absence of control (on our side) over evolution of PyTorch internals. If some of the PyTorch internals are modified only this module should idealy be impacted.</p> <p>Here there is NO notion of dynamic axes all shapes are supposedly defined based on provided input example. At latter stage in other modules the dynamic shapes need to be introduced if requested by user.</p>"},{"location":"reference/torch_to_nnef/torch_graph/#torch_to_nnef.torch_graph.Data","title":"Data  <code>dataclass</code>","text":"<pre><code>Data(name: str, data: T.Any)\n</code></pre> <p>               Bases: <code>NamedItem</code></p> <p>Base abstract T2N IR data holder.</p>"},{"location":"reference/torch_to_nnef/torch_graph/#torch_to_nnef.torch_graph.FixedTensorList","title":"FixedTensorList  <code>dataclass</code>","text":"<pre><code>FixedTensorList(name: str, data: T.Sequence[T.Union[TensorVariable, PythonConstant]])\n</code></pre> <p>               Bases: <code>Data</code></p> <p>FixedTensorList is a list that contains tensor constant or not.</p>"},{"location":"reference/torch_to_nnef/torch_graph/#torch_to_nnef.torch_graph.TensorVariable","title":"TensorVariable  <code>dataclass</code>","text":"<pre><code>TensorVariable(name: str, data: T.Optional[torch.Tensor], shape: T.Optional[T.List[int]], dtype: T.Optional[torch.dtype], quant: T.Optional[T.Dict[str, T.Any]] = None, _traced_data: T.Optional[torch.Tensor] = None)\n</code></pre> <p>               Bases: <code>Data</code></p>"},{"location":"reference/torch_to_nnef/torch_graph/#torch_to_nnef.torch_graph.TensorVariable.tracing_data","title":"tracing_data  <code>property</code>","text":"<pre><code>tracing_data\n</code></pre> <p>Generate data if is not fixed based on tensor information.</p> <p>we use it to produce computation trace</p>"},{"location":"reference/torch_to_nnef/torch_graph/#torch_to_nnef.torch_graph.TorchModuleIRGraph","title":"TorchModuleIRGraph","text":"<pre><code>TorchModuleIRGraph(torch_module_tracer: TorchModuleTracer, omit_useless_nodes: bool = True, is_root_module: bool = False)\n</code></pre> <p>Torch Graph intermediate representation from: jit.trace with recursion.</p> <p>This is not direct torch._C.Graph but simpler abstraction, with:</p> <p>A list of data nodes in <code>self.data_nodes</code> A list of operations nodes in <code>self.op_nodes</code></p> <p><code>self.inputs</code> is a list of reference of some <code>self.data_nodes</code> <code>self.outputs</code> is a list of reference of some <code>self.data_nodes</code></p> <p>This abstraction of the vanilla Torch Graph allow to manipulate graph in order to check/complete missing data informations and ignore useless operations for our transcription needs.</p> <p>It's also allows to be less reliant on base graph in case of modification of PyTorch Internals (think Adapter Pattern).</p> <p>Warning !     Only NOT nested data container (TupleTensors, FixedTensorList, ...)     are supported for now</p>"},{"location":"reference/torch_to_nnef/torch_graph/#torch_to_nnef.torch_graph.TorchModuleIRGraph.parse","title":"parse","text":"<pre><code>parse(nnef_variable_naming_scheme: VariableNamingScheme = DEFAULT_VARNAME_SCHEME, provided_inputs=None, provided_outputs=None, forced_inputs_names=None, forced_outputs_names=None)\n</code></pre> <p>Core parsing transforming nn.Module into torch_to_nnef IR.</p>"},{"location":"reference/torch_to_nnef/torch_graph/#torch_to_nnef.torch_graph.TorchModuleIRGraph.printall","title":"printall","text":"<pre><code>printall()\n</code></pre> <p>Display Helper Graph infos in stdout of your tty.</p>"},{"location":"reference/torch_to_nnef/torch_graph/#torch_to_nnef.torch_graph.TorchModuleIRGraph.remap_node","title":"remap_node","text":"<pre><code>remap_node(from_node, to_node)\n</code></pre> <p>Remap a data_node to another.</p>"},{"location":"reference/torch_to_nnef/torch_graph/#torch_to_nnef.torch_graph.TorchModuleTracer","title":"TorchModuleTracer","text":"<pre><code>TorchModuleTracer(module: nn.Module, traced_module: T.Optional[torch.jit.TracedModule] = None, fn_name: str = 'forward', args: T.Optional[T.Tuple[T.Any, ...]] = None)\n</code></pre> <p>Evaluate Optimized traced Function code so that signature always match.</p> <p>original Module is passed to do proper un-boxing later on. This is needed because we have a re-routing based on actual module classtype.</p> <p>Create a tracer for module.</p> <p>The tracer stores the original module, an optional pre\u2011traced <code>torch.jit.TracedModule</code> (which allows re\u2011use of a previously computed trace), the name of the forward method to trace, and the arguments used for tracing.  The arguments are post\u2011processed by :func:<code>maybe_quantize_args_tensor</code> to ensure compatibility with quantized modules.</p>"},{"location":"reference/torch_to_nnef/torch_graph/#torch_to_nnef.torch_graph.TorchModuleTracer.torch_graph","title":"torch_graph  <code>property</code>","text":"<pre><code>torch_graph\n</code></pre> <p>Return the underlying PyTorch graph object.</p> <p>The actual <code>torch.Graph</code> is retrieved from the traced module. When a different forward method is requested (<code>fn_name</code> differs from \"forward\"), the corresponding sub\u2011graph is returned instead.</p>"},{"location":"reference/torch_to_nnef/torch_graph/#torch_to_nnef.torch_graph.TorchModuleTracer.traced_module","title":"traced_module  <code>property</code>","text":"<pre><code>traced_module\n</code></pre> <p>Return the traced module, computing it lazily if required.</p> <p>If <code>self._traced_module</code> is <code>None</code> the method will perform a <code>jit.trace</code> on <code>self.mod</code> with <code>self.args</code> while handling possible PyTorch version nuances.  Any <code>RuntimeError</code> raised by <code>torch.jit.trace</code> is wrapped into a :class:<code>~torch_to_nnef.exceptions.T2NErrorTorchJitTraceFailed</code> exception.</p>"},{"location":"reference/torch_to_nnef/torch_graph/#torch_to_nnef.torch_graph.TorchOp","title":"TorchOp  <code>dataclass</code>","text":"<pre><code>TorchOp(kind: str, module_path: str, inputs: T.List[Data], outputs: T.List[TtupleOrVar], scope: str, op_ref: T.Optional[T.Callable], call_name: T.Optional[str])\n</code></pre>"},{"location":"reference/torch_to_nnef/torch_graph/#torch_to_nnef.torch_graph.TorchOp.call_op","title":"call_op","text":"<pre><code>call_op()\n</code></pre> <p>Produce operation output based on traced inputs with real torch call.</p> <p>This operation call is done via self.args arguments (for now). Which means that we need to have all args needed in parameters order, following at least 1 underling torch operation signature.</p> <p>NOTE: we use a different approach than original torch.onnx which pass parameter by keyword arguments, this is due to the fact that we are not aware of argument name being provided in exported graph (     from what we understand torch.onnx solve this via explicit     rerouting of all signatures, which might be a bit bulky     in most case ).</p>"},{"location":"reference/torch_to_nnef/torch_graph/#torch_to_nnef.torch_graph.TorchOp.realise_output_type_and_size","title":"realise_output_type_and_size","text":"<pre><code>realise_output_type_and_size(approx: bool = True) -&gt; bool\n</code></pre> <p>Trace output and try to find type shape and constant realisation.</p>"},{"location":"reference/torch_to_nnef/torch_graph/#torch_to_nnef.torch_graph.TorchOp.update_call_op_arg_kwargs","title":"update_call_op_arg_kwargs","text":"<pre><code>update_call_op_arg_kwargs(args)\n</code></pre> <p>Custom adaptation to call aten fn with torch exposed py fn.</p>"},{"location":"reference/torch_to_nnef/torch_graph/ir_data/","title":"torch_to_nnef.torch_graph.ir_data","text":""},{"location":"reference/torch_to_nnef/torch_graph/ir_data/#torch_to_nnef.torch_graph.ir_data","title":"torch_to_nnef.torch_graph.ir_data","text":"<p>Abstractions used in torch_to_nnef internal graph data IR.</p> <p>The goal is that these elements are: - extracted/parsed from PyTorch graph data structs - translated to NNEF graph data structs</p>"},{"location":"reference/torch_to_nnef/torch_graph/ir_data/#torch_to_nnef.torch_graph.ir_data.BlobTorchScriptObject","title":"BlobTorchScriptObject  <code>dataclass</code>","text":"<pre><code>BlobTorchScriptObject(name: str, data: T.Any)\n</code></pre> <p>               Bases: <code>Data</code></p> <p>Used only in Quantized Operators from our current obervation.</p>"},{"location":"reference/torch_to_nnef/torch_graph/ir_data/#torch_to_nnef.torch_graph.ir_data.Data","title":"Data  <code>dataclass</code>","text":"<pre><code>Data(name: str, data: T.Any)\n</code></pre> <p>               Bases: <code>NamedItem</code></p> <p>Base abstract T2N IR data holder.</p>"},{"location":"reference/torch_to_nnef/torch_graph/ir_data/#torch_to_nnef.torch_graph.ir_data.DictTensors","title":"DictTensors  <code>dataclass</code>","text":"<pre><code>DictTensors(name: str, data: T.Dict[str, TensorVariable])\n</code></pre> <p>               Bases: <code>Data</code></p> <p>Used as transition object only.</p> <p>None should be remaining once graph is fully expanded</p>"},{"location":"reference/torch_to_nnef/torch_graph/ir_data/#torch_to_nnef.torch_graph.ir_data.FixedTensorList","title":"FixedTensorList  <code>dataclass</code>","text":"<pre><code>FixedTensorList(name: str, data: T.Sequence[T.Union[TensorVariable, PythonConstant]])\n</code></pre> <p>               Bases: <code>Data</code></p> <p>FixedTensorList is a list that contains tensor constant or not.</p>"},{"location":"reference/torch_to_nnef/torch_graph/ir_data/#torch_to_nnef.torch_graph.ir_data.TensorVariable","title":"TensorVariable  <code>dataclass</code>","text":"<pre><code>TensorVariable(name: str, data: T.Optional[torch.Tensor], shape: T.Optional[T.List[int]], dtype: T.Optional[torch.dtype], quant: T.Optional[T.Dict[str, T.Any]] = None, _traced_data: T.Optional[torch.Tensor] = None)\n</code></pre> <p>               Bases: <code>Data</code></p>"},{"location":"reference/torch_to_nnef/torch_graph/ir_data/#torch_to_nnef.torch_graph.ir_data.TensorVariable.tracing_data","title":"tracing_data  <code>property</code>","text":"<pre><code>tracing_data\n</code></pre> <p>Generate data if is not fixed based on tensor information.</p> <p>we use it to produce computation trace</p>"},{"location":"reference/torch_to_nnef/torch_graph/ir_data/#torch_to_nnef.torch_graph.ir_data.TupleTensors","title":"TupleTensors  <code>dataclass</code>","text":"<pre><code>TupleTensors(name: str, data: T.List[TensorVariable])\n</code></pre> <p>               Bases: <code>Data</code></p> <p>Used as transition object only.</p> <p>None should be remaining once graph is fully expanded</p>"},{"location":"reference/torch_to_nnef/torch_graph/ir_graph/","title":"torch_to_nnef.torch_graph.ir_graph","text":""},{"location":"reference/torch_to_nnef/torch_graph/ir_graph/#torch_to_nnef.torch_graph.ir_graph","title":"torch_to_nnef.torch_graph.ir_graph","text":""},{"location":"reference/torch_to_nnef/torch_graph/ir_graph/#torch_to_nnef.torch_graph.ir_graph.TorchModuleIRGraph","title":"TorchModuleIRGraph","text":"<pre><code>TorchModuleIRGraph(torch_module_tracer: TorchModuleTracer, omit_useless_nodes: bool = True, is_root_module: bool = False)\n</code></pre> <p>Torch Graph intermediate representation from: jit.trace with recursion.</p> <p>This is not direct torch._C.Graph but simpler abstraction, with:</p> <p>A list of data nodes in <code>self.data_nodes</code> A list of operations nodes in <code>self.op_nodes</code></p> <p><code>self.inputs</code> is a list of reference of some <code>self.data_nodes</code> <code>self.outputs</code> is a list of reference of some <code>self.data_nodes</code></p> <p>This abstraction of the vanilla Torch Graph allow to manipulate graph in order to check/complete missing data informations and ignore useless operations for our transcription needs.</p> <p>It's also allows to be less reliant on base graph in case of modification of PyTorch Internals (think Adapter Pattern).</p> <p>Warning !     Only NOT nested data container (TupleTensors, FixedTensorList, ...)     are supported for now</p>"},{"location":"reference/torch_to_nnef/torch_graph/ir_graph/#torch_to_nnef.torch_graph.ir_graph.TorchModuleIRGraph.parse","title":"parse","text":"<pre><code>parse(nnef_variable_naming_scheme: VariableNamingScheme = DEFAULT_VARNAME_SCHEME, provided_inputs=None, provided_outputs=None, forced_inputs_names=None, forced_outputs_names=None)\n</code></pre> <p>Core parsing transforming nn.Module into torch_to_nnef IR.</p>"},{"location":"reference/torch_to_nnef/torch_graph/ir_graph/#torch_to_nnef.torch_graph.ir_graph.TorchModuleIRGraph.printall","title":"printall","text":"<pre><code>printall()\n</code></pre> <p>Display Helper Graph infos in stdout of your tty.</p>"},{"location":"reference/torch_to_nnef/torch_graph/ir_graph/#torch_to_nnef.torch_graph.ir_graph.TorchModuleIRGraph.remap_node","title":"remap_node","text":"<pre><code>remap_node(from_node, to_node)\n</code></pre> <p>Remap a data_node to another.</p>"},{"location":"reference/torch_to_nnef/torch_graph/ir_helpers/","title":"torch_to_nnef.torch_graph.ir_helpers","text":""},{"location":"reference/torch_to_nnef/torch_graph/ir_helpers/#torch_to_nnef.torch_graph.ir_helpers","title":"torch_to_nnef.torch_graph.ir_helpers","text":""},{"location":"reference/torch_to_nnef/torch_graph/ir_helpers/#torch_to_nnef.torch_graph.ir_helpers.aten_name_to_torch_fn","title":"aten_name_to_torch_fn","text":"<pre><code>aten_name_to_torch_fn(aten_name: str)\n</code></pre> <p>Get aten cpp torch operator raw python binding.</p>"},{"location":"reference/torch_to_nnef/torch_graph/ir_helpers/#torch_to_nnef.torch_graph.ir_helpers.dynamic_tensor_list_parse","title":"dynamic_tensor_list_parse","text":"<pre><code>dynamic_tensor_list_parse(node_c_value: torch._C.Value)\n</code></pre> <p>Hold outputs of aten::chunk and other pytorch graph Tensor[] .</p>"},{"location":"reference/torch_to_nnef/torch_graph/ir_module_tracer/","title":"torch_to_nnef.torch_graph.ir_module_tracer","text":""},{"location":"reference/torch_to_nnef/torch_graph/ir_module_tracer/#torch_to_nnef.torch_graph.ir_module_tracer","title":"torch_to_nnef.torch_graph.ir_module_tracer","text":"<p>implements :class:<code>TorchModuleTracer</code>.</p> <p>It traces a <code>torch.nn.Module</code> with <code>torch.jit.trace</code> and exposes the resulting graph.</p> <p>with few related helper functions (e.g. <code>_is_io_quantized_module</code> and <code>maybe_quantize_args_tensor</code>) provide small utilities used during tracing.</p>"},{"location":"reference/torch_to_nnef/torch_graph/ir_module_tracer/#torch_to_nnef.torch_graph.ir_module_tracer.TorchModuleTracer","title":"TorchModuleTracer","text":"<pre><code>TorchModuleTracer(module: nn.Module, traced_module: T.Optional[torch.jit.TracedModule] = None, fn_name: str = 'forward', args: T.Optional[T.Tuple[T.Any, ...]] = None)\n</code></pre> <p>Evaluate Optimized traced Function code so that signature always match.</p> <p>original Module is passed to do proper un-boxing later on. This is needed because we have a re-routing based on actual module classtype.</p> <p>Create a tracer for module.</p> <p>The tracer stores the original module, an optional pre\u2011traced <code>torch.jit.TracedModule</code> (which allows re\u2011use of a previously computed trace), the name of the forward method to trace, and the arguments used for tracing.  The arguments are post\u2011processed by :func:<code>maybe_quantize_args_tensor</code> to ensure compatibility with quantized modules.</p>"},{"location":"reference/torch_to_nnef/torch_graph/ir_module_tracer/#torch_to_nnef.torch_graph.ir_module_tracer.TorchModuleTracer.torch_graph","title":"torch_graph  <code>property</code>","text":"<pre><code>torch_graph\n</code></pre> <p>Return the underlying PyTorch graph object.</p> <p>The actual <code>torch.Graph</code> is retrieved from the traced module. When a different forward method is requested (<code>fn_name</code> differs from \"forward\"), the corresponding sub\u2011graph is returned instead.</p>"},{"location":"reference/torch_to_nnef/torch_graph/ir_module_tracer/#torch_to_nnef.torch_graph.ir_module_tracer.TorchModuleTracer.traced_module","title":"traced_module  <code>property</code>","text":"<pre><code>traced_module\n</code></pre> <p>Return the traced module, computing it lazily if required.</p> <p>If <code>self._traced_module</code> is <code>None</code> the method will perform a <code>jit.trace</code> on <code>self.mod</code> with <code>self.args</code> while handling possible PyTorch version nuances.  Any <code>RuntimeError</code> raised by <code>torch.jit.trace</code> is wrapped into a :class:<code>~torch_to_nnef.exceptions.T2NErrorTorchJitTraceFailed</code> exception.</p>"},{"location":"reference/torch_to_nnef/torch_graph/ir_module_tracer/#torch_to_nnef.torch_graph.ir_module_tracer.maybe_quantize_args_tensor","title":"maybe_quantize_args_tensor","text":"<pre><code>maybe_quantize_args_tensor(module, args)\n</code></pre> <p>Quantize tensors in args if module expects quantized input.</p> <p>The function walks the args tuple, and for each tensor that is not already quantized, it creates a fake quantized representation using <code>torch.quantize_per_tensor</code> with a dummy scale/zero point.  The quantized tensor is not representative of real data; it is solely used to keep the tracing machinery happy when the module expects quantized inputs.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <p>A <code>torch.nn.Module</code> instance.</p> required <code>args</code> <p>A tuple of inputs supplied to the module.  <code>None</code> is accepted for modules that do not require any positional arguments.</p> required <p>Returns:</p> Type Description <p>A potentially modified tuple where tensors are quantized when</p> <p>necessary.</p>"},{"location":"reference/torch_to_nnef/torch_graph/ir_naming/","title":"torch_to_nnef.torch_graph.ir_naming","text":""},{"location":"reference/torch_to_nnef/torch_graph/ir_naming/#torch_to_nnef.torch_graph.ir_naming","title":"torch_to_nnef.torch_graph.ir_naming","text":""},{"location":"reference/torch_to_nnef/torch_graph/ir_naming/#torch_to_nnef.torch_graph.ir_naming.apply_nnef_variable_naming_scheme","title":"apply_nnef_variable_naming_scheme","text":"<pre><code>apply_nnef_variable_naming_scheme(torch_ir_graph, scheme: VariableNamingScheme = DEFAULT_VARNAME_SCHEME)\n</code></pre> <p>Rename availlable data node following a scheme.</p> <p>by default the natural_verbose pattern built is as close as possible to PyTorch graph context info. This pattern might come as too verbose.</p> <p>we propose a more concise numeric pattern that allow easier debug when looking at NNEF export correctness.</p>"},{"location":"reference/torch_to_nnef/torch_graph/ir_naming/#torch_to_nnef.torch_graph.ir_naming.remove_useless_digits_from_module_names","title":"remove_useless_digits_from_module_names","text":"<pre><code>remove_useless_digits_from_module_names(torch_mod_ir_graph, lower: bool)\n</code></pre> <p>Cleanup final namings in graph.</p> <ul> <li>Remove useless digits from module names   for example:     '_20__post_attention_layernorm_4__weight_expanded_1__weight'     would become     '_20__post_attention_layernorm__weight_expanded__weight'     if there is no naming collision with this simlification</li> </ul>"},{"location":"reference/torch_to_nnef/torch_graph/ir_op/","title":"torch_to_nnef.torch_graph.ir_op","text":""},{"location":"reference/torch_to_nnef/torch_graph/ir_op/#torch_to_nnef.torch_graph.ir_op","title":"torch_to_nnef.torch_graph.ir_op","text":"<p>Abstractions used in torch_to_nnef internal graph Operations IR.</p> <p>(decoupled from PyTorch and NNEF)</p> <p>The goal is that these elements are: - extracted/parsed from PyTorch graph operations - translated to NNEF graph operations</p>"},{"location":"reference/torch_to_nnef/torch_graph/ir_op/#torch_to_nnef.torch_graph.ir_op.InputsAlignBetweenAtenAndTorch","title":"InputsAlignBetweenAtenAndTorch","text":"<p>Mapping inputs between Python <code>torch.$1</code> and cpp <code>aten::$2</code>.</p> <p>Because function arguments are not 1 to 1</p>"},{"location":"reference/torch_to_nnef/torch_graph/ir_op/#torch_to_nnef.torch_graph.ir_op.TorchOp","title":"TorchOp  <code>dataclass</code>","text":"<pre><code>TorchOp(kind: str, module_path: str, inputs: T.List[Data], outputs: T.List[TtupleOrVar], scope: str, op_ref: T.Optional[T.Callable], call_name: T.Optional[str])\n</code></pre>"},{"location":"reference/torch_to_nnef/torch_graph/ir_op/#torch_to_nnef.torch_graph.ir_op.TorchOp.call_op","title":"call_op","text":"<pre><code>call_op()\n</code></pre> <p>Produce operation output based on traced inputs with real torch call.</p> <p>This operation call is done via self.args arguments (for now). Which means that we need to have all args needed in parameters order, following at least 1 underling torch operation signature.</p> <p>NOTE: we use a different approach than original torch.onnx which pass parameter by keyword arguments, this is due to the fact that we are not aware of argument name being provided in exported graph (     from what we understand torch.onnx solve this via explicit     rerouting of all signatures, which might be a bit bulky     in most case ).</p>"},{"location":"reference/torch_to_nnef/torch_graph/ir_op/#torch_to_nnef.torch_graph.ir_op.TorchOp.realise_output_type_and_size","title":"realise_output_type_and_size","text":"<pre><code>realise_output_type_and_size(approx: bool = True) -&gt; bool\n</code></pre> <p>Trace output and try to find type shape and constant realisation.</p>"},{"location":"reference/torch_to_nnef/torch_graph/ir_op/#torch_to_nnef.torch_graph.ir_op.TorchOp.update_call_op_arg_kwargs","title":"update_call_op_arg_kwargs","text":"<pre><code>update_call_op_arg_kwargs(args)\n</code></pre> <p>Custom adaptation to call aten fn with torch exposed py fn.</p>"},{"location":"reference/torch_to_nnef/torch_graph/torch_const/","title":"torch_to_nnef.torch_graph.torch_const","text":""},{"location":"reference/torch_to_nnef/torch_graph/torch_const/#torch_to_nnef.torch_graph.torch_const","title":"torch_to_nnef.torch_graph.torch_const","text":"<p>Some Core PyTorch core naming, used to trace/parse their JIT.</p>"},{"location":"reference/torch_to_nnef/utils/","title":"torch_to_nnef.utils","text":""},{"location":"reference/torch_to_nnef/utils/#torch_to_nnef.utils","title":"torch_to_nnef.utils","text":""},{"location":"reference/torch_to_nnef/utils/#torch_to_nnef.utils.ReactiveNamedItemDict","title":"ReactiveNamedItemDict","text":"<pre><code>ReactiveNamedItemDict(items: T.Optional[T.Iterable[NamedItem]] = None)\n</code></pre> <p>Named items ordered Dict data structure.</p> <p>Ensure that 'NO' 2 items are inserted with same 'name' attribute and maintains fast name update and with some additive colision protections.</p> <p>Warning! only aimed at NamedItem subclass.</p> <p>Expose a 'list' like interface. (with limited index access)</p> Example <p>from dataclasses import dataclass @dataclass ... class DummyItem(NamedItem): ...     name: str ... namespace = ReactiveNamedItemDict() item = DummyItem(\"hello\") for i in \"abc\": ...     namespace.append(DummyItem(i)) namespace.append(item) try: ...     namespace.append(DummyItem(\"a\")) ...     assert False ... except T2NErrorDataNodeValue: ...     pass item.name = \"world\" namespace.append(DummyItem(\"hello\"))</p>"},{"location":"reference/torch_to_nnef/utils/#torch_to_nnef.utils.ReactiveNamedItemDict.append","title":"append","text":"<pre><code>append(item: NamedItem)\n</code></pre> <p>Append item to ordered set.</p> <p>WARNING: This is crucial that all added items use this function as it set the hook to listen to name changes</p>"},{"location":"reference/torch_to_nnef/utils/#torch_to_nnef.utils.SemanticVersion","title":"SemanticVersion","text":"<pre><code>SemanticVersion(**kwargs)\n</code></pre> <p>Helper to check a version is higher than another.</p> <p>Attributes:</p> Name Type Description <code>TAGS</code> <p>each versions level (should not be modified in most cases) ordering being done from left to right.</p> Example <p>version = SemanticVersion.from_str(\"1.2.13\") \"1.2.12\" &lt; version &lt; \"1.2.14\" True \"1.3.12\" &lt; version False version == \"1.2.13\" True</p> <p>Init.</p> <p>(depends on TAGS but default is:)</p> Name Type Description Default <code>major</code> <p>int</p> required <code>minor</code> <p>int</p> required <code>patch</code> <p>int</p> required"},{"location":"reference/torch_to_nnef/utils/#torch_to_nnef.utils.cache","title":"cache","text":"<pre><code>cache(func: T.Callable[..., C]) -&gt; C\n</code></pre> <p>LRU cache helper that avoid pylint complains.</p>"},{"location":"reference/torch_to_nnef/utils/#torch_to_nnef.utils.cd","title":"cd","text":"<pre><code>cd(path)\n</code></pre> <p>Context manager for changing the current working directory.</p>"},{"location":"reference/torch_to_nnef/utils/#torch_to_nnef.utils.dedup_list","title":"dedup_list","text":"<pre><code>dedup_list(lst: T.List[T.Any]) -&gt; T.List[T.Any]\n</code></pre> <p>Remove duplicates from list while preserving order.</p>"},{"location":"reference/torch_to_nnef/utils/#torch_to_nnef.utils.flatten_dict","title":"flatten_dict","text":"<pre><code>flatten_dict(d: MutableMapping, parent_key: str = '', sep: str = '.') -&gt; MutableMapping\n</code></pre> <p>Flatten a nested dictionary.</p>"},{"location":"reference/torch_to_nnef/utils/#torch_to_nnef.utils.flatten_dict_tuple_or_list","title":"flatten_dict_tuple_or_list","text":"<pre><code>flatten_dict_tuple_or_list(obj: T.Any, collected_types: T.Optional[T.List[T.Type]] = None, collected_idxes: T.Optional[T.List[int]] = None, current_idx: int = 0) -&gt; T.Tuple[T.Tuple[T.Tuple[T.Type, ...], T.Tuple[T.Union[int, str], ...], T.Any], ...]\n</code></pre> <p>Flatten dict/list/tuple recursively, return types, indexes and values.</p> <p>Flatten happen in depth first search order</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>dict/tuple/list or anything else (structure can be arbitrary deep) this contains N number of element non dict/list/tuple</p> required <code>collected_types</code> <code>Optional[List[Type]]</code> <p>do not set</p> <code>None</code> <code>collected_idxes</code> <code>Optional[List[int]]</code> <p>do not set</p> <code>None</code> <code>current_idx</code> <code>int</code> <p>do not set</p> <code>0</code> <p>Returns:</p> Type Description <code>Tuple[Tuple[Tuple[Type, ...], Tuple[Union[int, str], ...], Any], ...]</code> <p>tuple of N tuples each containing a tuple of: types, indexes and the element</p> Example <p>If initial obj=[{\"a\": 1, \"b\": 3}] it will output:     (         ((list, dict), (0, \"a\"), 1),         ((list, dict), (0, \"b\"), 3),     )</p>"},{"location":"reference/torch_to_nnef/utils/#torch_to_nnef.utils.fullname","title":"fullname","text":"<pre><code>fullname(o) -&gt; str\n</code></pre> <p>Full class name with module path from an object.</p>"},{"location":"reference/torch_to_nnef/utils/#torch_to_nnef.utils.init_empty_weights","title":"init_empty_weights","text":"<pre><code>init_empty_weights(include_buffers: T.Optional[bool] = None) -&gt; T.Iterator[None]\n</code></pre> <p>A context manager under which models init with meta device.</p> <p>Borrowed from <code>accelerate</code></p> <p>A context manager under which models are initialized with all parameters on the meta device, therefore creating an empty model. Useful when just initializing the model would blow the available RAM.</p> <p>Parameters:</p> Name Type Description Default <code>include_buffers</code> <code>Optional[bool]</code> <p>Whether or not to also put all buffers on the meta device while initializing.</p> <code>None</code> <p>Returns:</p> Type Description <code>Iterator[None]</code> <p>(None) Just a context manager</p> <p>Example: <pre><code>import torch.nn as nn\nfrom  import init_empty_weights\n\n# Initialize a model with 100 billions parameters in no time and\n# without using any RAM.\nwith init_empty_weights():\n    tst = nn.Sequential(*[nn.Linear(10000, 10000) for _ in range(1000)])\n</code></pre></p> <p> <p>Any model created under this context manager has no weights. As such you can't do something like <code>model.to(some_device)</code> with it. To load weights inside an empty model, see [<code>load_checkpoint_and_dispatch</code>]. Make sure to overwrite the default device_map param for [<code>load_checkpoint_and_dispatch</code>], otherwise dispatch is not called.</p> <p></p>"},{"location":"reference/torch_to_nnef/utils/#torch_to_nnef.utils.init_on_device","title":"init_on_device","text":"<pre><code>init_on_device(device: torch.device, include_buffers: T.Optional[bool] = None) -&gt; T.Iterator[None]\n</code></pre> <p>Context manager under which models are init on the specified device.</p> <p>Borrowed from <code>accelerate</code></p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>device</code> <p>Device to initialize all parameters on.</p> required <code>include_buffers</code> <code>Optional[bool]</code> <p>Whether or not to also put all buffers on the meta device while initializing.</p> <code>None</code> <p>Example: <pre><code>import torch.nn as nn\nfrom accelerate import init_on_device\n\nwith init_on_device(device=torch.device(\"cuda\")):\n    tst = nn.Linear(100, 100)  # on `cuda` device\n</code></pre></p>"},{"location":"reference/torch_to_nnef/utils/#torch_to_nnef.utils.torch_version","title":"torch_version","text":"<pre><code>torch_version() -&gt; SemanticVersion\n</code></pre> <p>Semantic version for torch.</p>"}]}