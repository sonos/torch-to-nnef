extension tract_registry tract_core;

fragment scaled_dot_product_attention_{% rank %}d{% if scale %}_with_scale{% end %}{% if causal %}_causal{% end %}_{% dtype %}(
    query: tensor<scalar>,
    key: tensor<scalar>,
    value: tensor<scalar>,
    attn_mask: tensor<scalar>
    {% if scale %}
    scale: tensor<scalar>
    {% end %}
) -> ( out : tensor<scalar>) {
  # https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html
  transposed_key = transpose(key, axes = [0, 1, 3, 2]);
  dot_qk = matmul(query, transposed_key, transposeA = false, transposeB = false);
  {% if not scale % }
    qshape = tract_core_shape_of(query);
    qsize = tract_core_cast(slice(qshape, axes=[0], begin=[{% rank - 1 %}], end=[{% rank %}], stride=[1]), to="{% dtype %}");
    scale = div(1.0, sqrt(qsize));
  {% end %}
  # {% if causal % }
  #   temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)
  #   attn_bias.masked_fill_(temp_mask.logical_not(), float("-inf"))
  #   attn_bias.to(query.dtype)
  #   attn_mask += attn_bias;
  # {% end %}
  scaled_dot_qk = mul(dot_qk, scale);
  scaled_dot_qk_with_attn_mask = add(scaled_dot_qk, attn_mask);
  attn_weight = softmax(scaled_dot_qk_with_attn_mask,  axes=[3]);
  out = matmul(attn_weight, value, transposeA = false, transposeB = false);
}
